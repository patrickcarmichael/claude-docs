---
title: "Llamaindex: Supported LLM Providers (50+)"
description: "Supported LLM Providers (50+) section of Llamaindex documentation"
source: "https://llamaindex.com"
last_updated: "2025-11-08"
---

## Supported LLM Providers (50+)


### Major Cloud Providers

**OpenAI**
- GPT-4, GPT-4 Turbo
- GPT-3.5 Turbo
- Text and Vision models
- Integration: High-level and low-level APIs
- Configuration: API key authentication
- Use Cases: Production chatbots, complex reasoning, multi-modal analysis

**Azure OpenAI**
- Full OpenAI model parity on Azure infrastructure
- Enterprise features: VPC isolation, compliance
- Configuration: Endpoint URL and API keys
- Use Cases: Enterprise deployments with compliance requirements

**Google Gemini & Vertex AI**
- Gemini Pro, Gemini Pro Vision
- Vertex AI LLM APIs
- MakerSuite integration
- Use Cases: Multi-modal applications, enterprise AI

**AWS Bedrock**
- Access to multiple model providers
- Claudification: Anthropic Claude models
- Amazon Titan models
- Third-party models via Bedrock
- Use Cases: AWS-native applications, unified model access

**Anthropic Claude**
- Claude 3 family (Opus, Sonnet, Haiku)
- Claude 2 and legacy models
- Long context windows
- Vision capabilities
- Configuration: API key or AWS Bedrock endpoint
- Use Cases: Complex reasoning, code generation, analysis

### Open Source & Self-Hosted

**Ollama**
- Run large language models locally
- Supported models: Llama 2, Mistral, Neural Chat, Dolphin, Phi
- Zero setup for local deployment
- Configuration: Local endpoint (default: localhost:11434)
- Use Cases: Privacy-first applications, offline inference, development

**LlamaCPP**
- CPU-optimized inference
- GGUF format model support
- Quantized model execution
- Configuration: Model path and inference parameters
- Use Cases: Edge devices, low-resource environments, local development

**LM Studio**
- User-friendly model management
- GPU acceleration support
- Compatible with multiple model formats
- Local API server
- Use Cases: Easy local experimentation, GUI-based model management

**vLLM**
- High-throughput LLM serving
- Efficient batching and caching
- Distributed inference
- Configuration: vLLM server endpoint
- Use Cases: Production inference, batch processing, high-concurrency serving

**Hugging Face Models**
- Access to hundreds of thousands of models
- Text-generation, instruction-tuned models
- Transformers library integration
- Configuration: Model ID and API tokens
- Use Cases: Custom model fine-tuning, open-source model exploration

### Specialized & Emerging Providers

**Cohere**
- Specialized for enterprise NLP
- Command and Generate models
- Multilingual support
- Use Cases: Content generation, semantic search, information extraction

**Mistral AI**
- Open-weight models
- Mistral 7B, Mistral Medium, Mistral Large
- European AI focus
- Use Cases: Cost-effective production deployments, open-source alternatives

**Groq**
- Ultra-fast LLM inference
- GroqCloud API
- Real-time response capabilities
- Use Cases: High-speed chatbots, time-critical applications

**Together AI**
- Open-source model marketplace
- Distributed inference platform
- Support for dozens of models
- Use Cases: Cost-conscious deployments, model experimentation

**Fireworks AI**
- Fast and affordable LLM API
- Model fine-tuning capabilities
- Competitive pricing
- Use Cases: Scalable production deployments, cost optimization

**Replicate**
- Run open-source models in cloud
- Model discovery and marketplace
- Version control for models
- Use Cases: Reproducible ML, easy model distribution

**LiteLLM**
- Unified LLM API proxy
- Multi-provider abstraction
- Cost tracking and fallback handling
- Use Cases: Provider flexibility, cost optimization, failover strategies

**DeepSeek**
- Chinese LLM provider
- High-performance models
- Use Cases: Asian market applications, language-specific needs

**Grok (xAI)**
- Latest frontier model
- Real-time internet integration
- Use Cases: Current-awareness applications, research

**Perplexity AI**
- Retrieval-augmented generation
- Internet-connected LLM
- Use Cases: Real-time information synthesis

---

## Navigation

- [üìë Back to Index](./index.md)
- [üìÑ Full Documentation](./documentation.md)
- [üìù Original Source](../llms-full.txt)

**Previous:** [‚Üê Overview](./01-overview.md)

**Next:** [Configuration and Usage ‚Üí](./03-configuration-and-usage.md)
