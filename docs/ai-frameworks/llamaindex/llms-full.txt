# LlamaIndex LLM Integrations and Providers

## Overview

LlamaIndex is the leading framework for building LLM-powered agents over your data with integrations for 50+ LLM providers. The framework emphasizes flexibility and extensibility, supporting both cloud-based and self-hosted solutions.

Core philosophy: "You can use LLMs as auto-complete, chatbots, agents, and more. It just makes using them easier."

## Supported LLM Providers (50+)

### Major Cloud Providers

**OpenAI**
- GPT-4, GPT-4 Turbo
- GPT-3.5 Turbo
- Text and Vision models
- Integration: High-level and low-level APIs
- Configuration: API key authentication
- Use Cases: Production chatbots, complex reasoning, multi-modal analysis

**Azure OpenAI**
- Full OpenAI model parity on Azure infrastructure
- Enterprise features: VPC isolation, compliance
- Configuration: Endpoint URL and API keys
- Use Cases: Enterprise deployments with compliance requirements

**Google Gemini & Vertex AI**
- Gemini Pro, Gemini Pro Vision
- Vertex AI LLM APIs
- MakerSuite integration
- Use Cases: Multi-modal applications, enterprise AI

**AWS Bedrock**
- Access to multiple model providers
- Claudification: Anthropic Claude models
- Amazon Titan models
- Third-party models via Bedrock
- Use Cases: AWS-native applications, unified model access

**Anthropic Claude**
- Claude 3 family (Opus, Sonnet, Haiku)
- Claude 2 and legacy models
- Long context windows
- Vision capabilities
- Configuration: API key or AWS Bedrock endpoint
- Use Cases: Complex reasoning, code generation, analysis

### Open Source & Self-Hosted

**Ollama**
- Run large language models locally
- Supported models: Llama 2, Mistral, Neural Chat, Dolphin, Phi
- Zero setup for local deployment
- Configuration: Local endpoint (default: localhost:11434)
- Use Cases: Privacy-first applications, offline inference, development

**LlamaCPP**
- CPU-optimized inference
- GGUF format model support
- Quantized model execution
- Configuration: Model path and inference parameters
- Use Cases: Edge devices, low-resource environments, local development

**LM Studio**
- User-friendly model management
- GPU acceleration support
- Compatible with multiple model formats
- Local API server
- Use Cases: Easy local experimentation, GUI-based model management

**vLLM**
- High-throughput LLM serving
- Efficient batching and caching
- Distributed inference
- Configuration: vLLM server endpoint
- Use Cases: Production inference, batch processing, high-concurrency serving

**Hugging Face Models**
- Access to hundreds of thousands of models
- Text-generation, instruction-tuned models
- Transformers library integration
- Configuration: Model ID and API tokens
- Use Cases: Custom model fine-tuning, open-source model exploration

### Specialized & Emerging Providers

**Cohere**
- Specialized for enterprise NLP
- Command and Generate models
- Multilingual support
- Use Cases: Content generation, semantic search, information extraction

**Mistral AI**
- Open-weight models
- Mistral 7B, Mistral Medium, Mistral Large
- European AI focus
- Use Cases: Cost-effective production deployments, open-source alternatives

**Groq**
- Ultra-fast LLM inference
- GroqCloud API
- Real-time response capabilities
- Use Cases: High-speed chatbots, time-critical applications

**Together AI**
- Open-source model marketplace
- Distributed inference platform
- Support for dozens of models
- Use Cases: Cost-conscious deployments, model experimentation

**Fireworks AI**
- Fast and affordable LLM API
- Model fine-tuning capabilities
- Competitive pricing
- Use Cases: Scalable production deployments, cost optimization

**Replicate**
- Run open-source models in cloud
- Model discovery and marketplace
- Version control for models
- Use Cases: Reproducible ML, easy model distribution

**LiteLLM**
- Unified LLM API proxy
- Multi-provider abstraction
- Cost tracking and fallback handling
- Use Cases: Provider flexibility, cost optimization, failover strategies

**DeepSeek**
- Chinese LLM provider
- High-performance models
- Use Cases: Asian market applications, language-specific needs

**Grok (xAI)**
- Latest frontier model
- Real-time internet integration
- Use Cases: Current-awareness applications, research

**Perplexity AI**
- Retrieval-augmented generation
- Internet-connected LLM
- Use Cases: Real-time information synthesis

## Configuration and Usage

### Basic Configuration Pattern

```python
from llama_index.llms import [ProviderName]

llm = [ProviderName](
    api_key="your-api-key",
    # provider-specific parameters
)

from llama_index.core import Settings
Settings.llm = llm
```

### Provider-Specific Configuration

Each provider supports:
- **Authentication**: API keys, tokens, credentials
- **Model Selection**: Specific model versions to use
- **Parameters**: Temperature, max_tokens, system prompts
- **Advanced Options**: Caching, streaming, batch processing

### Multi-Provider Strategy

LlamaIndex enables:
- **Provider Switching**: Easy switching between providers
- **Fallback Handling**: Automatic failover to alternate providers
- **Cost Optimization**: Use different providers for different tasks
- **Feature Matching**: Select providers for specific capabilities

## Use Cases by Provider Category

### For Production Reliability
- **Primary**: OpenAI, Azure OpenAI, AWS Bedrock
- **Fallback**: Anthropic Claude, Cohere
- **Benefit**: Established SLAs, enterprise support

### For Cost Optimization
- **Primary**: Ollama (free, local), Mistral, Together AI
- **Secondary**: Groq, Fireworks
- **Benefit**: Significant cost reduction at scale

### For Privacy & Control
- **Primary**: Ollama, LlamaCPP, LM Studio
- **Secondary**: Self-hosted vLLM
- **Benefit**: Complete data sovereignty, no external calls

### For Cutting-Edge Capabilities
- **Primary**: OpenAI GPT-4, Claude 3 Opus
- **Secondary**: Google Gemini, Mistral Large
- **Benefit**: Latest research, best performance

### For Multi-Modal Applications
- **Primary**: OpenAI (Vision), Gemini
- **Secondary**: Claude 3 Vision
- **Benefit**: Image understanding, document analysis

### For Real-Time Applications
- **Primary**: Groq, Fireworks
- **Secondary**: Together AI
- **Benefit**: Sub-second latencies

## Integration Examples

### OpenAI Integration
```python
from llama_index.llms.openai import OpenAI

llm = OpenAI(model="gpt-4", temperature=0.7)
```

### Anthropic Claude Integration
```python
from llama_index.llms.anthropic import Anthropic

llm = Anthropic(model="claude-3-opus-20240229")
```

### Local Ollama Integration
```python
from llama_index.llms.ollama import Ollama

llm = Ollama(model="mistral", base_url="http://localhost:11434")
```

### AWS Bedrock Integration
```python
from llama_index.llms.bedrock import Bedrock

llm = Bedrock(model="anthropic.claude-3-sonnet-20240229-v1:0")
```

## Advanced Features

### Streaming Support
- Real-time token streaming for interactive applications
- Supported by: All major providers
- Use Cases: Chat interfaces, live response generation

### Function Calling / Tool Use
- Structured output for agent workflows
- Supported by: OpenAI, Anthropic, Gemini, Mistral
- Use Cases: Agent task execution, structured extraction

### Vision/Multi-Modal
- Image analysis and understanding
- Supported by: OpenAI, Claude 3, Gemini
- Use Cases: Document analysis, visual QA

### Long Context Windows
- Extended input length support
- Leaders: Claude 3 (200K tokens), GPT-4 Turbo (128K), Gemini (1M)
- Use Cases: Document processing, long conversation history

### Token Counting
- Accurate token estimation before API calls
- Supported by: All providers with native support
- Use Cases: Cost prediction, budget management

### Caching and Optimization
- Reduce API costs through intelligent caching
- Supported by: Anthropic, OpenAI
- Use Cases: Frequent queries, cost optimization

## Selection Criteria

**Speed**: Groq > Fireworks > Together AI > OpenAI > Others

**Cost**: Ollama (free) > Mistral > Together AI > Groq > OpenAI > Claude

**Quality**: Claude 3 Opus = GPT-4 > Gemini Pro > Mistral Large > Open Source

**Reliability**: OpenAI > Azure OpenAI > AWS Bedrock > Anthropic

**Privacy**: Ollama > LlamaCPP > Self-hosted vLLM > AWS Bedrock

**Features**: OpenAI ≈ Claude 3 ≈ Gemini > Mistral > Groq

## Fallback & Failover Patterns

LlamaIndex supports implementing fallback chains:

1. **Primary**: High-quality provider (OpenAI, Claude)
2. **Secondary**: Reliable fallback (Cohere, Together AI)
3. **Tertiary**: Cost-optimized backup (Mistral, Groq)
4. **Final**: Local fallback (Ollama)

Benefits:
- Enhanced reliability
- Automatic cost optimization
- Graceful degradation under high load

## Performance Considerations

### Latency Rankings
1. Groq (50-200ms) - Ultra-fast
2. Fireworks (100-300ms) - Very fast
3. Together AI (200-400ms) - Fast
4. OpenAI (300-800ms) - Standard
5. Claude (300-800ms) - Standard
6. Ollama (depends on hardware) - Variable

### Throughput Optimization
- Use vLLM for batch inference
- Implement token budgeting
- Cache common prompts
- Use streaming for UI responsiveness

### Cost Optimization Strategies
- Use cheaper models for classification
- Reserve expensive models for complex reasoning
- Implement prompt caching
- Use batch APIs when available
- Monitor token usage per provider

## LLM Selection Decision Tree

```
1. Need latest capabilities?
   → Yes: OpenAI GPT-4, Claude 3 Opus, Gemini
   → No: Proceed to 2

2. Have strict privacy requirements?
   → Yes: Ollama, LlamaCPP, Self-hosted vLLM
   → No: Proceed to 3

3. Cost is critical?
   → Yes: Mistral, Together AI, Groq
   → No: Proceed to 4

4. Need real-time low latency?
   → Yes: Groq, Fireworks
   → No: OpenAI, Claude, Gemini acceptable

5. Require vision capabilities?
   → Yes: OpenAI, Claude 3, Gemini
   → No: Any provider works

6. Enterprise requirements?
   → Yes: Azure OpenAI, AWS Bedrock, Anthropic Enterprise
   → No: Any provider acceptable
```

## Resources

- **Main Documentation**: https://docs.llamaindex.ai/
- **Framework Guide**: https://developers.llamaindex.ai/python/framework/
- **LlamaHub**: Community-contributed integrations
- **LlamaCloud**: Managed document processing and indexing
- **Blog**: Practical tutorials and use case guides

## Community & Support

- **Discord**: Active community support
- **Twitter**: Latest announcements and updates
- **LinkedIn**: Industry insights and thought leadership
- **GitHub**: Open-source contributions and issues

## Compatibility Notes

- **Python Versions**: 3.8+
- **Async Support**: Built-in async/await patterns
- **Streaming**: Full streaming support across providers
- **Custom Implementations**: Extensible LLM base class for custom providers

---

Last Updated: November 2024
Based on LlamaIndex framework documentation from developers.llamaindex.ai
