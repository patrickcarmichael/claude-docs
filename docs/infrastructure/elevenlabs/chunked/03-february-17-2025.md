**Navigation:** [← Previous](./02-august-11-2025.md) | [Index](./index.md) | [Next →](./04-voice-remixing.md)

# February 17, 2025

### Agents Platform

- **Tool calling fix**: Fixed an issue where tool calling was not working with agents using gpt-4o mini. This was due to a breaking change in the OpenAI API.
- **Tool calling improvements**: Added support for tool calling with dynamic variables inside objects and arrays.
- **Dynamic variables**: Fixed an issue where dynamic variables of a conversation were not being displayed correctly.

### Voice Isolator

- **Fixed**: Fixed an issue that caused the voice isolator to not work correctly temporarily.

### Workspace

- **Billing**: Improved billing visibility by differentiating rollover, cycle, gifted, and usage-based credits.
- **Usage Analytics**: Improved usage analytics load times and readability.
- **Fine grained fiat billing**: Added support for customizable pricing based on several factors.

### API

<Accordion title="View API changes">
- Added `phone_numbers` property to [Agent responses](/docs/api-reference/agents/get)
- Added usage metrics to subscription_extras in [User endpoint](/docs/api-reference/user/get):
  - `unused_characters_rolled_over_from_previous_period`
  - `overused_characters_rolled_over_from_previous_period`
  - `usage` statistics
- Added `enable_conversation_initiation_client_data_from_webhook` to [Agent creation](/docs/api-reference/agents/create)
- Updated [Agent](/docs/api-reference/agents) endpoints with consolidated settings for:
  - `platform_settings`
  - `overrides`
  - `safety`
- Deprecated `with_settings` parameter in [Voice retrieval endpoint](/docs/api-reference/voices/get)
</Accordion>



# February 10, 2025


## Agents Platform

- **Updated Pricing**: Updated self-serve pricing for Agents Platform with [reduced cost and a more generous free tier](/docs/agents-platform/overview#pricing-tiers).
- **Knowledge Base UI**: Created a new page to easily manage your [knowledge base](https://elevenlabs.io/app/agents/knowledge-base).
- **Live calls**: Added number of live calls in progress in the user [dashboard](https://elevenlabs.io/app/agents) and as a new endpoint.
- **Retention**: Added ability to customize transcripts and audio recordings [retention settings](/docs/agents-platform/customization/privacy/retention).
- **Audio recording**: Added a new option to [disable audio recordings](/docs/agents-platform/customization/privacy/audio-saving).
- **8k PCM support**: Added support for 8k PCM audio for both input and output.


## Studio

- **GenFM**: Updated the create podcast endpoint to accept [multiple input sources](/docs/api-reference/studio/create-podcast).
- **GenFM**: Fixed an issue where GenFM was creating empty podcasts.


## Enterprise

- **New workspace group endpoints**: Added new endpoints to manage [workspace groups](/docs/api-reference/workspace/search-user-groups).

### API

<AccordionGroup>
  <Accordion title="Deprecated Endpoints">
    
    **Studio (formerly Projects)**

    All `/v1/projects/*` endpoints have been deprecated in favor of the new `/v1/studio/projects/*` endpoints. The following endpoints are now deprecated:

    - All operations on `/v1/projects/`
    - All operations related to chapters, snapshots, and content under `/v1/projects/*`

    **Agents Platform**
    - `POST /v1/convai/add-tool` - Use `POST /v1/convai/tools` instead

  </Accordion>

  <Accordion title="Breaking Changes">
    - `DELETE /v1/convai/agents/{agent_id}` - Response type is no longer an object
    - `GET /v1/convai/tools` - Response type changed from array to object with a `tools` property
  </Accordion>

  <Accordion title="Modified Endpoints">
    **Agents Platform Updates**
    - `GET /v1/convai/agents/{agent_id}` - Updated conversation configuration and agent properties
    - `PATCH /v1/convai/agents/{agent_id}` - Added `use_tool_ids` parameter for tool management
    - `POST /v1/convai/agents/create` - Added tool integration via `use_tool_ids`

    **Knowledge Base & Tools**
    - `GET /v1/convai/agents/{agent_id}/knowledge-base/{documentation_id}` - Added `name` and `access_level` properties
    - `GET /v1/convai/knowledge-base/{documentation_id}` - Added `name` and `access_level` properties
    - `GET /v1/convai/tools/{tool_id}` - Added `dependent_agents` property
    - `PATCH /v1/convai/tools/{tool_id}` - Added `dependent_agents` property

    **GenFM**
    - `POST /v1/projects/podcast/create` - Added support for multiple input sources

  </Accordion>

  <Accordion title="New Endpoints">
    **Studio (formerly Projects)**
    
    New endpoints replacing the deprecated `/v1/projects/*` endpoints
    - `GET /v1/studio/projects`: List all projects
    - `POST /v1/studio/projects`: Create a project
    - `GET /v1/studio/projects/{project_id}`: Get project details
    - `DELETE /v1/studio/projects/{project_id}`: Delete a project

    **Knowledge Base Management**
    - `GET /v1/convai/knowledge-base`: List all knowledge base documents
    - `DELETE /v1/convai/knowledge-base/{documentation_id}`: Delete a knowledge base
    - `GET /v1/convai/knowledge-base/{documentation_id}/dependent-agents`: List agents using this knowledge base

    **Workspace Groups** - New enterprise features for team management
    - `GET /v1/workspace/groups/search`: Search workspace groups
    - `POST /v1/workspace/groups/{group_id}/members`: Add members to a group
    - `POST /v1/workspace/groups/{group_id}/members/remove`: Remove members from a group

    **Tools**
    - `POST /v1/convai/tools`: Create new tools for agents

  </Accordion>
</AccordionGroup>


## Socials

- **ElevenLabs Developers**: Follow our new developers account on X [@ElevenLabsDevs](https://x.com/intent/user?screen_name=elevenlabsdevs)



# February 4, 2025

### Agents Platform

- **Agent monitoring**: Added a new dashboard for monitoring ElevenLabs agents' activity. Check out your's [here](https://elevenlabs.io/app/agents).
- **Proactive conversations**: Enhanced capabilities with improved timeout retry logic. [Learn more](/docs/agents-platform/customization/conversation-flow)
- **Tool calls**: Fixed timeout issues occurring during tool calls
- **Allowlist**: Fixed implementation of allowlist functionality.
- **Content summarization**: Added Gemini as a fallback model to ensure service reliability
- **Widget stability**: Fixed issue with dynamic variables causing the Agents Platform widget to fail

### Reader

- **Trending content**: Added carousel showcasing popular articles and trending content
- **New publications**: Introduced dedicated section for recent ElevenReader Publishing releases

### Studio (formerly Projects)

- **Projects is now Studio** and is now generally available to everyone
- **Chapter content editing**: Added support for editing chapter content through the public API, enabling programmatic updates to chapter text and metadata
- **GenFM public API**: Added public API support for podcast creation through GenFM. Key features include:
  - Conversation mode with configurable host and guest voices
  - URL-based content sourcing
  - Customizable duration and highlights
  - Webhook callbacks for status updates
  - Project snapshot IDs for audio downloads

### SDKs

- **Swift**: fixed an issue where resources were not being released after the end of a session
- **Python**: added uv support
- **Python**: fixed an issue where calls were not ending correctly

### API

<Accordion title="View API changes">
- Added POST `v1/workspace/invites/add-bulk` [endpoint](/docs/api-reference/workspace/invite-multiple-users) to enable inviting multiple users simultaneously
- Added POST `v1/projects/podcast/create` [endpoint](/docs/api-reference/studio/create-podcast) for programmatic podcast generation through GenFM
- Added 'v1/convai/knowledge-base/:documentation_id' [endpoints](/docs/api-reference/knowledge-base/) with CRUD operations for Agents Platform
- Added PATCH `v1/projects/:project_id/chapters/:chapter_id` [endpoint](/docs/api-reference/studio/update-chapter) for updating project chapter content and metadata
- Added `group_ids` parameter to [Workspace Invite endpoint](/docs/api-reference/workspace/invite-user) for group-based access control
- Added structured `content` property to [Chapter response objects](/docs/api-reference/studio/get-chapter)
- Added `retention_days` and `delete_transcript_and_pii` data retention parameters to [Agent creation](/docs/api-reference/agents/create)
- Added structured response to [AudioNative content](/docs/api-reference/audio-native/create#response.body.project_id)
- Added `convai_chars_per_minute` usage metric to [User endpoint](/docs/api-reference/user/get)
- Added `media_metadata` field to [Dubbing response objects](/docs/api-reference/dubbing/get)
- Added GDPR-compliant `deletion_settings` to [Conversation responses](/docs/api-reference/conversations/get-conversation#response.body.metadata.deletion_settings)
- Deprecated Knowledge Base legacy endpoints:
  - POST `/v1/convai/agents/{agent_id}/add-to-knowledge-base`
  - GET `/v1/convai/agents/{agent_id}/knowledge-base/{documentation_id}`
- Updated Agent endpoints with consolidated [privacy control parameters](/docs/api-reference/agents/create)
</Accordion>



# January 27, 2025

### Docs

- **Shipped our new docs**: we're keen to hear your thoughts, you can reach out by opening an issue on [GitHub](https://github.com/elevenlabs/elevenlabs-docs) or chatting with us on [Discord](https://discord.gg/elevenlabs)

### Agents Platform

- **Dynamic variables**: Available in the dashboard and SDKs. [Learn more](/docs/agents-platform/customization/personalization/dynamic-variables)
- **Interruption handling**: Now possible to ignore user interruptions in Agents Platform. [Learn more](/docs/agents-platform/customization/conversation-flow#interruptions)
- **Twilio integration**: Shipped changes to increase audio quality when integrating with Twilio
- **Latency optimization**: Published detailed blog post on latency optimizations. [Read more](https://elevenlabs.io/blog/how-do-you-optimize-latency-for-conversational-ai)
- **PCM 8000**: Added support for PCM 8000 to ElevenLabs agents
- **Websocket improvements**: Fixed unexpected websocket closures

### Projects

- **Auto-regenerate**: Auto-regeneration now available by default at no extra cost
- **Content management**: Added `updateContent` method for dynamic content updates
- **Audio conversion**: New auto-convert and auto-publish flags for seamless workflows

### API

<Accordion title="View API changes">
- Added `Update Project` endpoint for [project editing](/docs/api-reference/studio/edit-project#:~:text=List%20projects-,POST,Update%20project,-GET)
- Added `Update Content` endpoint for [AudioNative content management](/docs/api-reference/audio-native/update-content)
- Deprecated `quality_check_on` parameter in [project operations](/docs/api-reference/studio/add-project#request.body.quality_check_on). It is now enabled for all users at no extra cost 
- Added `apply_text_normalization` parameter to project creation with modes 'auto', 'on', 'apply_english' and 'off' for controlling text normalization during [project creation](/docs/api-reference/studio/add-project#request.body.apply_text_normalization)
- Added alpha feature `auto_assign_voices` in [project creation](/docs/api-reference/studio/add-project#request.body.auto_assign_voices) to automatically assign voices to phrases 
- Added `auto_convert` flag to project creation to automatically convert [projects to audio](/docs/api-reference/audio-native/create#request.body.auto_convert)
- Added support for creating ElevenLabs agents with [dynamic variables](/docs/api-reference/agents/create#request.body.conversation_config.agent.dynamic_variables)
- Added `voice_slots_used` to `Subscription` model to track number of custom voices used in a workspace to the `User` [endpoint](/docs/api-reference/user/subscription/get#response.body.voice_slots_used)
- Added `user_id` field to `User` [endpoint](/docs/api-reference/user/get#response.body.user_id)
- Marked legacy AudioNative creation parameters (`image`, `small`, `sessionization`) as deprecated [parameters](/docs/api-reference/audio-native/create#request.body.image)
- Agents platform now supports `call_limits` containing either `agent_concurrency_limit` or `daily_limit` or both parameters to control simultaneous and daily conversation limits for [agents](/docs/api-reference/agents/create#request.body.platform_settings.call_limits)
- Added support for `language_presets` in `conversation_config` to customize language-specific [settings](/docs/api-reference/agents/create#request.body.conversation_config.language_presets)
</Accordion>

### SDKs

- **Cross-Runtime Support**: Now compatible with **Bun 1.1.45+** and **Deno 2.1.7+**
- **Regenerated SDKs**: We regenerated our SDKs to be up to date with the latest API spec. Check out the latest [Python SDK release](https://github.com/elevenlabs/elevenlabs-python/releases/tag/1.50.5) and [JS SDK release](https://github.com/elevenlabs/elevenlabs-js/releases/tag/v1.50.4)
- **Dynamic Variables**: Fixed an issue where dynamic variables were not being handled correctly, they are now correctly handled in all SDKs



# January 16, 2025


## Product

### Agents Platform

- **Additional languages**: Add a language dropdown to your widget so customers can launch conversations in their preferred language. Learn more [here](/docs/agents-platform/customization/language).
- **End call tool**: Let the agent automatically end the call with our new “End Call” tool. Learn more [here](/docs/agents-platform/customization/tools)
- **Flash default**: Flash, our lowest latency model, is now the default for new agents. In your agent dashboard under “voice”, you can toggle between Turbo and Flash. Learn more about Flash [here](https://elevenlabs.io/blog/meet-flash).
- **Privacy**: Set concurrent call and daily call limits, turn off audio recordings, add feedback collection, and define customer terms & conditions.
- **Increased tool limits**: Increase the number of tools available to your agent from 5 to 15. Learn more [here](/docs/agents-platform/customization/tools).



# January 2, 2025


## Product

- **Workspace Groups and Permissions**: Introduced new workspace group management features to enhance access control within organizations. [Learn more](https://elevenlabs.io/blog/workspace-groups-and-permissions).



# December 19, 2024


## Model

- **Introducing Flash**: Our fastest text-to-speech model yet, generating speech in just 75ms. Access it via the API with model IDs `eleven_flash_v2` and `eleven_flash_v2_5`. Perfect for low-latency Agents Platform applications. [Try it now](https://elevenlabs.io/docs/api-reference/text-to-speech).


## Launches

- **[TalkToSanta.io](https://www.talktosanta.io)**: Experience Agents Platform in action by talking to Santa this holiday season. For every conversation with santa we donate 2 dollars to [Bridging Voice](https://www.bridgingvoice.org) (up to $11,000).

- **[AI Engineer Pack](https://aiengineerpack.com)**: Get $50+ in credits from leading AI developer tools, including ElevenLabs.



# December 6, 2024


## Product

- **GenFM Now on Web**: Access GenFM directly from the website in addition to the ElevenReader App, [try it now](https://elevenlabs.io/app/projects).



# December 3, 2024


## API

- **Credit Usage Limits**: Set specific credit limits for API keys to control costs and manage usage across different use cases by setting "Access" or "No Access" to features like Dubbing, Audio Native, and more. [Check it out](https://elevenlabs.io/app/settings/api-keys)
- **Workspace API Keys**: Now support access permissions, such as "Read" or "Read and Write" for User, Workspace, and History resources.
- **Improved Key Management**:
  - Redesigned interface moving from modals to dedicated pages
  - Added detailed descriptions and key information
  - Enhanced visibility of key details and settings



# November 29, 2024


## Product

- **GenFM**: Launched in the ElevenReader app. [Learn more](https://elevenlabs.io/blog/genfm-on-elevenreader)
- **Agents Platform**: Now generally available to all customers. [Try it now](https://elevenlabs.io/conversational-ai)
- **TTS Redesign**: The website TTS redesign is now rolled out to all customers.
- **Auto-regenerate**: Now available in Projects. [Learn more](https://elevenlabs.io/blog/auto-regenerate-is-live-in-projects)
- **Reader Platform Improvements**:

  - Improved content sharing with enhanced landing pages and social media previews.
  - Added podcast rating system and improved voice synchronization.

- **Projects revamp**:
  - Restore past generations, lock content, assign speakers to sentence fragments, and QC at 2x speed. [Learn more](https://elevenlabs.io/blog/narrate-any-project)
  - Auto-regeneration identifies mispronunciations and regenerates audio at no extra cost. [Learn more](https://elevenlabs.io/blog/auto-regenerate-is-live-in-projects)


## API

- **Agents Platform**: [SDKs and APIs](https://elevenlabs.io/docs/agents-platform/quickstart) now available.



# October 27, 2024


## API

- **u-law Audio Formats**: Added u-law audio formats to the Convai API for integrations with Twilio.
- **TTS Websocket Improvements**: TTS websocket improvements, flushes and generation work more intuitively now.
- **TTS Websocket Auto Mode**: A streamlined mode for using websockets. This setting reduces latency by disabling chunk scheduling and buffers. Note: Using partial sentences will result in significantly reduced quality.
- **Improvements to latency consistency**: Improvements to latency consistency for all models.


## Website

- **TTS Redesign**: The website TTS redesign is now in alpha!



# October 20, 2024


## API

- **Normalize Text with the API**: Added the option normalize the input text in the TTS API. The new parameter is called `apply_text_normalization` and works on all models. For v2.5 models, this feature is available with Enterprise plans only.


## Product

- **Voice Design**: The Voice Design feature is now in beta!



# October 13, 2024


## Model

- **Stability Improvements**: Significant audio stability improvements across all models, most noticeable on `turbo_v2` and `turbo_v2.5`, when using:
  - Websockets
  - Projects
  - Reader app
  - TTS with request stitching
  - ConvAI
- **Latency Improvements**: Reduced time to first byte latency by approximately 20-30ms for all models.


## API

- **Remove Background Noise Voice Samples**: Added the ability to remove background noise from voice samples using our audio isolation model to improve quality for IVCs and PVCs at no additional cost.
- **Remove Background Noise STS Input**: Added the ability to remove background noise from STS audio input using our audio isolation model to improve quality at no additional cost.


## Feature

- **Agents Platform Beta**: Agents Platform is now in beta.



# Text to Speech

> Learn how to turn text into lifelike spoken audio with ElevenLabs.


## Overview

ElevenLabs [Text to Speech (TTS)](/docs/api-reference/text-to-speech) API turns text into lifelike audio with nuanced intonation, pacing and emotional awareness. [Our models](/docs/models) adapt to textual cues across 32 languages and multiple voice styles and can be used to:

* Narrate global media campaigns & ads
* Produce audiobooks in multiple languages with complex emotional delivery
* Stream real-time audio from text

Listen to a sample:

<elevenlabs-audio-player audio-title="George" audio-src="https://storage.googleapis.com/eleven-public-cdn/audio/marketing/george.mp3" />

Explore our [voice library](https://elevenlabs.io/community) to find the perfect voice for your project.

<CardGroup cols={2}>
  <Card title="Developer quickstart" icon="duotone book-sparkles" href="/docs/quickstart">
    Learn how to integrate text to speech into your application.
  </Card>

  <Card title="Product guide" icon="duotone book-user" href="/docs/product-guides/playground/text-to-speech">
    Step-by-step guide for using text to speech in ElevenLabs.
  </Card>
</CardGroup>

### Voice quality

For real-time applications, Flash v2.5 provides ultra-low 75ms latency, while Multilingual v2 delivers the highest quality audio with more nuanced expression.

<CardGroup cols={2} rows={2}>
  <Card title={<div className="flex items-start gap-2"><div>Eleven v3</div><div><img src="file:d66e9c4a-daa5-42e3-84d7-a745c2eea254" alt="Alpha" /></div></div>} href="/docs/models#eleven-v3-alpha">
    Our most emotionally rich, expressive speech synthesis model

    <div>
      <div>
        Dramatic delivery and performance
      </div>

      <div>
        70+ languages supported
      </div>

      <div>
        3,000 character limit
      </div>

      <div>
        Support for natural multi-speaker dialogue
      </div>
    </div>
  </Card>

  <Card title="Eleven Multilingual v2" href="/docs/models#multilingual-v2">
    Lifelike, consistent quality speech synthesis model

    <div>
      <div>
        Natural-sounding output
      </div>

      <div>
        29 languages supported
      </div>

      <div>
        10,000 character limit
      </div>

      <div>
        Most stable on long-form generations
      </div>
    </div>
  </Card>

  <Card title="Eleven Flash v2.5" href="/docs/models#flash-v25">
    Our fast, affordable speech synthesis model

    <div>
      <div>
        Ultra-low latency (~75ms†)
      </div>

      <div>
        32 languages supported
      </div>

      <div>
        40,000 character limit
      </div>

      <div>
        Faster model, 50% lower price per character
      </div>
    </div>
  </Card>

  <Card title="Eleven Turbo v2.5" href="/docs/models#turbo-v25">
    High quality, low-latency model with a good balance of quality and speed

    <div>
      <div>
        High quality voice generation
      </div>

      <div>
        32 languages supported
      </div>

      <div>
        40,000 character limit
      </div>

      <div>
        Low latency (~250ms-300ms†), 50% lower price per character
      </div>
    </div>
  </Card>
</CardGroup>

<div>
  <div>
    [Explore all](/docs/models)
  </div>
</div>

### Voice options

ElevenLabs offers thousands of voices across 32 languages through multiple creation methods:

* [Voice library](/docs/capabilities/voices) with 3,000+ community-shared voices
* [Professional voice cloning](/docs/capabilities/voices#cloned) for highest-fidelity replicas
* [Instant voice cloning](/docs/capabilities/voices#cloned) for quick voice replication
* [Voice design](/docs/capabilities/voices#voice-design) to generate custom voices from text descriptions

Learn more about our [voice options](/docs/capabilities/voices).

### Supported formats

The default response format is "mp3", but other formats like "PCM", & "μ-law" are available.

* **MP3**
  * Sample rates: 22.05kHz - 44.1kHz
  * Bitrates: 32kbps - 192kbps
  * 22.05kHz @ 32kbps
  * 44.1kHz @ 32kbps, 64kbps, 96kbps, 128kbps, 192kbps
* **PCM (S16LE)**
  * Sample rates: 16kHz - 44.1kHz
  * Bitrates: 8kHz, 16kHz, 22.05kHz, 24kHz, 44.1kHz, 48kHz
  * 16-bit depth
* **μ-law**
  * 8kHz sample rate
  * Optimized for telephony applications
* **A-law**
  * 8kHz sample rate
  * Optimized for telephony applications
* **Opus**
  * Sample rate: 48kHz
  * Bitrates: 32kbps - 192kbps

<Success>
  Higher quality audio options are only available on paid tiers - see our [pricing
  page](https://elevenlabs.io/pricing/api) for details.
</Success>

### Supported languages

Our multilingual v2 models support 29 languages:

*English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian.*

Flash v2.5 supports 32 languages - all languages from v2 models plus:

*Hungarian, Norwegian & Vietnamese*

Simply input text in any of our supported languages and select a matching voice from our [voice library](https://elevenlabs.io/community). For the most natural results, choose a voice with an accent that matches your target language and region.

### Prompting

The models interpret emotional context directly from the text input. For example, adding
descriptive text like "she said excitedly" or using exclamation marks will influence the speech
emotion. Voice settings like Stability and Similarity help control the consistency, while the
underlying emotion comes from textual cues.

Read the [prompting guide](/docs/best-practices/prompting) for more details.

<Note>
  Descriptive text will be spoken out by the model and must be manually trimmed or removed from the
  audio if desired.
</Note>


## FAQ

<AccordionGroup>
  <Accordion title="Can I clone my own voice?">
    Yes, you can create [instant voice clones](/docs/capabilities/voices#cloned) of your own voice
    from short audio clips. For high-fidelity clones, check out our [professional voice
    cloning](/docs/capabilities/voices#cloned) feature.
  </Accordion>

  <Accordion title="Do I own the audio output?">
    Yes. You retain ownership of any audio you generate. However, commercial usage rights are only
    available with paid plans. With a paid subscription, you may use generated audio for commercial
    purposes and monetize the outputs if you own the IP rights to the input content.
  </Accordion>

  <Accordion title="What qualifies as a free regeneration?">
    A free regeneration allows you to regenerate the same text to speech content without additional cost, subject to these conditions:

    * You can regenerate each piece of content up to 2 times for free
    * The content must be exactly the same as the previous generation. Any changes to the text, voice settings, or other parameters will require a new, paid generation

    Free regenerations are useful in case there is a slight distortion in the audio output. According to ElevenLabs' internal benchmarks, regenerations will solve roughly half of issues with quality, with remaining issues usually due to poor training data.
  </Accordion>

  <Accordion title="How do I reduce latency for real-time cases?">
    Use the low-latency Flash [models](/docs/models) (Flash v2 or v2.5) optimized for near real-time
    conversational or interactive scenarios. See our [latency optimization
    guide](/docs/best-practices/latency-optimization) for more details.
  </Accordion>

  <Accordion title="Why is my output sometimes inconsistent?">
    The models are nondeterministic. For consistency, use the optional [seed
    parameter](/docs/api-reference/text-to-speech/convert#request.body.seed), though subtle
    differences may still occur.
  </Accordion>

  <Accordion title="What's the best practice for large text conversions?">
    Split long text into segments and use streaming for real-time playback and efficient processing.
    To maintain natural prosody flow between chunks, include [previous/next text or previous/next
    request id parameters](/docs/api-reference/text-to-speech/convert#request.body.previous_text).
  </Accordion>
</AccordionGroup>



# Speech to Text

> Learn how to turn spoken audio into text with ElevenLabs.


## Overview

The ElevenLabs [Speech to Text (STT)](/docs/api-reference/speech-to-text) API turns spoken audio into text with state of the art accuracy. Our Scribe v1 [model](/docs/models) adapts to textual cues across 99 languages and multiple voice styles and can be used to:

* Transcribe podcasts, interviews, and other audio or video content
* Generate transcripts for meetings and other audio or video recordings

<CardGroup cols={2}>
  <Card title="Developer tutorial" icon="duotone book-sparkles" href="/docs/cookbooks/speech-to-text/quickstart">
    Learn how to integrate speech to text into your application.
  </Card>

  <Card title="Product guide" icon="duotone book-user" href="/docs/product-guides/playground/speech-to-text">
    Step-by-step guide for using speech to text in ElevenLabs.
  </Card>
</CardGroup>

<Info>
  Companies requiring HIPAA compliance must contact [ElevenLabs
  Sales](https://elevenlabs.io/contact-sales) to sign a Business Associate Agreement (BAA)
  agreement. Please ensure this step is completed before proceeding with any HIPAA-related
  integrations or deployments.
</Info>


## State of the art accuracy

The Scribe v1 model is capable of transcribing audio from up to 32 speakers with high accuracy. Optionally it can also transcribe audio events like laughter, applause, and other non-speech sounds.

The transcribed output supports exact timestamps for each word and audio event, plus diarization to identify the speaker for each word.

The Scribe v1 model is best used for when high-accuracy transcription is required rather than real-time transcription. A low-latency, real-time version will be released soon.


## Pricing

<Tabs>
  <Tab title="Developer API">
    | Tier     | Price/month | Hours included      | Price per included hour | Price per additional hour |
    | -------- | ----------- | ------------------- | ----------------------- | ------------------------- |
    | Free     | \$0         | Unavailable         | Unavailable             | Unavailable               |
    | Starter  | \$5         | 12 hours 30 minutes | \$0.40                  | Unavailable               |
    | Creator  | \$22        | 62 hours 51 minutes | \$0.35                  | \$0.48                    |
    | Pro      | \$99        | 300 hours           | \$0.33                  | \$0.40                    |
    | Scale    | \$330       | 1,100 hours         | \$0.30                  | \$0.33                    |
    | Business | \$1,320     | 6,000 hours         | \$0.22                  | \$0.22                    |
  </Tab>

  <Tab title="Product interface pricing">
    | Tier     | Price/month | Hours included  | Price per included hour |
    | -------- | ----------- | --------------- | ----------------------- |
    | Free     | \$0         | 12 minutes      | Unavailable             |
    | Starter  | \$5         | 1 hour          | \$5                     |
    | Creator  | \$22        | 4 hours 53 min  | \$4.5                   |
    | Pro      | \$99        | 24 hours 45 min | \$4                     |
    | Scale    | \$330       | 94 hours 17 min | \$3.5                   |
    | Business | \$1,320     | 440 hours       | \$3                     |
  </Tab>
</Tabs>

<Note>
  For reduced pricing at higher scale than 6,000 hours/month in addition to custom MSAs and DPAs,
  please [contact sales](https://elevenlabs.io/contact-sales).

  **Note: The free tier requires attribution and does not have commercial licensing.**
</Note>

Scribe has higher concurrency limits than other services from ElevenLabs.
Please see other concurrency limits [here](/docs/models#concurrency-and-priority)

| Plan       | STT Concurrency Limit |
| ---------- | --------------------- |
| Free       | 8                     |
| Starter    | 12                    |
| Creator    | 20                    |
| Pro        | 40                    |
| Scale      | 60                    |
| Business   | 60                    |
| Enterprise | Elevated              |


## Examples

The following example shows the output of the Scribe v1 model for a sample audio file.

<elevenlabs-audio-player audio-title="Nicole" audio-src="https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3" />

```javascript
{
  "language_code": "en",
  "language_probability": 1,
  "text": "With a soft and whispery American accent, I'm the ideal choice for creating ASMR content, meditative guides, or adding an intimate feel to your narrative projects.",
  "words": [
    {
      "text": "With",
      "start": 0.119,
      "end": 0.259,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 0.239,
      "end": 0.299,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "a",
      "start": 0.279,
      "end": 0.359,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 0.339,
      "end": 0.499,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "soft",
      "start": 0.479,
      "end": 1.039,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 1.019,
      "end": 1.2,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "and",
      "start": 1.18,
      "end": 1.359,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 1.339,
      "end": 1.44,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "whispery",
      "start": 1.419,
      "end": 1.979,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 1.959,
      "end": 2.179,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "American",
      "start": 2.159,
      "end": 2.719,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 2.699,
      "end": 2.779,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "accent,",
      "start": 2.759,
      "end": 3.389,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 4.119,
      "end": 4.179,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "I'm",
      "start": 4.159,
      "end": 4.459,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 4.44,
      "end": 4.52,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "the",
      "start": 4.5,
      "end": 4.599,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 4.579,
      "end": 4.699,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "ideal",
      "start": 4.679,
      "end": 5.099,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 5.079,
      "end": 5.219,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "choice",
      "start": 5.199,
      "end": 5.719,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 5.699,
      "end": 6.099,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "for",
      "start": 6.099,
      "end": 6.199,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 6.179,
      "end": 6.279,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "creating",
      "start": 6.259,
      "end": 6.799,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 6.779,
      "end": 6.979,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "ASMR",
      "start": 6.959,
      "end": 7.739,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 7.719,
      "end": 7.859,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "content,",
      "start": 7.839,
      "end": 8.45,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 9,
      "end": 9.06,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "meditative",
      "start": 9.04,
      "end": 9.64,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 9.619,
      "end": 9.699,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "guides,",
      "start": 9.679,
      "end": 10.359,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 10.359,
      "end": 10.409,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "or",
      "start": 11.319,
      "end": 11.439,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 11.42,
      "end": 11.52,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "adding",
      "start": 11.5,
      "end": 11.879,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 11.859,
      "end": 12,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "an",
      "start": 11.979,
      "end": 12.079,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 12.059,
      "end": 12.179,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "intimate",
      "start": 12.179,
      "end": 12.579,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 12.559,
      "end": 12.699,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "feel",
      "start": 12.679,
      "end": 13.159,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 13.139,
      "end": 13.179,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "to",
      "start": 13.159,
      "end": 13.26,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 13.239,
      "end": 13.3,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "your",
      "start": 13.299,
      "end": 13.399,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 13.379,
      "end": 13.479,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "narrative",
      "start": 13.479,
      "end": 13.889,
      "type": "word",
      "speaker_id": "speaker_0"
    },
    {
      "text": " ",
      "start": 13.919,
      "end": 13.939,
      "type": "spacing",
      "speaker_id": "speaker_0"
    },
    {
      "text": "projects.",
      "start": 13.919,
      "end": 14.779,
      "type": "word",
      "speaker_id": "speaker_0"
    }
  ]
}
```

The output is classified in three category types:

* `word` - A word in the language of the audio
* `spacing` - The space between words, not applicable for languages that don't use spaces like Japanese, Mandarin, Thai, Lao, Burmese and Cantonese
* `audio_event` - Non-speech sounds like laughter or applause


## Models

<CardGroup cols={1} rows={1}>
  <Card title="Scribe v1" href="/docs/models#scribe-v1">
    State-of-the-art speech recognition model

    <div>
      <div>
        Accurate transcription in 99 languages
      </div>

      <div>
        Precise word-level timestamps
      </div>

      <div>
        Speaker diarization
      </div>

      <div>
        Dynamic audio tagging
      </div>
    </div>
  </Card>
</CardGroup>

<div>
  <div>
    [Explore all](/docs/models)
  </div>
</div>


## Concurrency and priority

Concurrency is the concept of how many requests can be processed at the same time.

For Speech to Text, files that are over 8 minutes long are transcribed in parallel internally in order to speed up processing. The audio is chunked into four segments to be transcribed concurrently.

You can calculate the concurrency limit with the following calculation:

$$
Concurrency = \min(4, \text{round\_up}(\frac{\text{audio\_duration\_secs}}{480}))
$$

For example, a 15 minute audio file will be transcribed with a concurrency of 2, while a 120 minute audio file will be transcribed with a concurrency of 4.


## Supported languages

The Scribe v1 model supports 99 languages, including:

*Afrikaans (afr), Amharic (amh), Arabic (ara), Armenian (hye), Assamese (asm), Asturian (ast), Azerbaijani (aze), Belarusian (bel), Bengali (ben), Bosnian (bos), Bulgarian (bul), Burmese (mya), Cantonese (yue), Catalan (cat), Cebuano (ceb), Chichewa (nya), Croatian (hrv), Czech (ces), Danish (dan), Dutch (nld), English (eng), Estonian (est), Filipino (fil), Finnish (fin), French (fra), Fulah (ful), Galician (glg), Ganda (lug), Georgian (kat), German (deu), Greek (ell), Gujarati (guj), Hausa (hau), Hebrew (heb), Hindi (hin), Hungarian (hun), Icelandic (isl), Igbo (ibo), Indonesian (ind), Irish (gle), Italian (ita), Japanese (jpn), Javanese (jav), Kabuverdianu (kea), Kannada (kan), Kazakh (kaz), Khmer (khm), Korean (kor), Kurdish (kur), Kyrgyz (kir), Lao (lao), Latvian (lav), Lingala (lin), Lithuanian (lit), Luo (luo), Luxembourgish (ltz), Macedonian (mkd), Malay (msa), Malayalam (mal), Maltese (mlt), Mandarin Chinese (zho), Māori (mri), Marathi (mar), Mongolian (mon), Nepali (nep), Northern Sotho (nso), Norwegian (nor), Occitan (oci), Odia (ori), Pashto (pus), Persian (fas), Polish (pol), Portuguese (por), Punjabi (pan), Romanian (ron), Russian (rus), Serbian (srp), Shona (sna), Sindhi (snd), Slovak (slk), Slovenian (slv), Somali (som), Spanish (spa), Swahili (swa), Swedish (swe), Tamil (tam), Tajik (tgk), Telugu (tel), Thai (tha), Turkish (tur), Ukrainian (ukr), Umbundu (umb), Urdu (urd), Uzbek (uzb), Vietnamese (vie), Welsh (cym), Wolof (wol), Xhosa (xho) and Zulu (zul).*

### Breakdown of language support

Word Error Rate (WER) is a key metric used to evaluate the accuracy of transcription systems. It measures how many errors are present in a transcript compared to a reference transcript. Below is a breakdown of the WER for each language that Scribe v1 supports.

<AccordionGroup>
  <Accordion title="Excellent (≤ 5% WER)">
    Bulgarian (bul), Catalan (cat), Czech (ces), Danish (dan), Dutch (nld), English (eng), Finnish
    (fin), French (fra), Galician (glg), German (deu), Greek (ell), Hindi (hin), Indonesian (ind),
    Italian (ita), Japanese (jpn), Kannada (kan), Malay (msa), Malayalam (mal), Macedonian (mkd),
    Norwegian (nor), Polish (pol), Portuguese (por), Romanian (ron), Russian (rus), Serbian (srp),
    Slovak (slk), Spanish (spa), Swedish (swe), Turkish (tur), Ukrainian (ukr) and Vietnamese (vie).
  </Accordion>

  <Accordion title="High Accuracy (>5% to ≤10% WER)">
    Bengali (ben), Belarusian (bel), Bosnian (bos), Cantonese (yue), Estonian (est), Filipino (fil),
    Gujarati (guj), Hungarian (hun), Kazakh (kaz), Latvian (lav), Lithuanian (lit), Mandarin (cmn),
    Marathi (mar), Nepali (nep), Odia (ori), Persian (fas), Slovenian (slv), Tamil (tam) and Telugu
    (tel)
  </Accordion>

  <Accordion title="Good (>10% to ≤25% WER)">
    Afrikaans (afr), Arabic (ara), Armenian (hye), Assamese (asm), Asturian (ast), Azerbaijani
    (aze), Burmese (mya), Cebuano (ceb), Croatian (hrv), Georgian (kat), Hausa (hau), Hebrew (heb),
    Icelandic (isl), Javanese (jav), Kabuverdianu (kea), Korean (kor), Kyrgyz (kir), Lingala (lin),
    Maltese (mlt), Mongolian (mon), Māori (mri), Occitan (oci), Punjabi (pan), Sindhi (snd), Swahili
    (swa), Tajik (tgk), Thai (tha), Urdu (urd), Uzbek (uzb) and Welsh (cym).
  </Accordion>

  <Accordion title="Moderate (>25% to ≤50% WER)">
    Amharic (amh), Chichewa (nya), Fulah (ful), Ganda (lug), Igbo (ibo), Irish (gle), Khmer (khm),
    Kurdish (kur), Lao (lao), Luxembourgish (ltz), Luo (luo), Northern Sotho (nso), Pashto (pus),
    Shona (sna), Somali (som), Umbundu (umb), Wolof (wol), Xhosa (xho) and Zulu (zul).
  </Accordion>
</AccordionGroup>


## FAQ

<AccordionGroup>
  <Accordion title="Can I use speech to text with video files?">
    Yes, the API supports uploading both audio and video files for transcription.
  </Accordion>

  <Accordion title="What are the file size and duration limits?">
    Files up to 3 GB in size and up to 10 hours in duration are supported.
  </Accordion>

  <Accordion title="Which audio and video formats are supported?">
    The audio supported audio formats include:

    * audio/aac
    * audio/x-aac
    * audio/x-aiff
    * audio/ogg
    * audio/mpeg
    * audio/mp3
    * audio/mpeg3
    * audio/x-mpeg-3
    * audio/opus
    * audio/wav
    * audio/x-wav
    * audio/webm
    * audio/flac
    * audio/x-flac
    * audio/mp4
    * audio/aiff
    * audio/x-m4a

    Supported video formats include:

    * video/mp4
    * video/x-msvideo
    * video/x-matroska
    * video/quicktime
    * video/x-ms-wmv
    * video/x-flv
    * video/webm
    * video/mpeg
    * video/3gpp
  </Accordion>

  <Accordion title="When will you support more languages?">
    ElevenLabs is constantly expanding the number of languages supported by our models. Please check back frequently for updates.
  </Accordion>

  <Accordion title="Does speech to text API support webhooks?">
    Yes, asynchronous transcription results can be sent to webhooks configured in webhook settings in the UI. Learn more in the [webhooks cookbook](/docs/cookbooks/speech-to-text/webhooks).
  </Accordion>

  <Accordion title="Is a multichannel transcription mode supported?">
    Yes, the multichannel STT feature allows you to transcribe audio where each channel is processed independently and assigned a speaker ID based on its channel number. This feature supports up to 5 channels. Learn more in the [multichannel transcription cookbook](/docs/cookbooks/speech-to-text/multichannel-transcription).
  </Accordion>
</AccordionGroup>



# Text to Dialogue

> Learn how to create immersive, natural-sounding dialogue with ElevenLabs.


## Overview

The ElevenLabs [Text to Dialogue](/docs/api-reference/text-to-dialogue) API creates natural sounding expressive dialogue from text using the Eleven v3 model. Popular use cases include:

* Generating pitch perfect conversations for video games
* Creating immersive dialogue for podcasts and other audio content
* Bring audiobooks to life with expressive narration

Text to Dialogue is not intended for use in real-time applications like conversational agents. Several generations might be required to achieve the desired results. When integrating Text to Dialogue into your application, consider generating several generations and allowing the user to select the best one.

Listen to a sample:

<elevenlabs-audio-player audio-title="Dialogue example" audio-src="https://storage.googleapis.com/eleven-public-cdn/documentation_assets/audio/dialogue.mp3" />

<CardGroup cols={2}>
  <Card title="Developer tutorial" icon="duotone code" href="/docs/cookbooks/text-to-dialogue">
    Learn how to integrate text to dialogue into your application.
  </Card>

  <Card title="Eleven v3 prompting guide" icon="duotone book-sparkles" href="/docs/best-practices/prompting/eleven-v3">
    Learn how to use the Eleven v3 model to generate expressive dialogue.
  </Card>
</CardGroup>


## Voice options

ElevenLabs offers thousands of voices across 70+ languages through multiple creation methods:

* [Voice library](/docs/capabilities/voices) with 3,000+ community-shared voices
* [Professional voice cloning](/docs/capabilities/voices#cloned) for highest-fidelity replicas
* [Instant voice cloning](/docs/capabilities/voices#cloned) for quick voice replication
* [Voice design](/docs/capabilities/voices#voice-design) to generate custom voices from text descriptions

Learn more about our [voice options](/docs/capabilities/voices).


## Prompting

The models interpret emotional context directly from the text input. For example, adding
descriptive text like "she said excitedly" or using exclamation marks will influence the speech
emotion. Voice settings like Stability and Similarity help control the consistency, while the
underlying emotion comes from textual cues.

Read the [prompting guide](/docs/best-practices/prompting) for more details.

### Emotional deliveries with audio tags

<Warning>
  This feature is still under active development, actual results may vary.
</Warning>

The Eleven v3 model allows the use of non-speech audio events to influence the delivery of the dialogue. This is done by inserting the audio events into the text input wrapped in square brackets.

Audio tags come in a few different forms:

### Emotions and delivery

For example, \[sad], \[laughing] and \[whispering]

### Audio events

For example, \[leaves rustling], \[gentle footsteps] and \[applause].

### Overall direction

For example, \[football], \[wrestling match] and \[auctioneer].

Some examples include:

```
"[giggling] That's really funny!"
"[groaning] That was awful."
"Well, [sigh] I'm not sure what to say."
```

<elevenlabs-audio-player audio-title="Expressive dialogue" audio-src="https://storage.googleapis.com/eleven-public-cdn/documentation_assets/audio/dialogue-emotive.mp3" />

You can also use punctuation to indicate the flow of dialog, like interruptions:

```
"[cautiously] Hello, is this seat-"
"[jumping in] Free? [cheerfully] Yes it is."
```

<elevenlabs-audio-player audio-title="Interruption" audio-src="https://storage.googleapis.com/eleven-public-cdn/documentation_assets/audio/dialogue-interruption.mp3" />

Ellipses can be used to indicate trailing sentences:

```
"[indecisive] Hi, can I get uhhh..."
"[quizzically] The usual?"
"[elated] Yes! [laughs] I'm so glad you knew!"
```

<elevenlabs-audio-player audio-title="Ellipses" audio-src="https://storage.googleapis.com/eleven-public-cdn/documentation_assets/audio/dialogue-ellipses.mp3" />


## Supported formats

The default response format is "mp3", but other formats like "PCM", & "μ-law" are available.

* **MP3**
  * Sample rates: 22.05kHz - 44.1kHz
  * Bitrates: 32kbps - 192kbps
  * 22.05kHz @ 32kbps
  * 44.1kHz @ 32kbps, 64kbps, 96kbps, 128kbps, 192kbps
* **PCM (S16LE)**
  * Sample rates: 16kHz - 44.1kHz
  * Bitrates: 8kHz, 16kHz, 22.05kHz, 24kHz, 44.1kHz, 48kHz
  * 16-bit depth
* **μ-law**
  * 8kHz sample rate
  * Optimized for telephony applications
* **A-law**
  * 8kHz sample rate
  * Optimized for telephony applications
* **Opus**
  * Sample rate: 48kHz
  * Bitrates: 32kbps - 192kbps

<Success>
  Higher quality audio options are only available on paid tiers - see our [pricing
  page](https://elevenlabs.io/pricing/api) for details.
</Success>


## Supported languages

The Eleven v3 model supports 70+ languages, including:

*Afrikaans (afr), Arabic (ara), Armenian (hye), Assamese (asm), Azerbaijani (aze), Belarusian (bel), Bengali (ben), Bosnian (bos), Bulgarian (bul), Catalan (cat), Cebuano (ceb), Chichewa (nya), Croatian (hrv), Czech (ces), Danish (dan), Dutch (nld), English (eng), Estonian (est), Filipino (fil), Finnish (fin), French (fra), Galician (glg), Georgian (kat), German (deu), Greek (ell), Gujarati (guj), Hausa (hau), Hebrew (heb), Hindi (hin), Hungarian (hun), Icelandic (isl), Indonesian (ind), Irish (gle), Italian (ita), Japanese (jpn), Javanese (jav), Kannada (kan), Kazakh (kaz), Kirghiz (kir), Korean (kor), Latvian (lav), Lingala (lin), Lithuanian (lit), Luxembourgish (ltz), Macedonian (mkd), Malay (msa), Malayalam (mal), Mandarin Chinese (cmn), Marathi (mar), Nepali (nep), Norwegian (nor), Pashto (pus), Persian (fas), Polish (pol), Portuguese (por), Punjabi (pan), Romanian (ron), Russian (rus), Serbian (srp), Sindhi (snd), Slovak (slk), Slovenian (slv), Somali (som), Spanish (spa), Swahili (swa), Swedish (swe), Tamil (tam), Telugu (tel), Thai (tha), Turkish (tur), Ukrainian (ukr), Urdu (urd), Vietnamese (vie), Welsh (cym).*


## FAQ

<AccordionGroup>
  <Accordion title="Which models can I use?">
    Text to Dialogue is only available on the Eleven v3 model.
  </Accordion>

  <Accordion title="Do I own the audio output?">
    Yes. You retain ownership of any audio you generate. However, commercial usage rights are only
    available with paid plans. With a paid subscription, you may use generated audio for commercial
    purposes and monetize the outputs if you own the IP rights to the input content.
  </Accordion>

  <Accordion title="What qualifies as a free regeneration?">
    A free regeneration allows you to regenerate the same text to speech content without additional cost, subject to these conditions:

    * Only available within the ElevenLabs dashboard.
    * You can regenerate each piece of content up to 2 times for free.
    * The content must be exactly the same as the previous generation. Any changes to the text, voice settings, or other parameters will require a new, paid generation.

    Free regenerations are useful in case there is a slight distortion in the audio output. According to ElevenLabs' internal benchmarks, regenerations will solve roughly half of issues with quality, with remaining issues usually due to poor training data.
  </Accordion>

  <Accordion title="How many speakers can my dialogue have?">
    There is no limit to the number of speakers in a dialogue.
  </Accordion>

  <Accordion title="Why is my output sometimes inconsistent?">
    The models are nondeterministic. For consistency, use the optional [seed
    parameter](/docs/api-reference/text-to-speech/convert#request.body.seed), though subtle
    differences may still occur.
  </Accordion>

  <Accordion title="What's the best practice for large text conversions?">
    Split long text into segments and use streaming for real-time playback and efficient processing.
  </Accordion>
</AccordionGroup>



# Image & Video

> Generate and edit stunning images and videos from text prompts and visual references.


## Overview

Image & Video enables you to create high-quality visual content from simple text descriptions or reference images. Generate static images or dynamic videos in any style, then refine them iteratively with additional prompts, upscale for high-resolution output, and even add lip-sync with audio.

<Note>
  This feature is currently in beta.
</Note>

<CardGroup cols={2}>
  <Card title="Product guide" icon="duotone book-user" href="/docs/product-guides/playground/image-video">
    Complete guide to using Image & Video in ElevenLabs.
  </Card>
</CardGroup>


## Key capabilities

* **Image generation**: Create high-quality images from text prompts or reference images with models optimized for speed or quality
* **Video generation**: Generate dynamic videos with cinematic motion, physics realism, and integrated audio
* **Iterative refinement**: Refine generations with additional prompts and create variations
* **Enhancement tools**: Upscale resolution by up to 4x and apply realistic lip-sync with audio
* **Multiple models**: Access specialized models for different use cases, from rapid iteration to production-ready content
* **Reference support**: Guide generation with start frames, end frames, and style references. Supports a wide range of image file formats including JPG, PNG, WEBP, and more
* **Export flexibility**: Download as standalone files or import directly into Studio projects


## Workflow

The creation process moves you from inspiration to finished asset in four stages:

**Explore:** Discover community creations to find inspiration and study effective prompts.

**Generate:** Use the prompt box to describe what you want to create, select a model, and fine-tune settings.

**Iterate and enhance:** Review generations, create variations, and apply enhancements like upscaling and lip-syncing.

**Export:** Download finished assets or send them directly to Studio.


## Supported download formats

**Video:**

* **MP4**: Codecs H.264, H.265. Quality up to 4K (with upscaling)

**Image:**

* **PNG**: High-resolution, lossless output


## Models

Image & Video provides access to specialized models optimized for different use cases. Each model offers unique capabilities, from rapid iteration to production-ready quality.

Post-processing models require an existing generated output, though you can also upload your own image or video file.

<AccordionGroup>
  <Accordion title="Video generative models">
    <AccordionGroup>
      <Accordion title="OpenAI Sora 2 Pro">
        The most advanced, high-fidelity video model for cinematic results at your disposal.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame

        **Features:**

        * Highest-fidelity, professional-grade output with synced audio
        * Precise multi-shot control
        * Excels at complex motion and prompt adherence
        * Fixed durations: 4s, 8s, and 12s
        * Batch creation with up to 4 generations at a time

        **Output options:**

        * Resolutions: 720p, 1080p
        * Aspect ratios: 16:9, 9:16

        **Ideal for:**

        * Cinematic, professional-grade video content

        **Cost:** Starts at 12,000 credits for a generation

        <Note>
          End frame is not currently supported. Cannot provide image references. Sound is enabled by default.
        </Note>
      </Accordion>

      <Accordion title="OpenAI Sora 2">
        The standard, high-speed version of OpenAI's advanced video model, tuned for everyday content creation.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame

        **Features:**

        * Realistic, physics-aware videos with synced audio
        * Fine scene control
        * Fixed durations: 4s, 8s, and 12s
        * Batch creation with up to 4 generations at a time
        * Strong narrative and character consistency

        **Output options:**

        * Resolutions: 720p, 1080p
        * Aspect ratios: 16:9, 9:16

        **Ideal for:**

        * Everyday content creation with realistic physics

        **Cost:** Starts at 4,000 credits for default settings

        <Note>
          End frame is not currently supported. Cannot provide image references. Sound is enabled by default.
        </Note>
      </Accordion>

      <Accordion title="Google Veo 3.1">
        A professional-grade model for high-quality, cinematic video generation.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame
        * End Frame
        * Image References

        **Features:**

        * Excellent quality and creative control with negative prompts
        * Fully integrated and synchronized audio
        * Realistic dialogue, lip-sync, and sound effects
        * Fixed durations: 4s, 6s, and 8s
        * Batch creation with up to 4 generations at a time
        * Dedicated sound control

        **Output options:**

        * Resolutions: 720p, 1080p
        * Aspect ratios: 16:9, 9:16

        **Ideal for:**

        * High-quality, cinematic video generation with full creative control

        **Cost:** Starts at 8,000 credits for default settings

        <Note>
          Enabling and disabling sound will change the generation credits.
        </Note>
      </Accordion>

      <Accordion title="Kling 2.5">
        A balanced and versatile model for high-quality, full-HD video generation.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame

        **Features:**

        * Excels at simulating complex motion and realistic physics
        * Accurately models fluid dynamics and expressions
        * Fixed durations: 5s and 10s
        * Batch creation with up to 4 generations at a time

        **Output options:**

        * Resolutions: 1080p
        * Aspect ratios: 16:9, 1:1, 9:16

        **Ideal for:**

        * Realistic physics simulations and complex motion

        **Cost:** Starts at 3,500 credits for default settings

        <Note>
          End frame is not currently supported. Cannot provide image references. Sound control not available.
        </Note>
      </Accordion>

      <Accordion title="Google Veo 3.1 Fast">
        A high-speed model optimized for rapid previews and generations, delivering sharper visuals with lower latency.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame
        * End Frame

        **Features:**

        * Advanced creative control with negative prompts and dedicated sound control
        * Fixed durations: 4s, 6s, and 8s
        * Batch creation with up to 4 generations at a time
        * Accurately models real-world physics for realistic motion and interactions

        **Output options:**

        * Resolutions: 720p, 1080p
        * Aspect ratios: 16:9, 9:16

        **Ideal for:**

        * Quick iteration and A/B testing visuals
        * Fast-paced social media content creation

        **Cost:** Starts at 4,000 credits for default settings
      </Accordion>

      <Accordion title="Google Veo 3">
        Production-ready model delivering exceptional quality, strong physics realism, and coherent narrative audio.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame

        **Features:**

        * Advanced integrated "narrative audio" generation that matches video tone and story
        * Granular creative control with negative prompts and dedicated sound control
        * Fixed durations: 4s, 6s, and 8s
        * Batch creation with up to 4 generations at a time

        **Output options:**

        * Resolutions: 720p, 1080p
        * Aspect ratios: 16:9, 9:16

        **Ideal for:**

        * Final renders and professional marketing content
        * Short-form storytelling

        **Cost:** Starts at 8,000 credits for default settings
      </Accordion>

      <Accordion title="Google Veo 3 Fast">
        A high-speed, cost-efficient model for generating audio-backed video from text or a starting image.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame

        **Features:**

        * Granular creative control with negative prompts and dedicated sound control
        * Fixed durations: 4s, 6s, and 8s
        * Batch creation with up to 4 generations at a time

        **Output options:**

        * Resolutions: 720p, 1080p
        * Aspect ratios: 16:9, 9:16

        **Ideal for:**

        * Rapid iteration and previews
        * Cost-effective content creation

        **Cost:** Starts at 4,000 credits for default settings
      </Accordion>

      <Accordion title="Seedance 1 Pro">
        A specialized model for creating dynamic, multi-shot sequences with large movement and action.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame
        * End Frame

        **Features:**

        * Highly stable physics and seamless transitions between shots
        * Fixed durations: 3s, 4s, 5s, 6s, 7s, 8s, 9s, 10s, 11s, and 12s
        * Batch creation with up to 4 generations at a time
        * Maximum creative flexibility with numerous aspect ratio options

        **Output options:**

        * Resolutions: 480p, 720p, 1080p
        * Aspect ratios: 21:9, 16:9, 4:3, 1:1, 3:4, 9:16

        **Ideal for:**

        * Storytelling and action scenes requiring stable physics

        **Cost:** Starts at 4,800 credits for default settings

        <Note>
          Aspect ratio and resolution do not affect generation credits, but duration does.
        </Note>
      </Accordion>

      <Accordion title="Wan 2.5">
        A versatile model that delivers cinematic motion and high prompt fidelity from text or a starting image.

        **Generation inputs:**

        * Text-to-Video
        * Start Frame (Image-to-Video)

        **Features:**

        * Granular creative control with negative prompts and dedicated sound control
        * Fixed durations: 5s and 10s
        * Batch creation with up to 4 generations at a time

        **Output options:**

        * Resolutions: 480p, 720p, 1080p
        * Aspect ratios: 16:9, 1:1, 9:16

        **Ideal for:**

        * Cinematic content with strong prompt adherence

        **Cost:** Starts at 2,500 credits for default settings

        <Note>
          Generation cost varies based on selected settings.
        </Note>
      </Accordion>
    </AccordionGroup>
  </Accordion>

  <Accordion title="Image generative models">
    <AccordionGroup>
      <Accordion title="Google Nano Banana">
        A high-speed model for quick, high-quality image generation and editing directly from text prompts.

        **Features:**

        * Supports multiple image references to guide generation
        * Generates up to 4 images at a time

        **Output options:**

        * Aspect ratios: 21:9, 16:9, 5:4, 4:3, 3:2, 1:1, 2:3, 3:4, 4:5, 9:16

        **Ideal for:**

        * Rapid image creation and iteration

        **Cost:** Starts at 2,000 credits for default settings; varies based on number of generations
      </Accordion>

      <Accordion title="Seedream 4">
        A specialized image model for generating multi-shot sequences or scenes with large movement and action.

        **Features:**

        * Excels at creating images with stable physics and coherent transitions
        * Supports multiple image references to guide generation
        * Generates up to 4 images at a time

        **Output options:**

        * Aspect ratios: auto, 16:9, 4:3, 1:1, 3:4, 9:16

        **Ideal for:**

        * Action scenes and dynamic compositions

        **Cost:** Starts at 1,200 credits for default settings; varies based on number of generations
      </Accordion>

      <Accordion title="Flux 1 Kontext Pro">
        A professional model for advanced image generation and editing, offering strong scene coherence and style control.

        **Features:**

        * Image-based style control requiring a reference image to guide visual aesthetic
        * Generates up to 4 images at a time

        **Output options:**

        * Aspect ratios: 21:9, 16:9, 4:3, 3:2, 1:1, 2:3, 3:4, 4:5, 9:16, 9:21

        **Ideal for:**

        * Professional content with precise style requirements

        **Cost:** Starts at 1,600 credits; varies based on settings and number of generations
      </Accordion>

      <Accordion title="Wan 2.5">
        An image model with strong prompt fidelity and motion awareness, ideal for capturing dynamic action in a still frame.

        **Features:**

        * Granular control with negative prompts
        * Supports multiple image references to guide generation
        * Generates up to 4 images at a time

        **Output options:**

        * Aspect ratios: 16:9, 4:3, 1:1, 3:4, 9:16

        **Ideal for:**

        * Dynamic still images with motion awareness

        **Cost:** Starts at 2,000 credits; varies based on settings
      </Accordion>

      <Accordion title="OpenAI GPT Image 1">
        A versatile model for precise, high-quality image creation and detailed editing guided by natural language prompts.

        **Features:**

        * Supports multiple image references to guide generation
        * Generates up to 4 images at a time

        **Output options:**

        * Aspect ratios: 3:2, 1:1, 2:3
        * Quality options: low, medium, high

        **Ideal for:**

        * Creating and editing images with precise, text-based control

        **Cost:** Starts at 2,400 credits for default settings; varies based on settings and number of generations
      </Accordion>
    </AccordionGroup>
  </Accordion>

  <Accordion title="Lip-sync models">
    <AccordionGroup>
      <Accordion title="Omnihuman 1.5">
        A dedicated utility model for generating exceptionally realistic, humanlike lip-sync.

        **Inputs:**

        * Static source image
        * Speech audio file

        **Features:**

        * Animates the mouth on the source image to match provided audio
        * Creates high-fidelity "talking" video from still images
        * Lip-sync specific tool, not a full video generation model

        **Ideal for:**

        * Creating talking avatars
        * Adding dialogue to still images
        * Professional dubbing workflows

        **Cost:** Depends on generation input

        <Note>
          For best results, the image should contain a detectable figure.
        </Note>
      </Accordion>

      <Accordion title="Veed LipSync">
        A fast, affordable, and precise utility model for applying realistic lip-sync to videos.

        **Inputs:**

        * Source video
        * New speech audio file

        **Features:**

        * Re-animates mouth movements in source video to match new audio
        * Video-to-video lip-sync tool, not a full video generator

        **Ideal for:**

        * High-volume, cost-effective dubbing
        * Translating content
        * Correcting audio in video clips with realistic results

        **Cost:** Depends on generation input

        <Note>
          For best results, the video should contain a detectable figure.
        </Note>
      </Accordion>
    </AccordionGroup>
  </Accordion>

  <Accordion title="Upscaling model">
    <AccordionGroup>
      <Accordion title="Topaz Upscale">
        A dedicated utility model for image and video upscaling, designed to enhance resolution and detail up to 4x.

        **Features:**

        * Enhancement tool that processes existing media
        * Increases media size while preserving natural textures and minimizing artifacts
        * Highly granular upscale factors: 1x, 1.25x, 1.5x, 1.75x, 2x, 3x, 4x
        * Video-specific: Flexible frame rate control (keep source or convert to 24, 25, 30, 48, 50, or 60 fps)

        **Ideal for:**

        * Improving quality of generated media
        * Restoring legacy footage or photos
        * Preparing assets for high-resolution displays

        **Cost:** Depends on generation input
      </Accordion>
    </AccordionGroup>
  </Accordion>
</AccordionGroup>



# Voice changer

> Learn how to transform audio between voices while preserving emotion and delivery.


## Overview

ElevenLabs [voice changer](/docs/api-reference/speech-to-speech/convert) API lets you transform any source audio (recorded or uploaded) into a different, fully cloned voice without losing the performance nuances of the original. It’s capable of capturing whispers, laughs, cries, accents, and subtle emotional cues to achieve a highly realistic, human feel and can be used to:

* Change any voice while preserving emotional delivery and nuance
* Create consistent character voices across multiple languages and recording sessions
* Fix or replace specific words and phrases in existing recordings

<CardGroup cols={1}>
  <video width="100%" height="400" controls>
    <source src="https://eleven-public-cdn.elevenlabs.io/payloadcms/z2o584jt3pn-speech-to-speech-promo.mp4" type="video/mp4" />

    Your browser does not support the video tag.
  </video>
</CardGroup>

Explore our [voice library](https://elevenlabs.io/community) to find the perfect voice for your project.

<CardGroup cols={2}>
  <Card title="Developer quickstart" icon="duotone book-sparkles" href="/docs/cookbooks/voice-changer">
    Learn how to integrate voice changer into your application.
  </Card>

  <Card title="Product guide" icon="duotone book-user" href="/docs/product-guides/playground/voice-changer">
    Step-by-step guide for using voice changer in ElevenLabs.
  </Card>
</CardGroup>


## Supported languages

Our multilingual v2 models support 29 languages:

*English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian.*

The `eleven_english_sts_v2` model only supports English.


## Best practices

### Audio quality

* Record in a quiet environment to minimize background noise
* Maintain appropriate microphone levels - avoid too quiet or peaked audio
* Use `remove_background_noise=true` if environmental sounds are present

### Recording guidelines

* Keep segments under 5 minutes for optimal processing
* Feel free to include natural expressions (laughs, sighs, emotions)
* The source audio's accent and language will be preserved in the output

### Parameters

* **Style**: Set to 0% when input audio is already expressive
* **Stability**: Use 100% for maximum voice consistency
* **Language**: Choose source audio that matches your desired accent and language


## FAQ

<AccordionGroup>
  <Accordion title="Can I convert more than 5 minutes of audio?">
    Yes, but you must split it into smaller chunks (each under 5 minutes). This helps ensure stability
    and consistent output.
  </Accordion>

  <Accordion title="Can I use my own custom/cloned voice for output?">
    Absolutely. Provide your custom voice’s <code>voice\_id</code> and specify the correct{' '}
    <code>model\_id</code>.
  </Accordion>

  <Accordion title="How is billing handled?">
    You’re charged at 1000 characters’ worth of usage per minute of processed audio. There’s no
    additional fee based on file size.
  </Accordion>

  <Accordion title="Does the model reproduce background noise?">
    Possibly. Use <code>remove\_background\_noise=true</code> or the Voice Isolator tool to minimize
    environmental sounds in the final output.
  </Accordion>

  <Accordion title="Which model is best for English audio?">
    Though <code>eleven\_english\_sts\_v2</code> is available, our{' '}
    <code>eleven\_multilingual\_sts\_v2</code> model often outperforms it, even for English material.
  </Accordion>

  <Accordion title="How does style & stability work?">
    “Style” adds interpretative flair; “stability” enforces consistency. For high-energy performances
    in the source audio, turn style down and stability up.
  </Accordion>
</AccordionGroup>



# Voice isolator

> Learn how to isolate speech from background noise, music, and ambient sounds from any audio.


## Overview

ElevenLabs [voice isolator](/docs/api-reference/audio-isolation/audio-isolation) API transforms audio recordings with background noise into clean, studio-quality speech. This is particularly useful for audio recorded in noisy environments, or recordings containing unwanted ambient sounds, music, or other background interference.

Listen to a sample:

<CardGroup cols={2}>
  <elevenlabs-audio-player audio-title="Original audio" audio-src="https://eleven-public-cdn.elevenlabs.io/audio/voice-isolator/voice-isolator-promo-original.mp3" />

  <elevenlabs-audio-player audio-title="Isolated audio" audio-src="https://eleven-public-cdn.elevenlabs.io/audio/voice-isolator/voice-isolator-promo-isolated.mp3" />
</CardGroup>


## Usage

The voice isolator model extracts speech from background noise in both audio and video files.

<CardGroup cols={2}>
  <Card title="Developer quickstart" icon="duotone book-sparkles" href="/docs/cookbooks/voice-isolator">
    Learn how to integrate voice isolator into your application.
  </Card>

  <Card title="Product guide" icon="duotone book-user" href="/docs/product-guides/audio-tools/voice-isolator">
    Step-by-step guide for using voice isolator in ElevenLabs.
  </Card>
</CardGroup>

### Supported file types

* **Audio**: AAC, AIFF, OGG, MP3, OPUS, WAV, FLAC, M4A
* **Video**: MP4, AVI, MKV, MOV, WMV, FLV, WEBM, MPEG, 3GPP


## FAQ

* **Cost**: Voice isolator costs 1000 characters for every minute of audio.
* **File size and length**: Supports files up to 500MB and 1 hour in length.
* **Music vocals**: Not specifically optimized for isolating vocals from music, but may work depending on the content.



# Dubbing

> Learn how to translate audio and video while preserving the emotion, timing & tone of speakers.


## Overview

ElevenLabs [dubbing](/docs/api-reference/dubbing/create) API translates audio and video across 32 languages while preserving the emotion, timing, tone and unique characteristics of each speaker. Our model separates each speaker’s dialogue from the soundtrack, allowing you to recreate the original delivery in another language. It can be used to:

* Grow your addressable audience by 4x to reach international audiences
* Adapt existing material for new markets while preserving emotional nuance
* Offer content in multiple languages without re-recording voice talent

<CardGroup cols={1}>
  <video width="100%" height="400" controls>
    <source src="https://eleven-public-cdn.elevenlabs.io/payloadcms/zi2mer0h44p-dubbing-studio-demo.mp4" type="video/mp4" />

    Your browser does not support the video tag.
  </video>
</CardGroup>

We also offer a [fully managed dubbing service](https://elevenlabs.io/elevenstudios) for video and podcast creators.


## Usage

ElevenLabs dubbing can be used in three ways:

* **Dubbing Studio** in the user interface for fast, interactive control and editing
* **Programmatic integration** via our [API](/docs/api-reference/dubbing/create) for large-scale or automated workflows
* **Human-verified dubs via ElevenLabs Productions** - for more information, please reach out to [productions@elevenlabs.io](mailto:productions@elevenlabs.io)

The UI supports files up to **500MB** and **45 minutes**. The API supports files up to **1GB** and **2.5 hours**.

<CardGroup cols={2}>
  <Card title="Developer quickstart" icon="duotone book-sparkles" href="/docs/cookbooks/dubbing">
    Learn how to integrate dubbing into your application.
  </Card>

  <Card title="Product guide" icon="duotone book-user" href="/docs/product-guides/products/dubbing">
    Edit transcripts and translate videos step by step in Dubbing Studio.
  </Card>
</CardGroup>

### Key features

**Speaker separation**
Automatically detect multiple speakers, even with overlapping speech.

**Multi-language output**
Generate localized tracks in 32 languages.

**Preserve original voices**
Retain the speaker’s identity and emotional tone.

**Keep background audio**
Avoid re-mixing music, effects, or ambient sounds.

**Customizable transcripts**
Manually edit translations and transcripts as needed.

**Supported file types**
Videos and audio can be dubbed from various sources, including YouTube, X, TikTok, Vimeo, direct URLs, or file uploads.

**Video transcript and translation editing**
Our AI video translator lets you manually edit transcripts and translations to ensure your content is properly synced and localized. Adjust the voice settings to tune delivery, and regenerate speech segments until the output sounds just right.

<Note>
  A Creator plan or higher is required to dub audio files. For videos, a watermark option is
  available to reduce credit usage.
</Note>

### Cost

To reduce credit usage, you can:

* Dub only a selected portion of your file
* Use watermarks on video output (not available for audio)
* Fine-tune transcripts and regenerate individual segments instead of the entire clip

Refer to our [pricing page](https://elevenlabs.io/pricing) for detailed credit costs.


## List of supported languages for dubbing

| No | Language Name | Language Code |
| -- | ------------- | ------------- |
| 1  | English       | en            |
| 2  | Hindi         | hi            |
| 3  | Portuguese    | pt            |
| 4  | Chinese       | zh            |
| 5  | Spanish       | es            |
| 6  | French        | fr            |
| 7  | German        | de            |
| 8  | Japanese      | ja            |
| 9  | Arabic        | ar            |
| 10 | Russian       | ru            |
| 11 | Korean        | ko            |
| 12 | Indonesian    | id            |
| 13 | Italian       | it            |
| 14 | Dutch         | nl            |
| 15 | Turkish       | tr            |
| 16 | Polish        | pl            |
| 17 | Swedish       | sv            |
| 18 | Filipino      | fil           |
| 19 | Malay         | ms            |
| 20 | Romanian      | ro            |
| 21 | Ukrainian     | uk            |
| 22 | Greek         | el            |
| 23 | Czech         | cs            |
| 24 | Danish        | da            |
| 25 | Finnish       | fi            |
| 26 | Bulgarian     | bg            |
| 27 | Croatian      | hr            |
| 28 | Slovak        | sk            |
| 29 | Tamil         | ta            |


## FAQ

<AccordionGroup>
  <Accordion title="What content can I dub?">
    Dubbing can be performed on all types of short and long form video and audio content. We
    recommend dubbing content with a maximum of 9 unique speakers at a time to ensure a high-quality
    dub.
  </Accordion>

  <Accordion title="Does dubbing preserve the speaker's natural intonation?">
    Yes. Our models analyze each speaker’s original delivery to recreate the same tone, pace, and
    style in your target language.
  </Accordion>

  <Accordion title="What about overlapping speakers or background noise?">
    We use advanced source separation to isolate individual voices from ambient sound. Multiple
    overlapping speakers can be split into separate tracks.
  </Accordion>

  <Accordion title="Are there file size limits?">
    Via the user interface, the maximum file size is 1GB up to 45 minutes. Through the API, you can
    process files up to 1GB and 2.5 hours.
  </Accordion>

  <Accordion title="How do I handle fine-tuning or partial translations?">
    You can choose to dub only certain portions of your video/audio or tweak translations/voices in
    our interactive Dubbing Studio.
  </Accordion>
</AccordionGroup>



# Sound effects

> Learn how to create high-quality sound effects from text with ElevenLabs.


## Overview

ElevenLabs [sound effects](/docs/api-reference/text-to-sound-effects/convert) API turns text descriptions into high-quality audio effects with precise control over timing, style and complexity. The model understands both natural language and audio terminology, enabling you to:

* Generate cinematic sound design for films & trailers
* Create custom sound effects for games & interactive media
* Produce Foley and ambient sounds for video content

Listen to an example:

<elevenlabs-audio-player audio-title="Cinematic braam" audio-src="https://storage.googleapis.com/eleven-public-cdn/documentation_assets/audio/sfx-cinematic-braam.mp3" />


## Usage

Sound effects are generated using text descriptions & two optional parameters:

* **Duration**: Set a specific length for the generated audio (in seconds)

  * Default: Automatically determined based on the prompt
  * Range: 0.1 to 30 seconds
  * Cost: 40 credits per second when duration is specified

* **Looping**: Enable seamless looping for sound effects longer than 30 seconds

  * Creates sound effects that can be played on repeat without perceptible start/end points
  * Perfect for atmospheric sounds, ambient textures, and background elements
  * Example: Generate 30s of 'soft rain' then loop it endlessly for atmosphere in audiobooks, films, games

* **Prompt influence**: Control how strictly the model follows the prompt

  * High: More literal interpretation of the prompt
  * Low: More creative interpretation with added variations

<CardGroup cols={2}>
  <Card title="Developer quickstart" icon="duotone book-sparkles" href="/docs/cookbooks/sound-effects">
    Learn how to integrate sound effects into your application.
  </Card>

  <Card title="Product guide" icon="duotone book-user" href="/docs/product-guides/playground/sound-effects">
    Step-by-step guide for using sound effects in ElevenLabs.
  </Card>
</CardGroup>

### Prompting guide

#### Simple effects

For basic sound effects, use clear, concise descriptions:

* "Glass shattering on concrete"
* "Heavy wooden door creaking open"
* "Thunder rumbling in the distance"

<elevenlabs-audio-player audio-title="Wood chopping" audio-src="https://storage.googleapis.com/eleven-public-cdn/documentation_assets/audio/sfx-wood-chopping.mp3" />

#### Complex sequences

For multi-part sound effects, describe the sequence of events:

* "Footsteps on gravel, then a metallic door opens"
* "Wind whistling through trees, followed by leaves rustling"
* "Sword being drawn, then clashing with another blade"

<elevenlabs-audio-player audio-title="Walking and then falling" audio-src="https://storage.googleapis.com/eleven-public-cdn/documentation_assets/audio/sfx-walking-falling.mp3" />

#### Musical elements

The API also supports generation of musical components:

* "90s hip-hop drum loop, 90 BPM"
* "Vintage brass stabs in F minor"
* "Atmospheric synth pad with subtle modulation"

<elevenlabs-audio-player audio-title="90s drum loop" audio-src="https://storage.googleapis.com/eleven-public-cdn/documentation_assets/audio/sfx-90s-drum-loop.mp3" />

#### Audio Terminology

Common terms that can enhance your prompts:

* **Impact**: Collision or contact sounds between objects, from subtle taps to dramatic crashes
* **Whoosh**: Movement through air effects, ranging from fast and ghostly to slow-spinning or rhythmic
* **Ambience**: Background environmental sounds that establish atmosphere and space
* **One-shot**: Single, non-repeating sound
* **Loop**: Repeating audio segment
* **Stem**: Isolated audio component
* **Braam**: Big, brassy cinematic hit that signals epic or dramatic moments, common in trailers
* **Glitch**: Sounds of malfunction, jittering, or erratic movement, useful for transitions and sci-fi
* **Drone**: Continuous, textured sound that creates atmosphere and suspense


## FAQ

<AccordionGroup>
  <Accordion title="What's the maximum duration for generated effects?">
    The maximum duration is 30 seconds per generation. For longer sequences, you can either generate
    multiple effects and combine them, or use the looping feature to create seamless repeating sound
    effects.
  </Accordion>

  <Accordion title="Can I generate music with this API?">
    Yes, you can generate musical elements like drum loops, bass lines, and melodic samples.
    However, for full music production, consider combining multiple generated elements.
  </Accordion>

  <Accordion title="How do I ensure consistent quality?">
    Use detailed prompts, appropriate duration settings, and high prompt influence for more
    predictable results. For complex sounds, generate components separately and combine them.
  </Accordion>

  <Accordion title="What audio formats are supported?">
    Generated audio is provided in MP3 format with professional-grade quality. For WAV downloads of
    non-looping sound effects, audio is delivered at 48kHz sample rate - the industry standard for
    film, TV, video, and game audio, ensuring no resampling is needed for professional workflows.
  </Accordion>

  <Accordion title="How do looping sound effects work?">
    Looping sound effects are designed to play seamlessly on repeat without noticeable start or end
    points. This is perfect for creating continuous atmospheric sounds, ambient textures, or
    background elements that need to play indefinitely. For example, you can generate 30 seconds of
    rain sounds and loop them endlessly for background atmosphere in audiobooks, films, or games.
  </Accordion>
</AccordionGroup>



# Voices

> Learn how to create, customize, and manage voices with ElevenLabs.


## Overview

ElevenLabs provides models for voice creation & customization. The platform supports a wide range of voice options, including voices from our extensive [voice library](https://elevenlabs.io/app/voice-library), voice cloning, and artificially designed voices using text prompts.

### Voice types

* **Community**: Voices shared by the community from the ElevenLabs [voice library](/docs/product-guides/voices/voice-library).
* **Cloned**: Custom voices created using instant or professional [voice cloning](/docs/product-guides/voices/voice-cloning).
* **Voice design**: Artificially designed voices created with the [voice design](/docs/product-guides/voices/voice-design) tool.
* **Default**: Pre-designed, high-quality voices optimized for general use.

<Tip>
  Voices that you personally own, either created with Instant Voice Cloning, Professional Voice
  Cloning, or Voice Design, can be used for [Voice Remixing](/docs/capabilities/voice-remixing).
</Tip>

#### Community

The [voice library](/docs/product-guides/voices/voice-library) contains over 5,000 voices shared by the ElevenLabs community. Use it to:

* Discover unique voices shared by the ElevenLabs community.
* Add voices to your personal collection.
* Share your own voice clones for cash rewards when others use it.

<Success>
  Share your voice with the community, set your terms, and earn cash rewards when others use it.
  We've paid out over **\$1M** already.
</Success>

<CardGroup cols={1}>
  <Card title="Product guide" icon="duotone book-user" iconPosition="left" href="/docs/product-guides/voices/voice-library">
    Learn how to use voices from the voice library
  </Card>
</CardGroup>

#### Cloned

Clone your own voice from 30-second samples with Instant Voice Cloning, or create hyper-realistic voices using Professional Voice Cloning.

* **Instant Voice Cloning**: Quickly replicate a voice from short audio samples.
* **Professional Voice Cloning**: Generate professional-grade voice clones with extended training audio.

Voice-captcha technology is used to verify that **all** voice clones are created from your own voice samples.

<Note>
  A Creator plan or higher is required to create voice clones.
</Note>

<CardGroup cols={2}>
  <Card title="Instant Voice Cloning developer quickstart" icon="duotone book-sparkles" href="/docs/cookbooks/voices/instant-voice-cloning">
    Clone a voice instantly
  </Card>

  <Card title="Professional Voice Cloning developer quickstart" icon="duotone book-sparkles" href="/docs/cookbooks/voices/professional-voice-cloning">
    Create a perfect voice clone
  </Card>

  <Card title="Product guide" icon="duotone book-user" iconPosition="left" href="/docs/product-guides/voices/voice-cloning">
    Learn how to create instant & professional voice clones
  </Card>
</CardGroup>

#### Voice design

With [Voice Design](/docs/product-guides/voices/voice-design), you can create entirely new voices by specifying attributes like age, gender, accent, and tone. Generated voices are ideal for:

* Realistic voices with nuanced characteristics.
* Creative character voices for games and storytelling.

The voice design tool creates 3 voice previews, simply provide:

* A **voice description** between 20 and 1000 characters.
* A **text** to preview the voice between 100 and 1000 characters.

##### Voice design with Eleven v3 (alpha)

Using the new [Eleven v3 model](/docs/models#eleven-v3-alpha), voices that are capable of a wide range of emotion can be designed via a prompt.

Using v3 gets you the following benefits:

* More natural and versatile voice generation.
* Better control over voice characteristics.
* Audio tags supported in Preview generations.
* Backward compatibility with v2 models.

<CardGroup cols={2}>
  <Card title="Voice Design Developer quickstart" icon="duotone book-sparkles" href="/docs/cookbooks/voices/voice-design">
    Integrate voice design into your application.
  </Card>

  <Card title="Product guide" icon="duotone book-user" href="/docs/product-guides/voices/voice-design">
    Learn how to craft voices from a single prompt.
  </Card>
</CardGroup>

#### Default

Our curated set of default voices is optimized for core use cases. These voices are:

* **Reliable**: Available long-term.
* **Consistent**: Carefully crafted and quality-checked for performance.
* **Model-ready**: Fine-tuned on new models upon release.

<Info>
  Default voices are available to all users via the **my voices** tab in the [voice lab
  dashboard](https://elevenlabs.io/app/voice-lab). Default voices were previously referred to as
  `premade` voices. The latter term is still used when accessing default voices via the API.
</Info>

### Managing voices

All voices can be managed through **My Voices**, where you can:

* Search, filter, and categorize voices
* Add descriptions and custom tags
* Organize voices for quick access

Learn how to manage your voice collection in [My Voices documentation](/docs/product-guides/voices/voice-library).

* **Search and Filter**: Find voices using keywords or tags.
* **Preview Samples**: Listen to voice demos before adding them to **My Voices**.
* **Add to Collection**: Save voices for easy access in your projects.

> **Tip**: Try searching by specific accents or genres, such as "Australian narration" or "child-like character."

### Supported languages

All ElevenLabs voices support multiple languages. Experiment by converting phrases like `Hello! こんにちは! Bonjour!` into speech to hear how your own voice sounds across different languages.

ElevenLabs supports voice creation in 32 languages. Match your voice selection to your target region for the most natural results.

* **Default Voices**: Optimized for multilingual use.
* **Generated and Cloned Voices**: Accent fidelity depends on input samples or selected attributes.

Our multilingual v2 models support 29 languages:

*English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian.*

Flash v2.5 supports 32 languages - all languages from v2 models plus:

*Hungarian, Norwegian & Vietnamese*

[Learn more about our models](/docs/models)


## FAQ

<AccordionGroup>
  <Accordion title="Can I create a custom voice?">
    Yes, you can create custom voices with Voice Design or clone voices using Instant or
    Professional Voice Cloning. Both options are accessible in **My Voices**.
  </Accordion>

  <Accordion title="What is the difference between Instant and Professional Voice Cloning?">
    Instant Voice Cloning uses short audio samples for near-instantaneous voice creation.
    Professional Voice Cloning requires longer samples but delivers hyper-realistic, high-quality
    results.
  </Accordion>

  <Accordion title="Can I share my created voices?">
    Professional Voice Clones can be shared privately or publicly in the Voice Library. Generated
    voices and Instant Voice Clones cannot currently be shared.
  </Accordion>

  <Accordion title="How do I manage my voices?">
    Use **My Voices** to search, filter, and organize your voice collection. You can also delete,
    tag, and categorize voices for easier management.
  </Accordion>

  <Accordion title="How can I ensure my cloned voice matches the original?">
    Use clean and consistent audio samples. For Professional Voice Cloning, provide a variety of
    recordings in the desired speaking style.
  </Accordion>

  <Accordion title="Can I share voices I create?">
    Yes, Professional Voice Clones can be shared in the Voice Library. Instant Voice Clones and
    Generated Voices cannot currently be shared.
  </Accordion>

  <Accordion title="What are some common use cases for Generated Voices?">
    Generated Voices are ideal for unique characters in games, animations, and creative
    storytelling.
  </Accordion>

  <Accordion title="How do I access the Voice Library?">
    Go to **Voices > Voice Library** in your dashboard or access it via API.
  </Accordion>
</AccordionGroup>



---
**Navigation:** [← Previous](./02-august-11-2025.md) | [Index](./index.md) | [Next →](./04-voice-remixing.md)
