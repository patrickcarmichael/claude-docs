---
title: "Anthropic Documentation"
description: "Formatted documentation for Anthropic"
source: "llms-full.txt"
last_updated: "2025-11-08"
---

## How to use extended thinking

Here is an example of using extended thinking in the Messages API:
```bash
  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data \
  '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 16000,
      "thinking": {
          "type": "enabled",
          "budget_tokens": 10000
      },
      "messages": [
          {
              "role": "user",
              "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
          }
      ]
  }'
```
```python
  import anthropic

  client = anthropic.Anthropic()

  response = client.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=16000,
      thinking={
          "type": "enabled",
          "budget_tokens": 10000
      },
      messages=[{
          "role": "user",
          "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
      }]
  )

  # The response will contain summarized thinking blocks and text blocks

  for block in response.content:
      if block.type == "thinking":
          print(f"\nThinking summary: {block.thinking}")
      elif block.type == "text":
          print(f"\nResponse: {block.text}")
```
```typescript
  import Anthropic from '@anthropic-ai/sdk';

  const client = new Anthropic();

  const response = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 16000,
    thinking: {
      type: "enabled",
      budget_tokens: 10000
    },
    messages: [{
      role: "user",
      content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
  });

  // The response will contain summarized thinking blocks and text blocks
  for (const block of response.content) {
    if (block.type === "thinking") {
      console.log(`\nThinking summary: ${block.thinking}`);
    } else if (block.type === "text") {
      console.log(`\nResponse: ${block.text}`);
    }
  }
```
```java
  import com.anthropic.client.AnthropicClient;
  import com.anthropic.client.okhttp.AnthropicOkHttpClient;
  import com.anthropic.models.beta.messages.*;
  import com.anthropic.models.beta.messages.MessageCreateParams;
  import com.anthropic.models.messages.*;

  public class SimpleThinkingExample {
      public static void main(String[] args) {
          AnthropicClient client = AnthropicOkHttpClient.fromEnv();

          BetaMessage response = client.beta().messages().create(
                  MessageCreateParams.builder()
                          .model(Model.CLAUDE_OPUS_4_0)
                          .maxTokens(16000)
                          .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                          .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                          .build()
          );

          System.out.println(response);
      }
  }
```

To turn on extended thinking, add a `thinking` object, with the `type` parameter set to `enabled` and the `budget_tokens` to a specified token budget for extended thinking.

The `budget_tokens` parameter determines the maximum number of tokens Claude is allowed to use for its internal reasoning process. In Claude 4 models, this limit applies to full thinking tokens, and not to [the summarized output](#summarized-thinking). Larger budgets can improve response quality by enabling more thorough analysis for complex problems, although Claude may not use the entire budget allocated, especially at ranges above 32k.

`budget_tokens` must be set to a value less than `max_tokens`. However, when using [interleaved thinking with tools](#interleaved-thinking), you can exceed this limit as the token limit becomes your entire context window (200k tokens).

### Summarized thinking

With extended thinking enabled, the Messages API for Claude 4 models returns a summary of Claude's full thinking process. Summarized thinking provides the full intelligence benefits of extended thinking, while preventing misuse.

Here are some important considerations for summarized thinking:

* You're charged for the full thinking tokens generated by the original request, not the summary tokens.
* The billed output token count will **not match** the count of tokens you see in the response.
* The first few lines of thinking output are more verbose, providing detailed reasoning that's particularly helpful for prompt engineering purposes.
* As Anthropic seeks to improve the extended thinking feature, summarization behavior is subject to change.
* Summarization preserves the key ideas of Claude's thinking process with minimal added latency, enabling a streamable user experience and easy migration from Claude Sonnet 3.7 to Claude 4 models.
* Summarization is processed by a different model than the one you target in your requests. The thinking model does not see the summarized output.

>   **ðŸ“ Note**
>
> Claude Sonnet 3.7 continues to return full thinking output.

  In rare cases where you need access to full thinking output for Claude 4 models, [contact our sales team](mailto:sales@anthropic.com).

### Streaming thinking

You can stream extended thinking responses using [server-sent events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents).

When streaming is enabled for extended thinking, you receive thinking content via `thinking_delta` events.

For more documention on streaming via the Messages API, see [Streaming Messages](/en/docs/build-with-claude/streaming).

Here's how to handle streaming with thinking:
```bash
  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data \
  '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 16000,
      "stream": true,
      "thinking": {
          "type": "enabled",
          "budget_tokens": 10000
      },
      "messages": [
          {
              "role": "user",
              "content": "What is 27 * 453?"
          }
      ]
  }'
```
```python
  import anthropic

  client = anthropic.Anthropic()

  with client.messages.stream(
      model="claude-sonnet-4-5",
      max_tokens=16000,
      thinking={"type": "enabled", "budget_tokens": 10000},
      messages=[{"role": "user", "content": "What is 27 * 453?"}],
  ) as stream:
      thinking_started = False
      response_started = False

      for event in stream:
          if event.type == "content_block_start":
              print(f"\nStarting {event.content_block.type} block...")
              # Reset flags for each new block

              thinking_started = False
              response_started = False
          elif event.type == "content_block_delta":
              if event.delta.type == "thinking_delta":
                  if not thinking_started:
                      print("Thinking: ", end="", flush=True)
                      thinking_started = True
                  print(event.delta.thinking, end="", flush=True)
              elif event.delta.type == "text_delta":
                  if not response_started:
                      print("Response: ", end="", flush=True)
                      response_started = True
                  print(event.delta.text, end="", flush=True)
          elif event.type == "content_block_stop":
              print("\nBlock complete.")
```
```typescript
  import Anthropic from '@anthropic-ai/sdk';

  const client = new Anthropic();

  const stream = await client.messages.stream({
    model: "claude-sonnet-4-5",
    max_tokens: 16000,
    thinking: {
      type: "enabled",
      budget_tokens: 10000
    },
    messages: [{
      role: "user",
      content: "What is 27 * 453?"
    }]
  });

  let thinkingStarted = false;
  let responseStarted = false;

  for await (const event of stream) {
    if (event.type === 'content_block_start') {
      console.log(`\nStarting ${event.content_block.type} block...`);
      // Reset flags for each new block
      thinkingStarted = false;
      responseStarted = false;
    } else if (event.type === 'content_block_delta') {
      if (event.delta.type === 'thinking_delta') {
        if (!thinkingStarted) {
          process.stdout.write('Thinking: ');
          thinkingStarted = true;
        }
        process.stdout.write(event.delta.thinking);
      } else if (event.delta.type === 'text_delta') {
        if (!responseStarted) {
          process.stdout.write('Response: ');
          responseStarted = true;
        }
        process.stdout.write(event.delta.text);
      }
    } else if (event.type === 'content_block_stop') {
      console.log('\nBlock complete.');
    }
  }
```
```java
  import com.anthropic.client.AnthropicClient;
  import com.anthropic.client.okhttp.AnthropicOkHttpClient;
  import com.anthropic.core.http.StreamResponse;
  import com.anthropic.models.beta.messages.MessageCreateParams;
  import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
  import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
  import com.anthropic.models.messages.Model;

  public class SimpleThinkingStreamingExample {
      private static boolean thinkingStarted = false;
      private static boolean responseStarted = false;
      
      public static void main(String[] args) {
          AnthropicClient client = AnthropicOkHttpClient.fromEnv();

          MessageCreateParams createParams = MessageCreateParams.builder()
                  .model(Model.CLAUDE_OPUS_4_0)
                  .maxTokens(16000)
                  .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                  .addUserMessage("What is 27 * 453?")
                  .build();

          try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                       client.beta().messages().createStreaming(createParams)) {
              streamResponse.stream()
                      .forEach(event -> {
                          if (event.isContentBlockStart()) {
                              System.out.printf("\nStarting %s block...%n",
                                      event.asContentBlockStart()._type());
                              // Reset flags for each new block
                              thinkingStarted = false;
                              responseStarted = false;
                          } else if (event.isContentBlockDelta()) {
                              var delta = event.asContentBlockDelta().delta();
                              if (delta.isBetaThinking()) {
                                  if (!thinkingStarted) {
                                      System.out.print("Thinking: ");
                                      thinkingStarted = true;
                                  }
                                  System.out.print(delta.asBetaThinking().thinking());
                                  System.out.flush();
                              } else if (delta.isBetaText()) {
                                  if (!responseStarted) {
                                      System.out.print("Response: ");
                                      responseStarted = true;
                                  }
                                  System.out.print(delta.asBetaText().text());
                                  System.out.flush();
                              }
                          } else if (event.isContentBlockStop()) {
                              System.out.println("\nBlock complete.");
                          }
                      });
          }
      }
  }
```

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
  Try in Console
</TryInConsoleButton>

Example streaming output:
```json
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "Let me solve this step by step:\n\n1. First break down 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// Additional thinking deltas...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// Additional text deltas...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```
>   **ðŸ“ Note**
>
> When using streaming with thinking enabled, you might notice that text sometimes arrives in larger chunks alternating with smaller, token-by-token delivery. This is expected behavior, especially for thinking content.

  The streaming system needs to process content in batches for optimal performance, which can result in this "chunky" delivery pattern, with possible delays between streaming events. We're continuously working to improve this experience, with future updates focused on making thinking content stream more smoothly.

---

**ðŸ“š [Back to Index](./index.md)** | **ðŸ“„ [Full Version](./documentation.md)** | **ðŸ”— [Original](../llms-full.txt)**
