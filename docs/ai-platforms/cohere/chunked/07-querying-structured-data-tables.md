**Navigation:** [← Previous](./06-build-an-onboarding-assistant-with-cohere.md) | [Index](./index.md) | [Next →](./08-teams-and-roles-on-the-cohere-platform.md)

---

# Querying Structured Data (Tables)

> Build an agentic RAG system that can query structured data (tables).

<a target="_blank" href="https://colab.research.google.com/github/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/agentic-rag/agentic_rag_pt5_structured_data_tables.ipynb">
  Open in Colab
</a>

In the previous tutorials, we explored how to build agentic RAG applications over unstructured and semi-structured data. In this tutorial and the next, we'll turn our focus to structured data.

This tutorial focuses on querying tables, and the next tutorial will be about querying SQL databases.

Consider a scenario where you have a CSV file containing evaluation results for an LLM application.

A user might ask questions like "What's the average score for a specific use case?" or "Which configuration has the lowest latency?". These queries require not just retrieval, but also data analysis and interpretation.

In this tutorial, we'll cover:

* Creating a function to execute Python code
* Setting up a tool to interact with tabular data
* Building an agent for querying tabular data
* Running the agent

Let's get started by setting up our environment and defining the necessary tools for our agent.

## Setup

To get started, first we need to install the `cohere` library and create a Cohere client.

```python PYTHON
! pip install cohere pandas -qq
```

```python PYTHON
import json
import os
import cohere

co = cohere.ClientV2(
    "COHERE_API_KEY"
)  # Get your free API key: https://dashboard.cohere.com/api-keys
```

And here's the data we'll be working with. `evaluation_results.csv` is a CSV file containing evaluation results for a set of LLM applications - name extraction, email drafting, and article summarization.

The file has the following columns:

* `usecase`: The use case.
* `run`: The run ID.
* `score`: The evaluation score for a particular run.
* `temperature`: The temperature setting of the model for a particular run.
* `tokens`: The number of tokens generated by the model for a particular run.
* `latency`: The latency of the model's response for a particular run.

<Note>
   Important: the data can be 

  [found here](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/agentic-rag/evaluation_results.csv)

  . Make sure to have the file in the same directory as this notebook for the imports to work correctly. 
</Note>

```python PYTHON
import pandas as pd

df = pd.read_csv("evaluation_results.csv")

df.head()
```

<div>
  <table border="1" class="dataframe fern-table">
    <thead>
      <tr>
        <th />

        <th>
          usecase
        </th>

        <th>
          run
        </th>

        <th>
          score
        </th>

        <th>
          temperature
        </th>

        <th>
          tokens
        </th>

        <th>
          latency
        </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <th>
          0
        </th>

        <td>
          extract_names
        </td>

        <td>
          A
        </td>

        <td>
          0.5
        </td>

        <td>
          0.3
        </td>

        <td>
          103
        </td>

        <td>
          1.12
        </td>
      </tr>

      <tr>
        <th>
          1
        </th>

        <td>
          draft_email
        </td>

        <td>
          A
        </td>

        <td>
          0.6
        </td>

        <td>
          0.3
        </td>

        <td>
          252
        </td>

        <td>
          2.50
        </td>
      </tr>

      <tr>
        <th>
          2
        </th>

        <td>
          summarize_article
        </td>

        <td>
          A
        </td>

        <td>
          0.8
        </td>

        <td>
          0.3
        </td>

        <td>
          350
        </td>

        <td>
          4.20
        </td>
      </tr>

      <tr>
        <th>
          3
        </th>

        <td>
          extract_names
        </td>

        <td>
          B
        </td>

        <td>
          0.2
        </td>

        <td>
          0.3
        </td>

        <td>
          101
        </td>

        <td>
          2.85
        </td>
      </tr>

      <tr>
        <th>
          4
        </th>

        <td>
          draft_email
        </td>

        <td>
          B
        </td>

        <td>
          0.4
        </td>

        <td>
          0.3
        </td>

        <td>
          230
        </td>

        <td>
          3.20
        </td>
      </tr>
    </tbody>
  </table>
</div>

## Creating a function to execute Python code

Here, we introduce a new tool that allows the agent to execute Python code and return the result. The agent will use this tool to generate pandas code for querying the data.

To create this tool, we'll use the `PythonREPL` class from the `langchain_experimental.utilities` module. This class provides a sandboxed environment for executing Python code and returns the result.

First, we define a `python_tool` that uses the `PythonREPL` class to execute Python code and return the result.

Next, we define a `ToolInput` class to handle the input for the `python_tool`.

Finally, we create a function `analyze_evaluation_results` that takes a string of Python code as input, executes the code using the Python tool we created, and returns the result.

**IMPORTANT:**

The source code for tool definitions can be [found here](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/agentic-rag/tool_def.py). Make sure to have the `tool_def.py` file in the same directory as this notebook for the imports to work correctly.

```python PYTHON
from tool_def import (
    analyze_evaluation_results,
    analyze_evaluation_results_tool,
)
```

```python PYTHON
functions_map = {
    "analyze_evaluation_results": analyze_evaluation_results
}
```

## Setting up a tool to interact with tabular data

Next, we define the `analyze_evaluation_results` tool. There are many ways we can set up a tool to work with CSV data, and in this example, we are using the tool description to provide the agent with the necessary context for working with the CSV file, such as:

* the name of the CSV file to load
* the columns of the CSV file
* additional instructions on what libraries to use (in this case, `pandas`)

The parameter of this tool is the `code` string containing the Python code that the agent writes to analyze the data.

```python PYTHON
analyze_evaluation_results_tool = {
    "type": "function",
    "function": {
        "name": "analyze_evaluation_results",
        "description": "Generate Python code using the pandas library to analyze evaluation results from a dataframe called `evaluation_results`. The dataframe has columns 'usecase','run','score','temperature','tokens', and 'latency'. You must start with `import pandas as pd` and read a CSV file called `evaluation_results.csv` into the `evaluation_results` dataframe.",
        "parameters": {
            "type": "object",
            "properties": {
                "code": {
                    "type": "string",
                    "description": "Executable Python code",
                }
            },
            "required": ["code"],
        },
    },
}
```

```python PYTHON
tools = [analyze_evaluation_results_tool]
```

## Building an agent for querying tabular data

Next, let's create a `run_agent` function to run the agentic RAG workflow, the same as in Part 1.

The only change we are making here is to make the system message simpler and more specific since the agent now only has one tool.

```python PYTHON
system_message = """## Task and Context
ou are an assistant who helps developers analyze LLM application evaluation results from a CSV files."""
```

```python PYTHON
model = "command-a-03-2025"


def run_agent(query, messages=None):
    if messages is None:
        messages = []

    if "system" not in {m.get("role") for m in messages}:
        messages.append({"role": "system", "content": system_message})

    # Step 1: get user message
    print(f"Question:\n{query}")
    print("=" * 50)

    messages.append({"role": "user", "content": query})

    # Step 2: Generate tool calls (if any)
    response = co.chat(
        model=model, messages=messages, tools=tools, temperature=0.3
    )

    while response.message.tool_calls:

        print("TOOL PLAN:")
        print(response.message.tool_plan, "\n")
        print("TOOL CALLS:")
        for tc in response.message.tool_calls:
            if tc.function.name == "analyze_evaluation_results":
                print(f"Tool name: {tc.function.name}")
                tool_call_prettified = print(
                    "\n".join(
                        f"  {line}"
                        for line_num, line in enumerate(
                            json.loads(tc.function.arguments)[
                                "code"
                            ].splitlines()
                        )
                    )
                )
                print(tool_call_prettified)
            else:
                print(
                    f"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}"
                )
        print("=" * 50)

        messages.append(
            {
                "role": "assistant",
                "tool_calls": response.message.tool_calls,
                "tool_plan": response.message.tool_plan,
            }
        )

        # Step 3: Get tool results
        for tc in response.message.tool_calls:
            tool_result = functions_map[tc.function.name](
                **json.loads(tc.function.arguments)
            )
            tool_content = [
                (
                    {
                        "type": "document",
                        "document": {"data": json.dumps(tool_result)},
                    }
                )
            ]

            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tc.id,
                    "content": tool_content,
                }
            )

        # Step 4: Generate response and citations
        response = co.chat(
            model=model,
            messages=messages,
            tools=tools,
            temperature=0.3,
        )

    messages.append(
        {
            "role": "assistant",
            "content": response.message.content[0].text,
        }
    )

    # Print final response
    print("RESPONSE:")
    print(response.message.content[0].text)
    print("=" * 50)

    # Print citations (if any)
    verbose_source = (
        False  # Change to True to display the contents of a source
    )
    if response.message.citations:
        print("CITATIONS:\n")
        for citation in response.message.citations:
            print(
                f"Start: {citation.start}| End:{citation.end}| Text:'{citation.text}' "
            )
            print("Sources:")
            for idx, source in enumerate(citation.sources):
                print(f"{idx+1}. {source.id}")
                if verbose_source:
                    print(f"{source.tool_output}")
            print("\n")

    return messages
```

## Running the agent

Let's ask the agent a few questions, starting with this query about the average evaluation score in run A.

To answer this query, the agent needs to write Python code that uses the pandas library to calculate the average evaluation score in run A. And it gets the answer right.

```python PYTHON
messages = run_agent("What's the average evaluation score in run A")
# Answer: 0.63
```

```mdx
Question:
What's the average evaluation score in run A
==================================================


Python REPL can execute arbitrary code. Use with caution.


TOOL PLAN:
I will write and execute Python code to calculate the average evaluation score in run A. 

TOOL CALLS:
Tool name: analyze_evaluation_results
    import pandas as pd
    
    df = pd.read_csv("evaluation_results.csv")
    
    # Calculate the average evaluation score in run A
    average_score_run_A = df[df["run"] == "A"]["score"].mean()
    
    print(f"Average evaluation score in run A: {average_score_run_A}")
None
==================================================
RESPONSE:
The average evaluation score in run A is 0.63.
==================================================
CITATIONS:

Start: 41| End:46| Text:'0.63.' 
Sources:
1. analyze_evaluation_results_phqpwwat2hgf:0
```

Next, we ask a slightly more complex question, this time about the latency of the highest-scoring run for one use case. This requires the agent to filter based on the use case, find the highest-scoring run, and return the latency value.

```python PYTHON
messages = run_agent(
    "What's the latency of the highest-scoring run for the summarize_article use case?"
)
# Answer: 4.8
```

```mdx

Question:
What's the latency of the highest-scoring run for the summarize_article use case?
==================================================
TOOL PLAN:
I will write Python code to find the latency of the highest-scoring run for the summarize_article use case. 

TOOL CALLS:
Tool name: analyze_evaluation_results
    import pandas as pd
    
    df = pd.read_csv("evaluation_results.csv")
    
    # Filter for the summarize_article use case
    use_case_df = df[df["usecase"] == "summarize_article"]
    
    # Find the highest-scoring run
    highest_score_run = use_case_df.loc[use_case_df["score"].idxmax()]
    
    # Get the latency of the highest-scoring run
    latency = highest_score_run["latency"]
    
    print(f"Latency of the highest-scoring run: {latency}")
None
==================================================
RESPONSE:
The latency of the highest-scoring run for the summarize_article use case is 4.8.
==================================================
CITATIONS:

Start: 77| End:81| Text:'4.8.' 
Sources:
1. analyze_evaluation_results_es3hnnnp5pey:0
```

Next, we ask a question to compare the use cases in terms of token usage, and to show a markdown table to show the comparison.

```python PYTHON
messages = run_agent(
    "Which use case uses the least amount of tokens on average? Show the comparison of all use cases in a markdown table."
)
# Answer: extract_names (106.25), draft_email (245.75), summarize_article (355.75)
```

```mdx

Question:
Which use case uses the least amount of tokens on average? Show the comparison of all use cases in a markdown table.
==================================================
TOOL PLAN:
I will use the analyze_evaluation_results tool to generate Python code to find the use case that uses the least amount of tokens on average. I will also generate code to create a markdown table to compare all use cases. 

TOOL CALLS:
Tool name: analyze_evaluation_results
    import pandas as pd
    
    evaluation_results = pd.read_csv("evaluation_results.csv")
    
    # Group by 'usecase' and calculate the average tokens
    avg_tokens_by_usecase = evaluation_results.groupby('usecase')['tokens'].mean()
    
    # Find the use case with the least average tokens
    least_avg_tokens_usecase = avg_tokens_by_usecase.idxmin()
    
    print(f"Use case with the least average tokens: {least_avg_tokens_usecase}")
    
    # Create a markdown table comparing average tokens for all use cases
    markdown_table = avg_tokens_by_usecase.reset_index()
    markdown_table.columns = ["Use Case", "Average Tokens"]
    print(markdown_table.to_markdown(index=False))
None
==================================================
RESPONSE:
The use case that uses the least amount of tokens on average is extract_names.

Here is a markdown table comparing the average tokens for all use cases:

| Use Case | Average Tokens |
|:-------------------------|-------------------------------:|
| draft_email | 245.75 |
| extract_names | 106.25 |
| summarize_article | 355.75 |
==================================================
CITATIONS:

Start: 64| End:78| Text:'extract_names.' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0


Start: 156| End:164| Text:'Use Case' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0


Start: 167| End:181| Text:'Average Tokens' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0


Start: 248| End:259| Text:'draft_email' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0


Start: 262| End:268| Text:'245.75' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0


Start: 273| End:286| Text:'extract_names' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0


Start: 289| End:295| Text:'106.25' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0


Start: 300| End:317| Text:'summarize_article' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0


Start: 320| End:326| Text:'355.75' 
Sources:
1. analyze_evaluation_results_zp68h5304e3v:0
```

## Summary

In this tutorial, we learned about:

* How to create a function to execute Python code
* How to set up a tool to interact with tabular data
* How to run the agent

By implementing these techniques, we've expanded our agentic RAG system to handle structured data in the form of tables.

While this tutorial demonstrated how to work with tabular data using pandas and Python, the agentic RAG approach can be applied to other forms of structured data as well. This means we can build agents that can translate natural language queries into various types of data analysis tasks.

In Part 6, we'll learn how to do structured query generation for SQL databases.


# Querying Structured Data (SQL)

> Build an agentic RAG system that can query structured data (SQL).

<a target="_blank" href="https://colab.research.google.com/github/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/agentic-rag/agentic_rag_pt6_structured_data_SQL.ipynb">
  Open in Colab
</a>

In the previous tutorial, we explored how agentic RAG can handle complex queries on structured data in the form of tables using pandas. Now, we'll see how we can do the same for SQL databases.

Consider a scenario similar to the previous tutorial where we have evaluation results for an LLM application. However, instead of a CSV file, this data is now stored in a SQLite database. Users might still ask questions like "What's the average score for a specific use case?" or "Which configuration has the lowest latency?", but now we'll answer these using SQL queries instead of pandas operations.

In this tutorial, we'll cover:

* Setting up a SQLite database
* Creating a function to execute SQL queries
* Building an agent for querying SQL databases
* Running the agent with various types of queries

By implementing these techniques, we'll expand our agentic RAG system to handle structured data in SQL databases, complementing our previous work with tabular data in pandas.

Let's get started by setting up our environment and creating our SQLite database.

## Setup

To get started, first we need to install the `cohere` library and create a Cohere client.

```python PYTHON
! pip install cohere pandas -qq
```

```python PYTHON
import json
import os
import cohere
import sqlite3
import pandas as pd

co = cohere.ClientV2(
    "COHERE_API_KEY"
)  # Get your free API key: https://dashboard.cohere.com/api-keys
```

## Creating a SQLite database

Next, we'll create a SQLite database to store our evaluation results. SQLite is a lightweight, serverless database engine that's perfect for small to medium-sized applications. Here's what we're going to do:

1. Create a new SQLite database file named `evaluation_results.db`.
2. Create a table called `evaluation_results` with columns for `usecase`, `run`, `score`, `temperature`, `tokens`, and `latency`.
3. Insert sample data into the table to simulate our evaluation results.

<Note>
   Important: the data can be 

  [found here](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/agentic-rag/evaluation_results.db)

  . Make sure to have the file in the same directory as this notebook for the imports to work correctly. 
</Note>

```python PYTHON
# Create a connection to a new SQLite database (or connect to an existing one)
conn = sqlite3.connect("evaluation_results.db")
cursor = conn.cursor()

# Execute the CREATE TABLE command
cursor.execute(
    """
CREATE TABLE evaluation_results (
    usecase TEXT,
    run TEXT,
    score FLOAT,
    temperature FLOAT,
    tokens INTEGER,
    latency FLOAT
)
"""
)

# Execute the INSERT commands
data = [
    ("extract_names", "A", 0.5, 0.3, 103, 1.12),
    ("draft_email", "A", 0.6, 0.3, 252, 2.5),
    ("summarize_article", "A", 0.8, 0.3, 350, 4.2),
    ("extract_names", "B", 0.2, 0.3, 101, 2.85),
    ("draft_email", "B", 0.4, 0.3, 230, 3.2),
    ("summarize_article", "B", 0.6, 0.3, 370, 4.2),
    ("extract_names", "C", 0.7, 0.3, 101, 2.22),
    ("draft_email", "C", 0.5, 0.3, 221, 2.5),
    ("summarize_article", "C", 0.1, 0.3, 361, 3.9),
    ("extract_names", "D", 0.7, 0.5, 120, 3.2),
    ("draft_email", "D", 0.8, 0.5, 280, 3.4),
    ("summarize_article", "D", 0.9, 0.5, 342, 4.8),
]

cursor.executemany(
    "INSERT INTO evaluation_results VALUES (?,?,?,?,?,?)", data
)

# Commit the changes and close the connection
conn.commit()
conn.close()
```

## Creating a function to query a SQL database

Next, we'll define a function called `sql_table_query` that allows us to execute SQL queries on our evaluation\_results database.

This function will enable us to retrieve and analyze data from our evaluation\_results table, allowing for dynamic querying based on our specific needs.

```python PYTHON
def sql_table_query(query: str) -> dict:
    """
    Execute an SQL query on the evaluation_results table and return the result as a dictionary.

    Args:
    query (str): SQL query to execute on the evaluation_results table

    Returns:
    dict: Result of the SQL query
    """
    try:
        # Connect to the SQLite database
        conn = sqlite3.connect("evaluation_results.db")

        # Execute the query and fetch the results into a DataFrame
        df = pd.read_sql_query(query, conn)

        # Close the connection
        conn.close()

        # Convert DataFrame to dictionary
        result_dict = df.to_dict(orient="records")

        return result_dict

    except sqlite3.Error as e:
        print(f"An error occurred: {e}")
        return str(e)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return str(e)


functions_map = {"sql_table_query": sql_table_query}
```

We can test the function by running a simple query:

```python PYTHON
result = sql_table_query(
    "SELECT * FROM evaluation_results WHERE usecase = 'extract_names'"
)
print(result)
```

```mdx

[{'usecase': 'extract_names', 'run': 'A', 'score': 0.5, 'temperature': 0.3, 'tokens': 103, 'latency': 1.12}, {'usecase': 'extract_names', 'run': 'B', 'score': 0.2, 'temperature': 0.3, 'tokens': 101, 'latency': 2.85}, {'usecase': 'extract_names', 'run': 'C', 'score': 0.7, 'temperature': 0.3, 'tokens': 101, 'latency': 2.22}, {'usecase': 'extract_names', 'run': 'D', 'score': 0.7, 'temperature': 0.5, 'tokens': 120, 'latency': 3.2}]
```

## Setting up a tool to interact with the database

Next, we'll create a tool that will allow the agent to interact with the SQLite database containing our evaluation results.

```python PYTHON
sql_table_query_tool = {
    "type": "function",
    "function": {
        "name": "sql_table_query",
        "description": "Execute an SQL query on the evaluation_results table in the SQLite database. The table has columns 'usecase', 'run', 'score', 'temperature', 'tokens', and 'latency'.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "SQL query to execute on the evaluation_results table",
                }
            },
            "required": ["query"],
        },
    },
}

tools = [sql_table_query_tool]
```

## Building an agent for querying SQL data

Next, let's create a `run_agent` function to run the agentic RAG workflow,  just as we did in Part 1.

The only change we are making here is to make the system message more specific and describe the database schema to the agent.

```python PYTHON
system_message = """## Task and Context
You are an assistant who helps developers analyze LLM application evaluation results from a SQLite database. The database contains a table named 'evaluation_results' with the following schema:

- usecase (TEXT): The type of task being evaluated
- run (TEXT): The identifier for a specific evaluation run
- score (REAL): The performance score of the run
- temperature (REAL): The temperature setting used for the LLM
- tokens (INTEGER): The number of tokens used in the run
- latency (REAL): The time taken for the run in seconds

You can use SQL queries to analyze this data and provide insights to the developers."""
```

```python PYTHON
model = "command-a-03-2025"


def run_agent(query, messages=None):
    if messages is None:
        messages = []

    if "system" not in {m.get("role") for m in messages}:
        messages.append({"role": "system", "content": system_message})

    # Step 1: get user message
    print(f"Question:\n{query}")
    print("=" * 50)

    messages.append({"role": "user", "content": query})

    # Step 2: Generate tool calls (if any)
    response = co.chat(
        model=model, messages=messages, tools=tools, temperature=0.3
    )

    while response.message.tool_calls:

        print("Tool plan:")
        print(response.message.tool_plan, "\n")
        print("Tool calls:")
        for tc in response.message.tool_calls:
            # print(f"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}")
            if tc.function.name == "analyze_evaluation_results":
                print(f"Tool name: {tc.function.name}")
                tool_call_prettified = print(
                    "\n".join(
                        f"  {line}"
                        for line_num, line in enumerate(
                            json.loads(tc.function.arguments)[
                                "code"
                            ].splitlines()
                        )
                    )
                )
                print(tool_call_prettified)
            else:
                print(
                    f"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}"
                )
        print("=" * 50)

        messages.append(
            {
                "role": "assistant",
                "tool_calls": response.message.tool_calls,
                "tool_plan": response.message.tool_plan,
            }
        )

        # Step 3: Get tool results
        for tc in response.message.tool_calls:
            tool_result = functions_map[tc.function.name](
                **json.loads(tc.function.arguments)
            )
            tool_content = [
                (
                    {
                        "type": "document",
                        "document": {"data": json.dumps(tool_result)},
                    }
                )
            ]

            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tc.id,
                    "content": tool_content,
                }
            )

        # Step 4: Generate response and citations
        response = co.chat(
            model=model,
            messages=messages,
            tools=tools,
            temperature=0.3,
        )

    messages.append(
        {
            "role": "assistant",
            "content": response.message.content[0].text,
        }
    )

    # Print final response
    print("Response:")
    print(response.message.content[0].text)
    print("=" * 50)

    # Print citations (if any)
    verbose_source = (
        False  # Change to True to display the contents of a source
    )
    if response.message.citations:
        print("CITATIONS:\n")
        for citation in response.message.citations:
            print(
                f"Start: {citation.start}| End:{citation.end}| Text:'{citation.text}' "
            )
            print("Sources:")
            for idx, source in enumerate(citation.sources):
                print(f"{idx+1}. {source.id}")
                if verbose_source:
                    print(f"{source.tool_output}")
            print("\n")

    return messages
```

## Running the agent

Let's now ask the agent the same set of questions we asked in the previous chapter. While the previous chapter translates the questions into pandas Python code, this time the agent will be using SQL queries.

```python PYTHON
messages = run_agent("What's the average evaluation score in run A")
# Answer: 0.63
```

```mdx

Question:
What's the average evaluation score in run A
==================================================
Tool plan:
I will query the connected SQL database to find the average evaluation score in run A. 

Tool calls:
Tool name: sql_table_query | Parameters: {"query":"SELECT AVG(score) AS average_score\r\nFROM evaluation_results\r\nWHERE run = 'A';"}
==================================================
Response:
The average evaluation score in run A is 0.63.
==================================================
CITATIONS:

Start: 41| End:46| Text:'0.63.' 
Sources:
1. sql_table_query_97h16txpbeqs:0
```

```python PYTHON
messages = run_agent(
    "What's the latency of the highest-scoring run for the summarize_article use case?"
)
# Answer: 4.8
```

```mdx

Question:
What's the latency of the highest-scoring run for the summarize_article use case?
==================================================
Tool plan:
I will query the connected SQL database to find the latency of the highest-scoring run for the summarize_article use case.

I will filter the data for the summarize_article use case and order the results by score in descending order. I will then return the latency of the first result. 

Tool calls:
Tool name: sql_table_query | Parameters: {"query":"SELECT latency\r\nFROM evaluation_results\r\nWHERE usecase = 'summarize_article'\r\nORDER BY score DESC\r\nLIMIT 1;"}
==================================================
Response:
The latency of the highest-scoring run for the summarize_article use case is 4.8.
==================================================
CITATIONS:

Start: 77| End:81| Text:'4.8.' 
Sources:
1. sql_table_query_ekswkn14ra34:0
```

```python PYTHON
messages = run_agent(
    "Which use case uses the least amount of tokens on average? Show the comparison of all use cases in a markdown table."
)
# Answer: extract_names (106.25), draft_email (245.75), summarize_article (355.75)
```

```mdx

Question:
Which use case uses the least amount of tokens on average? Show the comparison of all use cases in a markdown table.
==================================================
Tool plan:
I will query the connected SQL database to find the average number of tokens used for each use case. I will then present this information in a markdown table. 

Tool calls:
Tool name: sql_table_query | Parameters: {"query":"SELECT usecase, AVG(tokens) AS avg_tokens\nFROM evaluation_results\nGROUP BY usecase\nORDER BY avg_tokens ASC;"}
==================================================
Response:
Here is a markdown table showing the average number of tokens used for each use case:

| Use Case | Average Tokens |
|---|---|
| extract_names | 106.25 |
| draft_email | 245.75 |
| summarize_article | 355.75 |

The use case that uses the least amount of tokens on average is **extract_names**.
==================================================
CITATIONS:

Start: 129| End:142| Text:'extract_names' 
Sources:
1. sql_table_query_50yjx2cecqx1:0


Start: 145| End:151| Text:'106.25' 
Sources:
1. sql_table_query_50yjx2cecqx1:0


Start: 156| End:167| Text:'draft_email' 
Sources:
1. sql_table_query_50yjx2cecqx1:0


Start: 170| End:176| Text:'245.75' 
Sources:
1. sql_table_query_50yjx2cecqx1:0


Start: 181| End:198| Text:'summarize_article' 
Sources:
1. sql_table_query_50yjx2cecqx1:0


Start: 201| End:207| Text:'355.75' 
Sources:
1. sql_table_query_50yjx2cecqx1:0


Start: 277| End:290| Text:'extract_names' 
Sources:
1. sql_table_query_50yjx2cecqx1:0
```

## Summary

In this tutorial, we learned about:

* How to set up a SQLite database for structured data
* How to create a function to execute SQL queries
* How to build an agent for querying the database
* How to run the agent

By implementing these techniques, we've further expanded our agentic RAG system to handle structured data in the form of SQL databases. This allows for more powerful and flexible querying capabilities, especially when dealing with large datasets or complex relationships between data.

This tutorial completes our exploration of structured data handling in the agentic RAG system, covering both tabular data (using pandas) and relational databases (using SQL). These capabilities significantly enhance the system's ability to work with diverse data formats and structures.


# Introduction to Cohere on Azure AI Foundry

> An introduction to Cohere on Azure AI Foundry, a fully managed service by Azure (API v2).

## What is Azure AI Foundry

Azure AI Foundry is a trusted platform that empowers developers to build and deploy innovative, responsible AI applications. It offers an enterprise-grade environment with cutting-edge tools and models, ensuring a safe and secure development process.

The platform facilitates collaboration, allowing teams to work together on the full lifecycle of application development. With Azure AI Foundry, developers can explore a wide range of models, services, and capabilities to build AI applications that meet their specific goals.

Hubs are the primary top-level Azure resource for AI Foundry. They provide a central way for a team to govern security, connectivity, and computing resources across playgrounds and projects. Once a hub is created, developers can create projects from it and access shared company resources without needing an IT administrator's repeated help.

Your new project will be added under your current hub, which provides security, governance controls, and shared configurations that all projects can use. Project workspaces that are created using a hub inherit the same security settings and shared resource access. Teams can create project workspaces as needed to organize their work, isolate data, and/or restrict access.

## Azure AI Foundry Features

* Build generative AI applications on an enterprise-grade platform.
* Explore, build, test, and deploy using cutting-edge AI tools and ML models, grounded in responsible AI practices.
* Collaborate with a team for the full life-cycle of application development.
* Improve your application's performance using tools like tracing to debug your application or compare evaluations to hone in on how you want your application to behave.
* Safegaurd every layer with trustworthy AI from the start and protect against any risks.

With AI Foundry, you can explore a wide variety of models, services and capabilities, and get to building AI applications that best serve your goals.

## Cohere Models on Azure AI Foundry

To get the most updated list of available models, visit the [Azure AI Foundry documentation here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-cohere-command?tabs=cohere-command-r-plus-08-2024\&pivots=programming-language-python).

## Pricing Mechanisms

Cohere models can be deployed to serverless API endpoints with pay-as-you-go billing. This kind of deployment provides a way to consume models as an API without hosting them on your subscription, while keeping the enterprise security and compliance that organizations need.

To get the most updated list of available models, visit the [Azure marketplace here](https://azuremarketplace.microsoft.com/en-us/marketplace/apps?page=1\&search=cohere).

## Deploying Cohere's Models on Azure AI Foundry.

To deploy Cohere's models on Azure AI Foundry, follow the steps described in [Azure AI Foundry  documentation here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio).

In summary, you will need to:

1. Set up AI Foundry Hub and a project
2. Find your model and model ID in the model catalog
3. Subscribe your project to the model offering
4. Deploy the model to a serverless API endpoint

Models that are offered by Cohere are billed through the Azure Marketplace. For such models, you're required to subscribe your project to the particular model offering.

## Conclusion

This page introduces Azure AI Foundry, a fully managed service by Azure that you can deploy Cohere's models on. We also went through the steps to get set up with Azure AI Foundry and deploy a Cohere model.

In the next sections, we will go through the various use cases of using Cohere's Command, Embed, and Rerank models on Azure AI Foundry.


# Text generation - Cohere on Azure AI Foundry

> A guide for performing text generation with Cohere's Command models on Azure AI Foundry (API v2).

[Open in GitHub](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/cohere-on-azure/v2/azure-ai-text-generation.ipynb)

In this tutorial, we'll explore text generation using Cohere's Command model on Azure AI Foundry.

Text generation is a fundamental capability that enables LLMs to generate text for various applications, such as providing detailed responses to questions, helping with writing and editing tasks, creating conversational responses, and assisting with code generation and documentation.

In this tutorial, we'll cover:

* Setting up the Cohere client
* Basic text generation
* Other typical use cases
* Building a chatbot

We'll use Cohere's Command model deployed on Azure to demonstrate these capabilities and help you understand how to effectively use text generation in your applications.

## Setup

First, you will need to deploy the Command model on Azure via Azure AI Foundry. The deployment will create a serverless API with pay-as-you-go token based billing. You can find more information on how to deploy models in the [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio).

In the example below, we are deploying the Command R+ (August 2024) model.

Once the model is deployed, you can access it via Cohere's Python SDK. Let's now install the Cohere SDK and set up our client.

To create a client, you need to provide the API key and the model's base URL for the Azure endpoint. You can get these information from the Azure AI Foundry platform where you deployed the model.

```python PYTHON
# %pip install cohere
import cohere

co = cohere.ClientV2(
    api_key="AZURE_API_KEY_CHAT",
    base_url="AZURE_ENDPOINT_CHAT",  # example: "https://cohere-command-r-plus-08-2024-xyz.eastus.models.ai.azure.com/"
)
```

## Creating some contextual information

Before we begin, let's create some context to use in our text generation tasks. In this example, we'll use a set of technical support frequently asked questions (FAQs) as our context.

```python PYTHON
# Technical support FAQ
faq_tech_support = """- Question: How do I set up my new smartphone with my mobile plan?
- Answer:
  - Insert your SIM card into the device.
  - Turn on your phone and follow the on-screen setup instructions.
  - Connect to your mobile network and enter your account details when prompted.
  - Download and install any necessary apps or updates.
  - Contact customer support if you need further assistance.

- Question: My internet connection is slow. How can I improve my mobile data speed?
- Answer:
  - Check your signal strength and move to an area with better coverage.
  - Restart your device and try connecting again.
  - Ensure your data plan is active and has sufficient data.
  - Consider upgrading your plan for faster speeds.

- Question: I can't connect to my mobile network. What should I do?
- Answer:
  - Check your SIM card is inserted correctly and not damaged.
  - Restart your device and try connecting again.
  - Ensure your account is active and not suspended.
  - Check for any network outages in your area.
  - Contact customer support for further assistance.

- Question: How do I set up my voicemail?
- Answer:
  - Dial your voicemail access number (usually provided by your carrier).
  - Follow the prompts to set up your voicemail greeting and password.
  - Record your voicemail greeting and save it.
  - Test your voicemail by calling your number and leaving a message.

- Question: I'm having trouble sending text messages. What could be the issue?
- Answer:
  - Check your signal strength and move to an area with better coverage.
  - Ensure your account has sufficient credit or an active plan.
  - Restart your device and try sending a message again.
  - Check your message settings and ensure they are correct.
  - Contact customer support if the issue persists."""
```

## Helper function to generate text

Now, let's define a function to generate text using the Command R+ model on Bedrock. We’ll use this function a few times throughout.

This function takes a user message and generates the response via the chat endpoint. Note that we don't need to specify the model as we have already set it up in the client.

```python PYTHON
def generate_text(message):
    response = co.chat(
        model="model",  # Pass a dummy string
        messages=[{"role": "user", "content": message}],
    )
    return response
```

## Text generation

Let's explore basic text generation as our first use case. The model takes a prompt as input and produces a relevant response as output.

Consider a scenario where a customer support agent uses an LLM to help draft responses to customer inquiries. The agent provides technical support FAQs as context along with the customer's question. The prompt is structured to include three components: the instruction, the context (FAQs), and the specific customer inquiry.

After passing this prompt to our `generate_text` function, we receive a response object. The actual generated text can be accessed through the `response.text` attribute.

```python PYTHON
inquiry = "I've noticed some fluctuations in my mobile network's performance recently. The connection seems stable most of the time, but every now and then, I experience brief periods of slow data speeds. It happens a few times a day and is quite inconvenient."

prompt = f"""Use the FAQs below to provide a concise response to this customer inquiry.

# Customer inquiry
{inquiry}

# FAQs
{faq_tech_support}"""

response = generate_text(prompt)

print(response.message.content[0].text)
```

```mdx
It's quite common to experience occasional fluctuations in mobile network performance, and there are a few steps you can take to address this issue. 

First, check your signal strength and consider moving to a different location with better coverage. Sometimes, even a small change in position can make a difference. If you find that you're in an area with low signal strength, this could be the primary reason for the slow data speeds. 

Next, try restarting your device. A simple restart can often resolve temporary glitches and improve your connection. After restarting, ensure that your data plan is active and has enough data allocated for your usage. If you're close to reaching your data limit, this could also impact your speeds. 

If the issue persists, it might be worth checking for any network outages in your area. Occasionally, temporary network issues can cause intermittent slowdowns. Contact your mobile network's customer support to inquire about any known issues and to receive further guidance. 

Additionally, consider the age and condition of your device. Older devices or those with outdated software might struggle to maintain consistent data speeds. Ensuring your device is up-to-date and well-maintained can contribute to a better overall network experience. 

If the problem continues, you may want to explore the option of upgrading your data plan. Higher-tier plans often offer faster speeds and more reliable connections, especially during peak usage times. Contact your mobile provider to discuss the available options and find a plan that better suits your needs.
```

## Text summarization

Another type of use case is text summarization. Now, let's summarize the customer inquiry into a single sentence. We add an instruction to the prompt and then pass the inquiry to the prompt.

```python PYTHON
prompt = f"""Summarize this customer inquiry into one short sentence.

Inquiry: {inquiry}"""

response = generate_text(prompt)

print(response.message.content[0].text)
```

```mdx
A customer is experiencing intermittent slow data speeds on their mobile network several times a day.
```

## Text rewriting

Text rewriting is a powerful capability that allows us to adapt content for different purposes while preserving the core message. This involves transforming the style, tone, or format of text to better suit the target audience or medium.

Let's look at an example where we convert a customer support chat response into a formal email. We'll construct the prompt by first stating our goal to rewrite the text, then providing the original chat response as context.

```python PYTHON
prompt = f"""Rewrite this customer support agent response into an email format, ready to send to the customer.

If you're experiencing brief periods of slow data speeds or difficulty sending text messages and connecting to your mobile network, here are some troubleshooting steps you can follow:

1. Check your signal strength - Move to an area with better coverage.
2. Restart your device and try connecting again.
3. Ensure your account is active and not suspended.
4. Contact customer support for further assistance. (This can include updating your plan for better network performance.)

Did these steps help resolve the issue? Let me know if you need further assistance."""

response = generate_text(prompt)

print(response.message.content[0].text)
```

```mdx
Subject: Troubleshooting Slow Data Speeds and Network Connection Issues

Dear [Customer's Name],

I hope this email finds you well. I understand that you may be facing some challenges with your mobile network, including slow data speeds and difficulties sending text messages. Here are some recommended troubleshooting steps to help resolve these issues:

- Signal Strength: Check the signal strength on your device and move to a different location if the signal is weak. Moving to an area with better coverage can often improve your connection.

- Restart Your Device: Sometimes, a simple restart can resolve temporary glitches. Please restart your device and then try connecting to the network again.

- Account Status: Verify that your account is active and in good standing. In some cases, service providers may temporarily suspend accounts due to various reasons, which can impact your network access. 

- Contact Customer Support: If the issue persists, please reach out to our customer support team for further assistance. Our team can help troubleshoot and provide additional guidance. We can also discuss your current plan and explore options to enhance your network performance if needed.

I hope these steps will help resolve the issue promptly. Please feel free to reply to this email if you have any further questions or if the problem continues. We are committed to ensuring your satisfaction and providing a seamless network experience.

Best regards,
[Your Name]
[Customer Support Agent]
[Company Name]
```

## Build a Chatbot

While our previous examples were single-turn interactions, the Chat endpoint enables us to create chatbots that maintain memory of past conversation turns. This capability allows developers to build conversational applications that preserve context throughout the dialogue.

Below, we implement a basic customer support chatbot that acts as a helpful service agent. We'll create a function called run\_chatbot that handles the conversation flow and displays messages and events. The function can take an optional chat history parameter to maintain conversational context across multiple turns.

```python PYTHON
# Define a system message
system_message = """## Task and Context
You are a helpful customer support agent that assists customers of a mobile network service."""


# Run the chatbot
def run_chatbot(message, messages=None):
    if messages is None:
        messages = []

    if "system" not in {m.get("role") for m in messages}:
        messages.append({"role": "system", "content": system_message})

    messages.append({"role": "user", "content": message})

    response = co.chat(
        model="model",  # Pass a dummy string
        messages=messages,
    )

    messages.append(
        {
            "role": "assistant",
            "content": response.message.content[0].text,
        }
    )

    print(response.message.content[0].text)

    return messages
```

```python PYTHON
messages = run_chatbot(
    "Hi. I've noticed some fluctuations in my mobile network's performance recently."
)
```

```mdx
Hello there! I'd be happy to assist you with this issue. Network performance fluctuations can be concerning, and it's important to identify the cause to ensure you have a smooth experience. 

Can you tell me more about the problems you've been experiencing? Are there specific times or locations where the network seems to perform poorly? Any details you can provide will help me understand the situation better and offer potential solutions.
```

```python PYTHON
messages = run_chatbot(
    "At times, the data speed is very poor. What should I do?",
    messages,
)
```

```mdx
I'm sorry to hear that you're experiencing slow data speeds. Here are some troubleshooting steps and tips to help improve your network performance:

- **Check Network Coverage:** First, ensure that you are in an area with good network coverage. You can check the coverage map provided by your mobile network service on their website. If you're in a location with known weak signal strength, moving to a different area might improve your data speed.

- **Restart Your Device:** Sometimes, a simple restart of your mobile device can help refresh the network connection. Power off your device, wait for a few moments, and then turn it back on.

- **Check for Network Updates:** Make sure your device is running the latest software and carrier settings. Updates often include improvements and optimizations for network performance. You can check for updates in your device's settings.

- **Manage Network Settings:**
   - *Network Mode:* Try switching to a different network mode (e.g., 4G/LTE, 3G) to see if a specific network band provides better speed.
   - *Airplane Mode:* Toggle Airplane mode on and off to reconnect to the network.
   - *Network Reset:* If the issue persists, you can try resetting your network settings, but note that this will erase saved Wi-Fi passwords.

- **Contact Customer Support:** If the problem continues, it might be beneficial to contact your mobile network's customer support team. They can check for any known issues in your area and provide further assistance. They might also guide you through advanced troubleshooting steps.

- **Consider Network Congestion:** Slow data speeds can sometimes occur during peak usage hours when the network is congested. Try using data-intensive apps during off-peak hours to see if that makes a difference.

- **Check Background Apps:** Certain apps running in the background can consume data and impact speed. Close any unnecessary apps to free up resources.

If the slow data speed persists despite these measures, it's advisable to reach out to your mobile network provider for further investigation and assistance. They can provide network-specific solutions and ensure you're getting the service you expect.
```

```python PYTHON
messages = run_chatbot("Thanks. What else can I check?", messages)
```

```mdx
ou're welcome! Here are some additional steps and factors to consider:

- **Device Health:** Ensure your device is in good working condition. An older device or one with hardware issues might struggle to maintain a fast data connection. Consider checking for any pending system updates that could optimize your device's performance.

- **SIM Card:** Try removing and reinserting your SIM card to ensure it is properly seated. A loose connection can impact network performance. If the issue persists, it might be worth asking your network provider for a SIM replacement.

- **Network Congestion at Specific Times:** Network speed can vary depending on the time of day. If possible, monitor your data speed during different parts of the day to identify any patterns. This can help determine if network congestion during peak hours is the primary cause.

- **Data Plan and Throttling:** Check your mobile data plan to ensure you haven't exceeded any data limits, which could result in reduced speeds. Some providers throttle speeds after a certain data threshold is reached.

- **Background Updates and Downloads:** Certain apps might be set to update or download content in the background, consuming data and potentially slowing down your connection. Review your app settings and consider disabling automatic updates or background data usage for apps that don't require real-time updates.

- **Network Diagnostics Tools:** Some mobile devices have built-in network diagnostics tools that can provide insights into your connection. These tools can help identify issues with signal strength, network latency, and more.

- **Wi-Fi Calling and Data Usage:** If your device supports Wi-Fi calling, ensure it is enabled. This can offload some data usage from the cellular network, potentially improving speeds.

- **Network Provider's App:** Download and install your mobile network provider's official app, if available. These apps often provide real-time network status updates and allow you to report issues directly.

If you've gone through these checks and the problem persists, contacting your network provider's technical support team is the next best step. They can provide further guidance based on your specific situation.
```

### View the chat history

Here's what is contained in the chat history after a few turns.

```python PYTHON
print("Chat history:")
for message in messages:
    print(message, "\n")
```

```mdx
Chat history:
{'role': 'system', 'content': '## Task and Context\nYou are a helpful customer support agent that assists customers of a mobile network service.'} 

{'role': 'user', 'content': "Hi. I've noticed some fluctuations in my mobile network's performance recently."} 

{'role': 'assistant', 'content': "Hello there! I'd be happy to assist you with this issue. Network performance fluctuations can be concerning, and it's important to identify the cause to ensure you have a smooth experience. \n\nCan you tell me more about the problems you've been experiencing? Are there specific times or locations where the network seems to perform poorly? Any details you can provide will help me understand the situation better and offer potential solutions."} 

{'role': 'user', 'content': 'At times, the data speed is very poor. What should I do?'} 

{'role': 'assistant', 'content': "I'm sorry to hear that you're experiencing slow data speeds. Here are some troubleshooting steps and tips to help improve your network performance:\n\n- **Check Network Coverage:** First, ensure that you are in an area with good network coverage. You can check the coverage map provided by your mobile network service on their website. If you're in a location with known weak signal strength, moving to a different area might improve your data speed.\n\n- **Restart Your Device:** Sometimes, a simple restart of your mobile device can help refresh the network connection. Power off your device, wait for a few moments, and then turn it back on.\n\n- **Check for Network Updates:** Make sure your device is running the latest software and carrier settings. Updates often include improvements and optimizations for network performance. You can check for updates in your device's settings.\n\n- **Manage Network Settings:**\n   - *Network Mode:* Try switching to a different network mode (e.g., 4G/LTE, 3G) to see if a specific network band provides better speed.\n   - *Airplane Mode:* Toggle Airplane mode on and off to reconnect to the network.\n   - *Network Reset:* If the issue persists, you can try resetting your network settings, but note that this will erase saved Wi-Fi passwords.\n\n- **Contact Customer Support:** If the problem continues, it might be beneficial to contact your mobile network's customer support team. They can check for any known issues in your area and provide further assistance. They might also guide you through advanced troubleshooting steps.\n\n- **Consider Network Congestion:** Slow data speeds can sometimes occur during peak usage hours when the network is congested. Try using data-intensive apps during off-peak hours to see if that makes a difference.\n\n- **Check Background Apps:** Certain apps running in the background can consume data and impact speed. Close any unnecessary apps to free up resources.\n\nIf the slow data speed persists despite these measures, it's advisable to reach out to your mobile network provider for further investigation and assistance. They can provide network-specific solutions and ensure you're getting the service you expect."} 

{'role': 'user', 'content': 'Thanks. What else can I check?'} 

{'role': 'assistant', 'content': "You're welcome! Here are some additional steps and factors to consider:\n\n- **Device Health:** Ensure your device is in good working condition. An older device or one with hardware issues might struggle to maintain a fast data connection. Consider checking for any pending system updates that could optimize your device's performance.\n\n- **SIM Card:** Try removing and reinserting your SIM card to ensure it is properly seated. A loose connection can impact network performance. If the issue persists, it might be worth asking your network provider for a SIM replacement.\n\n- **Network Congestion at Specific Times:** Network speed can vary depending on the time of day. If possible, monitor your data speed during different parts of the day to identify any patterns. This can help determine if network congestion during peak hours is the primary cause.\n\n- **Data Plan and Throttling:** Check your mobile data plan to ensure you haven't exceeded any data limits, which could result in reduced speeds. Some providers throttle speeds after a certain data threshold is reached.\n\n- **Background Updates and Downloads:** Certain apps might be set to update or download content in the background, consuming data and potentially slowing down your connection. Review your app settings and consider disabling automatic updates or background data usage for apps that don't require real-time updates.\n\n- **Network Diagnostics Tools:** Some mobile devices have built-in network diagnostics tools that can provide insights into your connection. These tools can help identify issues with signal strength, network latency, and more.\n\n- **Wi-Fi Calling and Data Usage:** If your device supports Wi-Fi calling, ensure it is enabled. This can offload some data usage from the cellular network, potentially improving speeds.\n\n- **Network Provider's App:** Download and install your mobile network provider's official app, if available. These apps often provide real-time network status updates and allow you to report issues directly.\n\nIf you've gone through these checks and the problem persists, contacting your network provider's technical support team is the next best step. They can provide further guidance based on your specific situation."} 
```

## Summary

In this tutorial, we learned about:

* How to set up the Cohere client to use the Command model deployed on Azure AI Foundry
* How to perform basic text generation
* How to use the model for other types of use cases
* How to build a chatbot using the Chat endpoint

In the next tutorial, we'll explore how to use the Embed model in semantic search applications.


# Semantic search - Cohere on Azure AI Foundry

> A guide for performing text semantic search with Cohere's Embed models on Azure AI Foundry (API v2).

[Open in GitHub](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/cohere-on-azure/v2/azure-ai-sem-search.ipynb)

In this tutorial, we'll explore semantic search using Cohere's Embed modelon Azure AI Foundry.

Semantic search enables search systems to capture the meaning and context of search queries, going beyond simple keyword matching to find relevant results based on semantic similarity.

With the Embed model, you can do this across languages. This is particularly powerful for multilingual applications where the same meaning can be expressed in different languages.

In this tutorial, we'll cover:

* Setting up the Cohere client
* Embedding text data
* Building a search index
* Performing semantic search queries

We'll use Cohere's Embed model deployed on Azure to demonstrate these capabilities and help you understand how to effectively implement semantic search in your applications.

## Setup

First, you will need to deploy the Embed model on Azure via Azure AI Foundry. The deployment will create a serverless API with pay-as-you-go token based billing. You can find more information on how to deploy models in the [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio).

In the example below, we are deploying the Embed 4 model.

Once the model is deployed, you can access it via Cohere's Python SDK. Let's now install the Cohere SDK and set up our client.

To create a client, you need to provide the API key and the model's base URL for the Azure endpoint. You can get these information from the Azure AI Foundry platform where you deployed the model.

```python PYTHON
# %pip install cohere hnswlib

import pandas as pd
import hnswlib
import re
import cohere

co = cohere.ClientV2(
    api_key="AZURE_API_KEY_EMBED",
    base_url="AZURE_ENDPOINT_EMBED",  # example: "https://embed-v-4-0-xyz.eastus.models.ai.azure.com/"
)
```

## Download dataset

For this example, we'll be using [MultiFIN](https://aclanthology.org/2023.findings-eacl.66.pdf) - an open-source dataset of financial article headlines in 15 different languages (including English, Turkish, Danish, Spanish, Polish, Greek, Finnish, Hebrew, Japanese, Hungarian, Norwegian, Russian, Italian, Icelandic, and Swedish).

We've prepared a CSV version of the MultiFIN dataset that includes an additional column containing English translations. While we won't use these translations for the model itself, they'll help us understand the results when we encounter headlines in Danish or Spanish. We'll load this CSV file into a pandas dataframe.

```python PYTHON
url = "https://raw.githubusercontent.com/cohere-ai/cohere-aws/main/notebooks/bedrock/multiFIN_train.csv"
df = pd.read_csv(url)

# Inspect dataset
df.head(5)
```

## Pre-Process Dataset

For this example, we'll work with a subset focusing on English, Spanish, and Danish content.

We'll perform several pre-processing steps: removing any duplicate entries, filtering to keep only our three target languages, and selecting the 80 longest articles as our working dataset.

```python PYTHON
# Ensure there is no duplicated text in the headers
def remove_duplicates(text):
    return re.sub(
        r"((\b\w+\b.{1,2}\w+\b)+).+\1", r"\1", text, flags=re.I
    )


df["text"] = df["text"].apply(remove_duplicates)

# Keep only selected languages
languages = ["English", "Spanish", "Danish"]
df = df.loc[df["lang"].isin(languages)]

# Pick the top 80 longest articles
df["text_length"] = df["text"].str.len()
df.sort_values(by=["text_length"], ascending=False, inplace=True)
top_80_df = df[:80]

# Language distribution
top_80_df["lang"].value_counts()
```

```mdx
lang
Spanish    33
English    29
Danish     18
Name: count, dtype: int64
```

## Embed and index documents

Let's embed our documents and store the embeddings. These embeddings are high-dimensional vectors (1,024 dimensions) that capture the semantic meaning of each document. We'll use Cohere's Embed 4 model that we have defined in the client setup.

The Embed 4 model require us to specify an `input_type` parameter that indicates what we're embedding. For semantic search, we use `search_document` for the documents we want to search through, and `search_query` for the search queries we'll make later.

We'll also keep track information about each document's language and translation to provide richer search results.

Finally, we'll build a search index with the `hnsw` vector library to store these embeddings efficiently, enabling faster document searches.

```python PYTHON
# Embed documents
# Embed documents
docs = top_80_df["text"].to_list()
docs_lang = top_80_df["lang"].to_list()
translated_docs = top_80_df[
    "translation"
].to_list()  # for reference when returning non-English results
doc_embs = co.embed(
    model="embed-v4.0",
    texts=docs,
    input_type="search_document",
    embedding_types=["float"],
).embeddings.float

# Create a search index
index = hnswlib.Index(space="ip", dim=1536)
index.init_index(
    max_elements=len(doc_embs), ef_construction=512, M=64
)
index.add_items(doc_embs, list(range(len(doc_embs))))
```

## Send Query and Retrieve Documents

Next, we build a function that takes a query as input, embeds it, and finds the three documents that are the most similar to the query.

```python PYTHON
# Retrieval of 4 closest docs to query
def retrieval(query):
    # Embed query and retrieve results
    query_emb = co.embed(
        model="embed-v4.0",  # Pass a dummy string
        texts=[query],
        input_type="search_query",
        embedding_types=["float"],
    ).embeddings.float

    doc_ids = index.knn_query(query_emb, k=3)[0][
        0
    ]  # we will retrieve 3 closest neighbors

    # Print and append results
    print(f"QUERY: {query.upper()} \n")
    retrieved_docs, translated_retrieved_docs = [], []

    for doc_id in doc_ids:
        # Append results
        retrieved_docs.append(docs[doc_id])
        translated_retrieved_docs.append(translated_docs[doc_id])

        # Print results
        print(f"ORIGINAL ({docs_lang[doc_id]}): {docs[doc_id]}")
        if docs_lang[doc_id] != "English":
            print(f"TRANSLATION: {translated_docs[doc_id]} \n----")
        else:
            print("----")
    print("END OF RESULTS \n\n")
    return retrieved_docs, translated_retrieved_docs
```

Let’s now try to query the index with a couple of examples, one each in English and Danish.

```python PYTHON
queries = [
    "Can data science help meet sustainability goals?",  # English example
    "Hvor kan jeg finde den seneste danske boligplan?",  # Danish example - "Where can I find the latest Danish property plan?"
]

for query in queries:
    retrieval(query)
```

```mdx
QUERY: CAN DATA SCIENCE HELP MEET SUSTAINABILITY GOALS? 

ORIGINAL (English): Using AI to better manage the environment could reduce greenhouse gas emissions, boost global GDP by up to 38m jobs by 2030
----
ORIGINAL (English): Quality of business reporting on the Sustainable Development Goals improves, but has a long way to go to meet and drive targets.
----
ORIGINAL (English): Only 10 years to achieve Sustainable Development Goals but businesses remain on starting blocks for integration and progress
----
END OF RESULTS 


QUERY: HVOR KAN JEG FINDE DEN SENESTE DANSKE BOLIGPLAN? 

ORIGINAL (Danish): Nyt fra CFOdirect: Ny PP&E-guide, FAQs om den nye leasingstandard, podcast om udfordringerne ved implementering af leasingstandarden og meget mere
TRANSLATION: New from CFOdirect: New PP&E guide, FAQs on the new leasing standard, podcast on the challenges of implementing the leasing standard and much more 
----
ORIGINAL (Danish): Lovforslag fremlagt om rentefri lån, udskudt frist for lønsumsafgift, førtidig udbetaling af skattekredit og loft på indestående på skattekontoen
TRANSLATION: Bills presented on interest -free loans, deferred deadline for payroll tax, early payment of tax credit and ceiling on the balance in the tax account 
----
ORIGINAL (Danish): Nyt fra CFOdirect: Shareholder-spørgsmål til ledelsen, SEC cybersikkerhedsguide, den amerikanske skattereform og meget mere
TRANSLATION: New from CFOdirect: Shareholder questions for management, the SEC cybersecurity guide, US tax reform and more 
----
END OF RESULTS 
```

With the first example, notice how the retrieval system was able to surface documents similar in meaning, for example, surfacing documents related to AI when given a query about data science. This is something that keyword-based search will not be able to capture.

As for the second example, this demonstrates the multilingual nature of the model. You can use the same model across different languages. The model can also perform cross-lingual search, such as the example of from the first retrieved document, where “PP\&E guide” is an English term that stands for “property, plant, and equipment,”.

## Summary

In this tutorial, we learned about:

* How to set up the Cohere client to use the Embed model deployed on Azure AI Foundry
* How to embed text data
* How to build a search index
* How to perform multilingualsemantic search

In the next tutorial, we'll explore how to use the Rerank model for reranking search results.


# Reranking - Cohere on Azure AI Foundry

> A guide for performing reranking with Cohere's Reranking models on Azure AI Foundry (API v2).

[Open in GitHub](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/cohere-on-azure/v2/azure-ai-reranking.ipynb)

In this tutorial, we'll explore reranking using Cohere's Rerank model on Azure AI Foundry.

Reranking is a crucial technique used in information retrieval systems, particularly for large-scale search applications. The process involves taking an initial set of retrieved documents and reordering them based on how relevant they are to the user's search query.

One of the most compelling aspects of reranking is its ease of implementation - despite providing substantial improvements to search results, Cohere's Rerank models can be integrated into any existing search system with just a single line of code, regardless of whether it uses semantic or traditional keyword-based search approaches.

In this tutorial, we'll cover:

* Setting up the Cohere client
* Retrieving documents
* Reranking documents
* Reranking semi structured data

We'll use Cohere's Embed model deployed on Azure to demonstrate these capabilities and help you understand how to effectively implement semantic search in your applications.

## Setup

First, you will need to deploy the Rerank model on Azure via Azure AI Foundry. The deployment will create a serverless API with pay-as-you-go token based billing. You can find more information on how to deploy models in the [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio).

In the example below, we are deploying the Rerank Multilingual v3 model.

Once the model is deployed, you can access it via Cohere's Python SDK. Let's now install the Cohere SDK and set up our client.

To create a client, you need to provide the API key and the model's base URL for the Azure endpoint. You can get these information from the Azure AI Foundry platform where you deployed the model.

```python PYTHON
# %pip install cohere

import cohere

co = cohere.ClientV2(
    api_key="AZURE_API_KEY_RERANK",
    base_url="AZURE_ENDPOINT_RERANK",  # example: "https://cohere-rerank-v3-multilingual-xyz.eastus.models.ai.azure.com/"
)
```

## Retrieve documents

For this example, we'll work with documents that have already been retrieved through an initial search stage (which could be semantic search, keyword matching, or another retrieval method).

Below is a list of nine documents representing the initial search results. Each document contains email data structured as a dictionary with two fields - Title and Content. This semi-structured format allows the Rerank endpoint to effectively process and reorder the results based on relevance.

```python PYTHON
documents = [
    {
        "Title": "Incorrect Password",
        "Content": "Hello, I have been trying to access my account for the past hour and it keeps saying my password is incorrect. Can you please help me?",
    },
    {
        "Title": "Confirmation Email Missed",
        "Content": "Hi, I recently purchased a product from your website but I never received a confirmation email. Can you please look into this for me?",
    },
    {
        "Title": "Questions about Return Policy",
        "Content": "Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.",
    },
    {
        "Title": "Customer Support is Busy",
        "Content": "Good morning, I have been trying to reach your customer support team for the past week but I keep getting a busy signal. Can you please help me?",
    },
    {
        "Title": "Received Wrong Item",
        "Content": "Hi, I have a question about my recent order. I received the wrong item and I need to return it.",
    },
    {
        "Title": "Customer Service is Unavailable",
        "Content": "Hello, I have been trying to reach your customer support team for the past hour but I keep getting a busy signal. Can you please help me?",
    },
    {
        "Title": "Return Policy for Defective Product",
        "Content": "Hi, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.",
    },
    {
        "Title": "Wrong Item Received",
        "Content": "Good morning, I have a question about my recent order. I received the wrong item and I need to return it.",
    },
    {
        "Title": "Return Defective Product",
        "Content": "Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.",
    },
]
```

## Rerank documents

Adding a reranking component is simple with Cohere Rerank. It takes just one line of code to implement.

Calling the Rerank endpoint requires the following arguments:

* `documents`: The list of documents, which we defined in the previous section
* `query`: The user query; we’ll use 'What emails have been about refunds?' as an example
* `top_n`:  The number of documents we want to be returned, sorted from the most to the least relevant document

When passing documents that contain multiple fields like in this case, for best performance we recommend formatting them as YAML strings.

```python PYTHON
import yaml

yaml_docs = [yaml.dump(doc, sort_keys=False) for doc in documents]

query = "What emails have been about refunds?"

results = co.rerank(
    model="model",  # Pass a dummy string
    documents=yaml_docs,
    query=query,
    top_n=3,
)
```

Since we set `top_n=3`, the response will return the three documents most relevant to our query. Each result includes both the document's original position (index) in our input list and a score indicating how well it matches the query.

Let's examine the reranked results below.

```python PYTHON
def return_results(results, documents):
    for idx, result in enumerate(results.results):
        print(f"Rank: {idx+1}")
        print(f"Score: {result.relevance_score}")
        print(f"Document: {documents[result.index]}\n")


return_results(results, documents)
```

```mdx
Rank: 1
Score: 8.547617e-05
Document: {'Title': 'Return Defective Product', 'Content': 'Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.'}

Rank: 2
Score: 5.1442214e-05
Document: {'Title': 'Questions about Return Policy', 'Content': 'Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.'}

Rank: 3
Score: 3.591301e-05
Document: {'Title': 'Return Policy for Defective Product', 'Content': 'Hi, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.'}
```

The search query was looking for emails about refunds. But none of the documents mention the word “refunds” specifically.

However, the Rerank model was able to retrieve the right documents. Some of the documents mentioned the word “return”, which has a very similar meaning to "refunds."

## Rerank semi structured data

The Rerank 3 model supports multi-aspect and semi-structured data like emails, invoices, JSON documents, code, and tables. By setting the rank fields, you can select which fields the model should consider for reranking.

In the following example, we’ll use an email data example. It is a semi-stuctured data that contains a number of fields – from, to, date, subject, and text.

The model will rerank based on order of the fields passed.

```python PYTHON
# Define the documents
emails = [
    {
        "from": "hr@co1t.com",
        "to": "david@co1t.com",
        "date": "2024-06-24",
        "subject": "A Warm Welcome to Co1t!",
        "text": "We are delighted to welcome you to the team! As you embark on your journey with us, you'll find attached an agenda to guide you through your first week.",
    },
    {
        "from": "it@co1t.com",
        "to": "david@co1t.com",
        "date": "2024-06-24",
        "subject": "Setting Up Your IT Needs",
        "text": "Greetings! To ensure a seamless start, please refer to the attached comprehensive guide, which will assist you in setting up all your work accounts.",
    },
    {
        "from": "john@co1t.com",
        "to": "david@co1t.com",
        "date": "2024-06-24",
        "subject": "First Week Check-In",
        "text": "Hello! I hope you're settling in well. Let's connect briefly tomorrow to discuss how your first week has been going. Also, make sure to join us for a welcoming lunch this Thursday at noon—it's a great opportunity to get to know your colleagues!",
    },
]

yaml_emails = [yaml.dump(doc, sort_keys=False) for doc in emails]
```

```python PYTHON
# Add the user query
query = "Any email about check ins?"

# Rerank the documents
results = co.rerank(
    model="model",  # Pass a dummy string
    query=query,
    documents=yaml_emails,
    top_n=2,
)

return_results(results, emails)
```

```mdx
Rank: 1
Score: 0.13477592
Document: {'from': 'john@co1t.com', 'to': 'david@co1t.com', 'date': '2024-06-24', 'subject': 'First Week Check-In', 'text': "Hello! I hope you're settling in well. Let's connect briefly tomorrow to discuss how your first week has been going. Also, make sure to join us for a welcoming lunch this Thursday at noon—it's a great opportunity to get to know your colleagues!"}

Rank: 2
Score: 0.0010083435
Document: {'from': 'it@co1t.com', 'to': 'david@co1t.com', 'date': '2024-06-24', 'subject': 'Setting Up Your IT Needs', 'text': 'Greetings! To ensure a seamless start, please refer to the attached comprehensive guide, which will assist you in setting up all your work accounts.'}
```

## Summary

In this tutorial, we learned about:

* How to set up the Cohere client to use the Rerank model deployed on Azure AI Foundry
* How to retrieve documents
* How to rerank documents
* How to rerank semi structured data

In the next tutorial, we'll learn how to build RAG applications by leveraging the models that we've looked at in the previous tutorials - Command, Embed, and Rerank.


# Retrieval augmented generation (RAG) - Cohere on Azure AI Foundry

> A guide for performing retrieval augmented generation (RAG) with Cohere's Command models on Azure AI Foundry (API v2).

[Open in GitHub](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/cohere-on-azure/v2/azure-ai-rag.ipynb)

Large Language Models (LLMs) excel at generating text and maintaining conversational context in chat applications. However, LLMs can sometimes hallucinate - producing responses that are factually incorrect. This is particularly important to mitigate in enterprise environments where organizations work with proprietary information that wasn't part of the model's training data.

Retrieval-augmented generation (RAG) addresses this limitation by enabling LLMs to incorporate external knowledge sources into their response generation process. By grounding responses in retrieved facts, RAG significantly reduces hallucinations and improves the accuracy and reliability of the model's outputs.

In this tutorial, we'll cover:

* Setting up the Cohere client
* Building a RAG application by combining retrieval and chat capabilities
* Managing chat history and maintaining conversational context
* Handling direct responses vs responses requiring retrieval
* Generating citations for retrieved information

In the next tutorial, we'll explore how to leverage Cohere's tool use features to build agentic applications.

We'll use Cohere's Command, Embed, and Rerank models deployed on Azure.

## Setup

First, you will need to deploy the Command, Embed, and Rerank models on Azure via Azure AI Foundry. The deployment will create a serverless API with pay-as-you-go token based billing. You can find more information on how to deploy models in the [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio).

Once the model is deployed, you can access it via Cohere's Python SDK. Let's now install the Cohere SDK and set up our client.

To create a client, you need to provide the API key and the model's base URL for the Azure endpoint. You can get these information from the Azure AI Foundry platform where you deployed the model.

```python PYTHON
# %pip install cohere hnswlib unstructured

import cohere

co_chat = cohere.ClientV2(
    api_key="AZURE_API_KEY_CHAT",
    base_url="AZURE_ENDPOINT_CHAT",  # example: "https://cohere-command-r-plus-08-2024-xyz.eastus.models.ai.azure.com/"
)

co_embed = cohere.ClientV2(
    api_key="AZURE_API_KEY_EMBED",
    base_url="AZURE_ENDPOINT_EMBED",  # example: "https://embed-v-4-0-xyz.eastus.models.ai.azure.com/"
)

co_rerank = cohere.ClientV2(
    api_key="AZURE_API_KEY_RERANK",
    base_url="AZURE_ENDPOINT_RERANK",  # example: "https://cohere-rerank-v3-multilingual-xyz.eastus.models.ai.azure.com/"
)
```

## A quick example

Let's begin with a simple example to explore how RAG works.

The foundation of RAG is having a set of documents for the LLM to reference. Below, we'll work with a small collection of basic documents. While RAG systems usually involve retrieving relevant documents based on the user's query (which we'll explore later), for now we'll keep it simple and use this entire small set of documents as context for the LLM.

We have seen how to use the Chat endpoint in the text generation chapter. To use the RAG feature, we simply need to add one additional parameter, `documents`, to the endpoint call. These are the documents we want to provide as the context for the model to use in its response.

```python PYTHON
documents = [
    {
        "title": "Tall penguins",
        "text": "Emperor penguins are the tallest.",
    },
    {
        "title": "Penguin habitats",
        "text": "Emperor penguins only live in Antarctica.",
    },
    {
        "title": "What are animals?",
        "text": "Animals are different from plants.",
    },
]
```

Let's see how the model responds to the question "What are the tallest living penguins?"

The model leverages the provided documents as context for its response. Specifically, when mentioning that Emperor penguins are the tallest species, it references `doc_0` - the document which states that "Emperor penguins are the tallest."

```python PYTHON
message = "What are the tallest living penguins?"

response = co_chat.chat(
    model="model",  # Pass a dummy string
    messages=[{"role": "user", "content": message}],
    documents=[{"data": doc} for doc in documents],
)

print("\nRESPONSE:\n")
print(response.message.content[0].text)

if response.message.citations:
    print("\nCITATIONS:\n")
    for citation in response.message.citations:
        print(citation)
```

```mdx
RESPONSE:

The tallest living penguins are the Emperor penguins. They only live in Antarctica.

CITATIONS:

start=36 end=53 text='Emperor penguins.' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})] type=None
start=59 end=83 text='only live in Antarctica.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})] type=None
```

## A more comprehensive example

Now that we’ve covered a basic RAG implementation, let’s look at a more comprehensive example of RAG that includes:

* Creating a retrieval system that converts documents into text embeddings and stores them in an index
* Building a query generation system that transforms user messages into optimized search queries
* Implementing a chat interface to handle LLM interactions with users
* Designing a response generation system capable of handling various query types

First, let’s import the necessary libraries for this project. This includes `hnswlib` for the vector library and `unstructured` for chunking the documents (more details on these later).

```python PYTHON
import uuid
import yaml
import hnswlib
from typing import List, Dict
from unstructured.partition.html import partition_html
from unstructured.chunking.title import chunk_by_title
```

## Define documents

Next, we’ll define the documents we’ll use for RAG. We’ll use a few pages from the Cohere documentation that discuss prompt engineering. Each entry is identified by its title and URL.

```python PYTHON
raw_documents = [
    {
        "title": "Crafting Effective Prompts",
        "url": "https://docs.cohere.com/docs/crafting-effective-prompts",
    },
    {
        "title": "Advanced Prompt Engineering Techniques",
        "url": "https://docs.cohere.com/docs/advanced-prompt-engineering-techniques",
    },
    {
        "title": "Prompt Truncation",
        "url": "https://docs.cohere.com/docs/prompt-truncation",
    },
    {
        "title": "Preambles",
        "url": "https://docs.cohere.com/docs/preambles",
    },
]
```

## Create vectorstore

The Vectorstore class handles the ingestion of documents into embeddings (or vectors) and the retrieval of relevant documents given a query.

It includes a few methods:

* `load_and_chunk`: Loads the raw documents from the URL and breaks them into smaller chunks
* `embed`: Generates embeddings of the chunked documents
* `index`: Indexes the document chunk embeddings to ensure efficient similarity search during retrieval
* `retrieve`: Uses semantic search to retrieve relevant document chunks from the index, given a query. It involves two steps: first, dense retrieval from the index via the Embed endpoint, and second, a reranking via the Rerank endpoint to boost the search results further.

```python PYTHON
class Vectorstore:

    def __init__(self, raw_documents: List[Dict[str, str]]):
        self.raw_documents = raw_documents
        self.docs = []
        self.docs_embs = []
        self.retrieve_top_k = 10
        self.rerank_top_k = 3
        self.load_and_chunk()
        self.embed()
        self.index()

    def load_and_chunk(self) -> None:
        """
        Loads the text from the sources and chunks the HTML content.
        """
        print("Loading documents...")

        for raw_document in self.raw_documents:
            elements = partition_html(url=raw_document["url"])
            chunks = chunk_by_title(elements)
            for chunk in chunks:
                self.docs.append(
                    {
                        "data": {
                            "title": raw_document["title"],
                            "text": str(chunk),
                            "url": raw_document["url"],
                        }
                    }
                )

    def embed(self) -> None:
        """
        Embeds the document chunks using the Cohere API.
        """
        print("Embedding document chunks...")

        batch_size = 90
        self.docs_len = len(self.docs)
        for i in range(0, self.docs_len, batch_size):
            batch = self.docs[i : min(i + batch_size, self.docs_len)]
            texts = [item["data"]["text"] for item in batch]
            docs_embs_batch = co_embed.embed(
                texts=texts,
                model="embed-v4.0",
                input_type="search_document",
                embedding_types=["float"],
            ).embeddings.float
            self.docs_embs.extend(docs_embs_batch)

    def index(self) -> None:
        """
        Indexes the document chunks for efficient retrieval.
        """
        print("Indexing document chunks...")

        self.idx = hnswlib.Index(space="ip", dim=1024)
        self.idx.init_index(
            max_elements=self.docs_len, ef_construction=512, M=64
        )
        self.idx.add_items(
            self.docs_embs, list(range(len(self.docs_embs)))
        )

        print(
            f"Indexing complete with {self.idx.get_current_count()} document chunks."
        )

    def retrieve(self, query: str) -> List[Dict[str, str]]:
        """
        Retrieves document chunks based on the given query.

        Parameters:
        query (str): The query to retrieve document chunks for.

        Returns:
        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.
        """

        # Dense retrieval
        query_emb = co_embed.embed(
            texts=[query],
            model="embed-v4.0",
            input_type="search_query",
            embedding_types=["float"],
        ).embeddings.float

        doc_ids = self.idx.knn_query(
            query_emb, k=self.retrieve_top_k
        )[0][0]

        # Reranking
        docs_to_rerank = [
            self.docs[doc_id]["data"] for doc_id in doc_ids
        ]
        yaml_docs = [
            yaml.dump(doc, sort_keys=False) for doc in docs_to_rerank
        ]
        rerank_results = co_rerank.rerank(
            query=query,
            documents=yaml_docs,
            model="model",  # Pass a dummy string
            top_n=self.rerank_top_k,
        )

        doc_ids_reranked = [
            doc_ids[result.index] for result in rerank_results.results
        ]

        docs_retrieved = []
        for doc_id in doc_ids_reranked:
            docs_retrieved.append(self.docs[doc_id]["data"])

        return docs_retrieved
```

## Process documents

With the Vectorstore set up, we can process the documents, which will involve chunking, embedding, and indexing.

```python PYTHON
# Create an instance of the Vectorstore class with the given sources
vectorstore = Vectorstore(raw_documents)
```

```mdx
Loading documents...
Embedding document chunks...
Indexing document chunks...
Indexing complete with 137 document chunks.
```

We can test if the retrieval is working by entering a search query.

```python PYTHON
vectorstore.retrieve("Prompting by giving examples")
```

```mdx
[{'title': 'Advanced Prompt Engineering Techniques',
  'text': 'Few-shot Prompting\n\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.',
  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'},
 {'title': 'Crafting Effective Prompts',
  'text': 'Incorporating Example Outputs\n\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.',
  'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'},
 {'title': 'Advanced Prompt Engineering Techniques',
  'text': 'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.',
  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}]
```

## Run chatbot

We can now run the chatbot. For this, we create a `run_chatbot` function that accepts the user message and the history of the conversation, if available.

```python PYTHON
def run_chatbot(query, messages=None):
    if messages is None:
        messages = []

    messages.append({"role": "user", "content": query})

    # Retrieve document chunks and format
    documents = vectorstore.retrieve(query)
    documents_formatted = []
    for doc in documents:
        documents_formatted.append({"data": doc})

    # Use document chunks to respond
    response = co_chat.chat(
        model="model",  # Pass a dummy string
        messages=messages,
        documents=documents_formatted,
    )

    # Print the chatbot response, citations, and documents
    print("\nRESPONSE:\n")
    print(response.message.content[0].text)

    if response.message.citations:
        print("\nCITATIONS:\n")
        for citation in response.message.citations:
            print("-" * 20)
            print(
                "start:",
                citation.start,
                "end:",
                citation.end,
                "text:",
                citation.text,
            )
            print("SOURCES:")
            print(citation.sources)

    # Add assistant response to messages
    messages.append(
        {
            "role": "assistant",
            "content": response.message.content[0].text,
        }
    )

    return messages
```

Here is a sample conversation consisting of a few turns.

```python PYTHON
messages = run_chatbot("Hello, I have a question")
```

```mdx
RESPONSE:

Hello there! How can I help you today?
```

```python PYTHON
messages = run_chatbot("How to provide examples in prompts", messages)
```

```
RESPONSE:

There are a few ways to provide examples in prompts.

One way is to provide a few relevant and diverse examples in the prompt. This can help steer the LLM towards a high-quality solution. Good examples condition the model to the expected response type and style.

Another way is to provide specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.

In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses.

CITATIONS:

--------------------
start: 68 end: 126 text: provide a few relevant and diverse examples in the prompt.
SOURCES:
[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Few-shot Prompting\n\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'})]
--------------------
start: 136 end: 187 text: help steer the LLM towards a high-quality solution.
SOURCES:
[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Few-shot Prompting\n\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'})]
--------------------
start: 188 end: 262 text: Good examples condition the model to the expected response type and style.
SOURCES:
[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Few-shot Prompting\n\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'})]
--------------------
start: 282 end: 321 text: provide specific examples to work from.
SOURCES:
[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Incorporating Example Outputs\n\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.', 'title': 'Crafting Effective Prompts', 'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'})]
--------------------
start: 335 end: 485 text: instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.
SOURCES:
[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Incorporating Example Outputs\n\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.', 'title': 'Crafting Effective Prompts', 'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'})]
--------------------
start: 527 end: 679 text: including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses.
SOURCES:
[DocumentSource(type='document', id='doc:2', document={'id': 'doc:2', 'text': 'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'})]
```

```python PYTHON
messages = run_chatbot(
    "What do you know about 5G networks?", messages
)
```

```mdx
RESPONSE:

I'm sorry, I could not find any information about 5G networks.
```

```python PYTHON
for message in messages:
    print(message, "\n")
```

```mdx
{'role': 'user', 'content': 'Hello, I have a question'} 

{'role': 'assistant', 'content': 'Hello! How can I help you today?'} 

{'role': 'user', 'content': 'How to provide examples in prompts'} 

{'role': 'assistant', 'content': 'There are a few ways to provide examples in prompts.\n\nOne way is to provide a few relevant and diverse examples in the prompt. This can help steer the LLM towards a high-quality solution. Good examples condition the model to the expected response type and style.\n\nAnother way is to provide specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.\n\nIn addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses.'} 

{'role': 'user', 'content': 'What do you know about 5G networks?'} 

{'role': 'assistant', 'content': "I'm sorry, I could not find any information about 5G networks."} 

```

There are a few observations worth pointing out:

* Direct response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval.
* Citation generation: For responses that do require retrieval ("What's the difference between zero-shot and few-shot prompting"), the endpoint returns the response together with the citations. These are fine-grained citations, which means they refer to specific spans of the generated text.
* Response synthesis: The model can decide if none of the retrieved documents provide the necessary information to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot retrieves external information from the index. However, it doesn’t use any of the information in its response as none of it is relevant to the question.

## Conclusion

In this tutorial, we learned about:

* How to set up the Cohere client to use the Command model deployed on Azure AI Foundry for chat
* How to build a RAG application by combining retrieval and chat capabilities
* How to manage chat history and maintain conversational context
* How to handle direct responses vs responses requiring retrieval
* How citations are automatically generated for retrieved information

In the next tutorial, we'll explore how to leverage Cohere's tool use features to build agentic applications.


# Tool use & agents - Cohere on Azure AI Foundry

> A guide for using tool use and building agents with Cohere's Command models on Azure AI Foundry (API v2).

[Open in GitHub](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/cohere-on-azure/v2/azure-ai-tool-use.ipynb)

Tool use enhances retrieval-augmented generation (RAG) capabilities by enabling applications to both answer questions and automate tasks.

Tools provide a broader access to external systems compared to traditional RAG. This approach leverages LLMs' inherent ability to reason and make decisions. By incorporating tools, developers can create agent-like applications that interact with external systems through both read and write operations.

In this chapter, we'll explore how to build an agentic application by building an agent that can answer questions and automate tasks, enabled by a number of tools.

## Setup

First, you will need to deploy the Command model on Azure via Azure AI Foundry. The deployment will create a serverless API with pay-as-you-go token based billing. You can find more information on how to deploy models in the [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio).

In the example below, we are deploying the Command R+ (August 2024) model.

Once the model is deployed, you can access it via Cohere's Python SDK. Let's now install the Cohere SDK and set up our client.

To create a client, you need to provide the API key and the model's base URL for the Azure endpoint. You can get these information from the Azure AI Foundry platform where you deployed the model.

```python PYTHON
# %pip install cohere
import cohere

co = cohere.ClientV2(
    api_key="AZURE_API_KEY_CHAT",
    base_url="AZURE_ENDPOINT_CHAT",  # example: "https://cohere-command-r-plus-08-2024-xyz.eastus.models.ai.azure.com/"
)
```

## Create tools

The pre-requisite, before we can run a tool use workflow, is to set up the tools. Let's create three tools:

* `search_faqs`: A tool for searching the FAQs of a company. For simplicity, we'll not implement any retrieval logic, but we'll simply pass a list of three predefined documents. In practice, we would set up a retrieval system as we did in Chapters 4, 5, and 6.
* `search_emails`: A tool for searching the emails. Same as above, we'll simply pass a list of predefined emails.
* `create_calendar_event`: A tool for creating new calendar events. Again, for simplicity, we'll only return mock successful event creations without actual implementation. In practice, we can connect to a calendar service API and implement all the necessary logic here.

Here, we are defining a Python function for each tool, but more broadly, the tool can be any function or service that can receive and send objects.

```python PYTHON
def search_faqs(query):
    faqs = [
        {
            "text": "Submitting Travel Expenses:\nSubmit your expenses through our user-friendly finance tool."
        },
        {
            "text": "Side Projects Policy:\nWe encourage you to explore your passions! Just ensure there's no conflict of interest with our business."
        },
        {
            "text": "Wellness Benefits:\nTo promote a healthy lifestyle, we provide gym memberships, on-site yoga classes, and health insurance."
        },
    ]
    return faqs


def search_emails(query):
    emails = [
        {
            "from": "hr@co1t.com",
            "to": "david@co1t.com",
            "date": "2024-06-24",
            "subject": "A Warm Welcome to Co1t, David!",
            "text": "We are delighted to have you on board. Please find attached your first week's agenda.",
        },
        {
            "from": "it@co1t.com",
            "to": "david@co1t.com",
            "date": "2024-06-24",
            "subject": "Instructions for IT Setup",
            "text": "Welcome, David! To get you started, please follow the attached guide to set up your work accounts.",
        },
        {
            "from": "john@co1t.com",
            "to": "david@co1t.com",
            "date": "2024-06-24",
            "subject": "First Week Check-In",
            "text": "Hi David, let's chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!",
        },
    ]
    return emails


def create_calendar_event(date: str, time: str, duration: int):
    # You can implement any logic here
    return {
        "is_success": True,
        "message": f"Created a {duration} hour long event at {time} on {date}",
    }


functions_map = {
    "search_faqs": search_faqs,
    "search_emails": search_emails,
    "create_calendar_event": create_calendar_event,
}
```

## Define tool schemas

The next step is to define the tool schemas in a format that can be accepted by the Chat endpoint. The schema must contain the following fields: `name`, `description`, and `parameter_definitions`.

This schema informs the LLM about what the tool does, and the LLM decides whether to use a particular tool based on it. Therefore, the more descriptive and specific the schema, the more likely the LLM will make the right tool call decisions.

```python PYTHON
tools = [
    {
        "type": "function",
        "function": {
            "name": "search_faqs",
            "description": "Given a user query, searches a company's frequently asked questions (FAQs) list and returns the most relevant matches to the query.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The query from the user",
                    }
                },
                "required": ["query"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "search_emails",
            "description": "Given a user query, searches a person's emails and returns the most relevant matches to the query.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The query from the user",
                    }
                },
                "required": ["query"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "create_calendar_event",
            "description": "Creates a new calendar event of the specified duration at the specified time and date. A new event cannot be created on the same time as an existing event.",
            "parameters": {
                "type": "object",
                "properties": {
                    "date": {
                        "type": "string",
                        "description": "the date on which the event starts, formatted as mm/dd/yy",
                    },
                    "time": {
                        "type": "string",
                        "description": "the time of the event, formatted using 24h military time formatting",
                    },
                    "duration": {
                        "type": "number",
                        "description": "the number of hours the event lasts for",
                    },
                },
                "required": ["date", "time", "duration"],
            },
        },
    },
]
```

## Run agent

Now, let's set up the agent using Cohere's tool use feature. We can think of a tool use system as consisting of four components:

* The user
* The application
* The LLM
* The tools

At its most basic, these four components interact in a workflow through four steps:

* Step 1: Get user message. The LLM gets the user message (via the application).
* Step 2: Generate tool calls. The LLM makes a decision on the tools to call (if any) and generates the tool calls.
* Step 3: Get tool results. The application executes the tools and sends the tool results to the LLM.
* Step 4: Generate response and citations. The LLM generates the response and citations and sends them back to the user.

Let's create a function called `run_assistant` to implement these steps and print out the key events and messages along the way. This function also optionally accepts the chat history as an argument to keep the state in a multi-turn conversation.

```python PYTHON
import json

system_message = """## Task and Context
You are an assistant who assists new employees of Co1t with their first week. You respond to their questions and assist them with their needs. Today is Monday, June 24, 2024"""


def run_assistant(query, messages=None):
    if messages is None:
        messages = []

    if "system" not in {m.get("role") for m in messages}:
        messages.append({"role": "system", "content": system_message})

    # Step 1: get user message
    print(f"Question:\n{query}")
    print("=" * 50)

    messages.append({"role": "user", "content": query})

    # Step 2: Generate tool calls (if any)
    response = co.chat(
        model="model",  # Pass a dummy string
        messages=messages,
        tools=tools,
    )

    while response.message.tool_calls:

        print("Tool plan:")
        print(response.message.tool_plan, "\n")
        print("Tool calls:")
        for tc in response.message.tool_calls:
            print(
                f"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}"
            )
        print("=" * 50)

        messages.append(
            {
                "role": "assistant",
                "tool_calls": response.message.tool_calls,
                "tool_plan": response.message.tool_plan,
            }
        )

        # Step 3: Get tool results
        for idx, tc in enumerate(response.message.tool_calls):
            tool_result = functions_map[tc.function.name](
                **json.loads(tc.function.arguments)
            )
            tool_content = []
            for data in tool_result:
                tool_content.append(
                    {
                        "type": "document",
                        "document": {"data": json.dumps(data)},
                    }
                )
                # Optional: add an "id" field in the "document" object, otherwise IDs are auto-generated
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tc.id,
                    "content": tool_content,
                }
            )

        # Step 4: Generate response and citations
        response = co.chat(
            model="model",  # Pass a dummy string
            messages=messages,
            tools=tools,
        )

    messages.append(
        {
            "role": "assistant",
            "content": response.message.content[0].text,
        }
    )

    # Print final response
    print("Response:")
    print(response.message.content[0].text)
    print("=" * 50)

    # Print citations (if any)
    if response.message.citations:
        print("\nCITATIONS:")
        for citation in response.message.citations:
            print(citation, "\n")

    return messages
```

Let’s now run the agent. We'll use an example of a new hire asking about IT access and the travel expense process.

Given three tools to choose from, the model is able to pick the right tools (in this case, `search_faqs` and `search_emails`) based on what the user is asking for.

Also, notice that the model first generates a plan about what it should do ("I will ...") before actually generating the tool call(s).

Additionally, the model also generates fine-grained citations in tool use mode based on the tool results it receives, the same way we saw with RAG.

```python PYTHON
messages = run_assistant(
    "Any doc on how do I submit travel expenses? Also, any emails about setting up IT access?"
)
```

```mdx
Question:
Any doc on how do I submit travel expenses? Also, any emails about setting up IT access?
==================================================
Tool plan:
I will search for a document on how to submit travel expenses, and also search for emails about setting up IT access. 

Tool calls:
Tool name: search_faqs | Parameters: {"query":"how to submit travel expenses"}
Tool name: search_emails | Parameters: {"query":"setting up IT access"}
==================================================
Response:
You can submit your travel expenses through the user-friendly finance tool.

You should have received an email from it@co1t.com with instructions for setting up your IT access.
==================================================

CITATIONS:
start=48 end=75 text='user-friendly finance tool.' sources=[ToolSource(type='tool', id='search_faqs_wkfggn2680c4:0', tool_output={'text': 'Submitting Travel Expenses:\nSubmit your expenses through our user-friendly finance tool.'})] type='TEXT_CONTENT' 

start=105 end=176 text='email from it@co1t.com with instructions for setting up your IT access.' sources=[ToolSource(type='tool', id='search_emails_8n0cvsh5xknt:1', tool_output={'date': '2024-06-24', 'from': 'it@co1t.com', 'subject': 'Instructions for IT Setup', 'text': 'Welcome, David! To get you started, please follow the attached guide to set up your work accounts.', 'to': 'david@co1t.com'})] type='TEXT_CONTENT' 
```

## Conclusion

In this tutorial, we learned about:

* How to set up tools with parameter definitions for the Cohere chat API
* How to define tools for building agentic applications
* How to set up the agent
* How to run a tool use workflow involving the user, the application, the LLM, and the tools


# Usage Policy

> Developers must outline and get approval for their use case to access the Cohere API, understanding the models and limitations. They should refer to model cards for detailed information and document potential harms of their application. Certain use cases, such as violence, hate speech, fraud, and privacy violations, are strictly prohibited.

(This document was updated on 11/21/2024)

Our Usage Policy applies to all Cohere products and services, including Cohere models, software, applications, and application programming interface (collectively *“Cohere Services”*).

The Usage Policy sets out universal requirements that apply to all users of the Cohere Services, and specific additional requirements that apply to users who create customer applications that integrate Cohere Services (each, a *“Customer Application”*).

We may update this Usage Policy from time to time by posting an updated version on our website.

If we learn that you have violated this Usage Policy or are otherwise misusing or abusing Cohere Services, we are entitled to restrict, suspend, or terminate your access to the Cohere Services. If you become aware of a violation of this Usage Policy, including by any Outputs, please notify us immediately at [safety@cohere.com](mailto:safety@cohere.com). If you are using the Cohere Services in our SaaS Platform, you can also report issues by using the thumbs down button on an Output.  “Outputs” means any information, text, image, audio or video content artificially created by Cohere Services.

## Universal Requirements

You must not use the Cohere Services to engage in, facilitate, or promote any of the following prohibited activities. Descriptions of prohibited activities are illustrative, not exhaustive.

* **Child Sexual Exploitation and Sexually Explicit Content Involving Minors**. Any activity that exploits, abuses, or endangers children, or otherwise compromises the safety of children; or any generation, creation, sharing, or facilitation of sexually explicit content involving minors, including pornographic content or content intended for sexual arousal or gratification. We will report child sexual abuse material that we become aware of to competent authorities and other organizations as appropriate.
* **Incitement of Violence or Harm.** Any use of the Cohere Services that (1) incites violence, threats, extremism, or terrorism; (2) glorifies or facilitates self-harm; (3) is sexually exploitative or abusive; (4) constitutes hate speech; or (5) promotes or glorifies racism, discrimination, hatred, or abuse, against any group or individual based on protected characteristics like race, ethnicity, national origin, religion, disability, sexual orientation, gender, or gender identity.
* **Illegal Activities.** Any illegal activity, or other violation of applicable law, including providing instructions on how to commit crimes, facilitating illegal activities or intentionally generating Outputs that may infringe, violate, or misappropriate the intellectual property rights of a third party.
* **Weapons and Controlled Substances.** Any activities that relate to the production, sale, trafficking, or marketing of weapons or controlled substances.
* **Compromising Privacy or Identity.** Violation of a person’s privacy rights or applicable privacy regulations, including unlawful access to or tracking of a person’s physical location; unlawful social scoring; real-time identification of a person or inference of emotions or protected characteristics of a person such as race or political opinions based on biometric data (including facial recognition);  or other unauthorized access to personal information.
* **Compromising Security.** Use of the Cohere Services to (1) compromise security or attempt to gain unauthorized access to computer systems or networks; (2) generate or propagate spam or carry out phishing or social engineering campaigns; (3) create or process any viruses or other computer programming routines that may damage, detrimentally interfere with, surreptitiously intercept, or expropriate any system or data; or (4) otherwise violate the integrity, availability, or confidentiality of a user, network, computing device, communications system, or software application.
* **Surveillance and Predictive Policing.** Any activities involving illegal profiling or surveillance, including spyware or communications surveillance, untargeted scraping of facial images to create or expand a facial recognition database, or predictive policing, i.e., assessing or predicting the risks of a person committing a criminal offence.
* **Fraudulent, Abusive, Misleading, or Deceptive Practices.** Use of the Cohere Services to (1) generate inauthentic content representing real persons, places, entities, events, or objects that could falsely appear as authentic or truthful (so-called “deep fakes”) or as having been created by a human (e.g., fake reviews) in a manner that is misleading, deceiving or harmful to persons, groups, or entities; (2) engage in academic dishonesty; (3) deploy subliminal or purposefully deceptive techniques to distort behaviour or impair decision-making in a manner that is reasonably likely to cause significant harm; or (4) engage in deceptive or abusive practices that exploit vulnerabilities such as age, socio-economic status, or disability (e.g. misleading advertising, exploitative lending or debt collection practices, or high-pressure sales tactics).
* **Misinformation and Political Campaigning/Lobbying.** Creation or promotion of harmful misinformation and disinformation, including defamatory or libelous content and political propaganda; attempting to manipulate public opinion on issues such as health, safety, government policies, laws, or political campaigns or politicians; or deterring people from participating in or otherwise attempting to disrupt democratic processes, including misrepresenting voting processes or qualifications and discouraging voting.
* **Abusing Cohere Services.** Any activities that aim to (1) circumvent, disable or otherwise interfere with security, safety or technical features or protocols; (2) exploit a vulnerability; or (3) otherwise intentionally bypass restrictions of the Cohere Services, including through jailbreaking, prompt injection attacks, or automation to circumvent bans or usage limitations.
* **High Risk Activities.** Activities (1) where the use or failure of the Cohere Services could reasonably be expected to result in death, harm to psychological or physical health or safety, or severe environmental or property damage; or (2) that use the Cohere Services for automated determinations about individuals in domains that affect their rights, safety, or access to essential services and benefits (e.g., employment, education, healthcare, migration, housing, law enforcement, legal advice/decisions, or financial or insurance products or services). For the avoidance of doubt, backoffice uses (e.g., document summarization, transcription, internal knowledge agents, etc.) are not considered High Risk Activities under this Usage Policy.

## Customer Application Requirements

You must ensure your Customer Application complies with the Universal Requirements of this Usage Policy and that users of your Customer Application understand and are required to comply with substantially similar requirements.

If your Customer Application is public-facing and interacts with human users (including consumers), like chatbots and interactive AI agents, you must: (1) disclose to the users that they are interacting with an AI system rather than a human; and (2) if the Customer Application interacts with minors, comply with any specific child safety regulations and implement appropriate additional safety controls such as age verification and content moderation.

## Research Exceptions

Cohere encourages responsible security and safety research. Limited exceptions to our Usage Policy are possible for research purposes if specifically authorized by us or permitted in accordance with our Responsible Disclosure Policy applicable to security research. For safety-related research that falls outside the scope of our [Responsible Disclosure Policy](https://trustcenter.cohere.com/) or to report a model safety issue, please contact [safety@cohere.com](mailto:safety@cohere.com).


# Command R and Command R+ Model Card

> This doc provides guidelines for using Cohere generation models ethically and constructively.

This documentation aims to guide developers in using language models constructively and ethically. To this end, we've included information below on how our Command R and Command R+ models perform on important safety benchmarks, the intended (and unintended) use cases they support, toxicity, and other technical specifications.

\[NOTE: This page was updated on October 31st, 2024.]

## Safety Benchmarks

The safety of our Command R and Command R+ models has been evaluated on the BOLD (Biases in Open-ended Language Generation) dataset (Dhamala et al, 2021), which contains nearly 24,000 prompts testing for biases based on profession, gender, race, religion, and political ideology.

Overall, both models show a lack of bias, with generations that are very rarely toxic. That said, there remain some differences in bias between the two, as measured by their respective sentiment and regard for "Gender" and "Religion" categories. Command R+, the more powerful model, tends to display slightly less bias than Command R.

Below, we report differences in privileged vs. minoritised groups for gender, race, and religion.

![](file:0f52ed99-2eb5-4093-9b1f-d0d0224fb399)

## Intended Use Cases

Command R models are trained for sophisticated text generation—which can include natural text, summarization, code, and markdown—as well as to support complex [Retrieval Augmented Generation](https://docs.cohere.com/docs/retrieval-augmented-generation-rag) (RAG) and [tool-use](https://docs.cohere.com/docs/tool-use) tasks.

Command R models support 23 languages, including 10 languages that are key to global business (English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Chinese, Arabic). While it has strong performance on these ten languages, the other 13 are lower-resource and less rigorously evaluated.

## Unintended and Prohibited Use Cases

We do not recommend using the Command R models on their own for decisions that could have a significant impact on individuals, including those related to access to financial services, employment, and housing.

Cohere’s [Usage Guidelines](https://cohere.com/responsibility) and customer agreements contain details about prohibited use cases, like social scoring, inciting violence or harm, and misinformation or other political manipulation.

## Usage Notes

For general guidance on how to responsibly leverage the Cohere platform, we recommend you consult our [Usage Guidelines](https://docs.cohere.com/docs/usage-guidelines) page.

In the next few sections, we offer some model-specific usage notes.

### Model Toxicity and Bias

Language  models  learn  the  statistical  relationships  present  in  training  datasets,  which  may  include toxic language and historical biases along race, gender, sexual orientation, ability, language, cultural, and  intersectional  dimensions. We recommend that developers be especially attuned to risks presented by toxic degeneration and the reinforcement of historical social biases.

#### Toxic  Degeneration

Models have been trained on a wide variety of text from many sources that contain toxic content (see Luccioni and Viviano, 2021). As a result, models  may  generate  toxic  text. This  may  include  obscenities, sexually  explicit content, and  messages  which  mischaracterize or stereotype  groups of people based on problematic historical biases perpetuated by internet communities (see Gehman et al., 2020 for more about toxic language model degeneration).

We have put safeguards in place to avoid generating harmful text, and while they are effective (see the "Safety Benchmarks" section above), it is still possible to encounter toxicity, especially over long conversations with multiple turns.

#### Reinforcing Historical Social Biases

Language models capture problematic associations and stereotypes that are prominent on the internet and society at large. They should not be used to make decisions about individuals or the groups they belong to. For example, it can be dangerous to use Generation model outputs in CV ranking systems due to known biases (Nadeem et al., 2020).

## Technical Notes

Now, we'll discuss some details of our underlying models that should be kept in mind.

### Language Limitations

This model is designed to excel at English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Chinese, and Arabic, and to generate in 13 other languages well. It will sometimes respond in other languages, but the generations are unlikely to be reliable.

### Sampling Parameters

A model's generation quality is highly dependent on its sampling parameters. Please consult [the documentation](https://docs.cohere.com/docs/advanced-generation-hyperparameters) for details about each parameter and tune the values used for your application. Parameters may require re-tuning upon a new model release.

### Prompt Engineering

Performance quality on generation tasks may increase when examples
are provided as part of the system prompt. See [the documentation](https://docs.cohere.com/docs/crafting-effective-prompts) for examples on how to do this.

### Potential for Misuse

Here we describe potential concerns around misuse of the Command R models, drawing on the NAACL Ethics Review Questions. By documenting adverse use cases, we aim to empower customers to prevent  adversarial actors from leveraging customer applications for the following malicious ends.

The examples in this section are not comprehensive; they are meant to be more model-specific and tangible than those in the Usage Guidelines, and are only meant to illustrate our understanding of potential harms. Each of these malicious use cases violates our Usage Guidelines and Terms of Use, and Cohere reserves the right to restrict API access at any time.

* **Astroturfing:** Generated text used to provide the illusion of discourse or expression of opinion
  by members of the public, on social media or any other channel.
* **Generation of misinformation and other harmful content:**  The generation of news or other
  articles which manipulate public opinion, or any content which aims to incite hate or mischaracterize a group of people.
* **Human-outside-the-loop:** The generation of text that could be used to make important decisions about people, without a human-in-the-loop.


# Cohere Labs Acceptable Use Policy

> "Promoting safe and ethical use of generative AI with guidelines to prevent misuse and abuse."

We believe that independent and open machine learning research is vital to realizing the benefits of generative AI equitably and ensuring robust assessments of risks of generative AI use.

We expect users of our models or model derivatives to comply with all applicable local and international laws and regulations. Additionally, you may not use or allow others to use our models or model derivatives in connection with any of the following strictly prohibited use cases:

**Violence and harm:** engaging in, promoting, or inciting violence, threats, hate speech self-harm, sexual exploitation, or targeting of individuals based on protected characteristics.

**Harassment and abuse:** engaging in, promoting, facilitating, or inciting activities that harass or abuse individuals or groups.

**Sexual exploitation, harm, or abuse:** encouraging, facilitating, or carrying out any activities that exploit, abuse, or endanger individuals, and particularly children, including soliciting, creating, or distributing child exploitative content or Child Sexual Abuse Material.

**Sensitive information:** collecting, disclosing, or inferring health, demographic, or other sensitive personal or private information about individuals without lawful authority or obtaining all rights and consents required by applicable laws.

**Fraud and deception:** misrepresenting generated content from models as human-created or allowing individuals to create false identities for malicious purposes, deception, or to cause harm, through methods including:

* propagation of spam, fraudulent activities such as catfishing, phishing, or generation of false reviews;
* creation or promotion of false representations of or defamatory content about real people, such as deepfakes; or
* creation or promotion of intentionally false claims or misinformation.

**Synthetic data for commercial uses:** generating synthetic data outputs for commercial purposes, including to train, improve, benchmark, enhance or otherwise develop model derivatives, or any products or services in connection with the foregoing.


# How to Start with the Cohere Toolkit

> Build and deploy RAG applications quickly with the Cohere Toolkit, which offers pre-built front-end and back-end components.

[Cohere Toolkit](https://github.com/cohere-ai/cohere-toolkit) is a collection of pre-built components enabling developers to quickly build and deploy [retrieval augmented generation](/docs/retrieval-augmented-generation-rag) (RAG) applications. With it, you can cut time-to-launch down from months to weeks, and deploy in as little as a few minutes.

The pre-built components fall into two big categories: front-end and back end.

* **Front-end**: The Cohere Toolkit front end is a web application built in Next.js. It includes a simple SQL database out of the box to store conversation history, documents, and citations, directly in the app.
* **Back-end**: The Cohere Toolkit back-end contains the preconfigured data sources and retrieval code needed to set up RAG on custom data sources, which are called "retrieval chains"). Users can also configure which model to use, selecting from Cohere models hosted on our native platform, Azure, or AWS Sagemaker. By default, we have configured a Langchain data retriever to test RAG on Wikipedia and your own uploaded documents.

Here's an image that shows how these different components work together:

<img src="file:675a981f-50fb-41e8-833a-5cde990ba811" />

## Cohere Toolkit Quick Start

You can get started quickly with toolkit on Google Cloud Run, Microsoft Azure, or locally. [Read this](https://github.com/cohere-ai/cohere-toolkit?tab=readme-ov-file#quick-start) for more details, including CLI commands to run after cloning the repo and environment variables that need to be set.

## Deploying Cohere Toolkit

The toolkit can be deployed on single containers, AWS ECS, and GCP. Find out how [here](https://github.com/cohere-ai/cohere-toolkit?tab=readme-ov-file#deployment-guides).

## Developing on Cohere Toolkit

If you want to configure old retrieval chains or add new ones, you'll need to work through a few steps. These include installing poetry, setting up your local database, testing, etc. More context is available [here](https://github.com/cohere-ai/cohere-toolkit?tab=readme-ov-file#setup-for-development).

## Working with Cohere Toolkit

The toolkit is powerful and flexible. There's a lot you can do with it, including adding your own [model deployment](https://github.com/cohere-ai/cohere-toolkit?tab=readme-ov-file#how-to-add-your-own-model-deployment), calling the toolkit's backend [over the API](https://github.com/cohere-ai/cohere-toolkit?tab=readme-ov-file#how-to-call-the-backend-as-an-api), adding a [connector](https://github.com/cohere-ai/cohere-toolkit?tab=readme-ov-file#how-to-add-a-connector-to-the-toolkit), and much else besides.

Following the links in this document or [read the full repository](https://github.com/cohere-ai/cohere-toolkit) to find everything you need!


# The Cohere Datasets API (and How to Use It)

> Learn about the Dataset API, including its file size limits, data retention, creation, validation, metadata, and more, with provided code snippets.

The Cohere platform allows you to upload and manage datasets that can be used in  batch embedding with [Embedding Jobs](/docs/embed-jobs-api). Datasets can be managed [in the Dashboard](https://dashboard.cohere.com/datasets) or programmatically using the [Datasets API](/reference/create-dataset).

![](file:20ae1ef7-8eff-47f3-beb4-a26d09d8f8ee)

<br />

### File Size Limits

There are certain limits to the files you can upload, specifically:

* A Dataset can be as large as 1.5GB
* Organizations have up to 10GB of storage across all their users

### Retention

You should also be aware of how Cohere handles data retention. This is the most important context:

* Datasets get deleted 30 days after creation
* You can also manually delete a dataset in the Dashboard UI or [using the Datasets API](/reference/delete-dataset)

## Managing Datasets using the Python SDK

### Getting Set up

First, let's install the SDK

```bash
pip install cohere
```

Import dependencies and set up the Cohere client.

```python PYTHON
import cohere

co = cohere.Client(api_key="Your API key")
```

(All the rest of the examples on this page will be in Python, but you can find more detailed instructions for getting set up by checking out the Github repositories for [Python](https://github.com/cohere-ai/cohere-python), [Typescript](https://github.com/cohere-ai/cohere-typescript), and [Go](https://github.com/cohere-ai/cohere-go).)

### Dataset Creation

Datasets are created by uploading files, specifying both a `name` for the dataset and the dataset `type`.

The file extension and file contents have to match the requirements for the selected dataset `type`. See the [table below](#supported-dataset-types) to learn more about the supported dataset types.

The dataset `name` is useful when browsing the datasets you've uploaded. In addition to its name, each dataset will also be assigned a unique `id` when it's created.

Here is an example code snippet illustrating the process of creating a dataset, with both the `name` and the dataset `type` specified.

```python PYTHON
my_dataset = co.datasets.create(
    name="shakespeare",
    data=open("./shakespeare.jsonl", "rb"),
    type="embed-input",
)

print(my_dataset.id)
```

### Dataset Validation

Whenever a dataset is created, the data is validated asynchronously against the rules for the specified dataset `type` . This validation is kicked off automatically on the backend, and must be completed before a dataset can be used with other endpoints.

Here's a code snippet showing how to check the validation status of a dataset you've created.

```python PYTHON
ds = co.wait(my_dataset)
print(ds.dataset.validation_status)
```

To help you interpret the results, here's a table specifying all the possible API error messages and what they mean:

| Error Code | Endpoint   | Error Explanation                                                          | Actions Required                                                                   |
| :--------- | :--------- | :------------------------------------------------------------------------- | :--------------------------------------------------------------------------------- |
| 400        | Create     | The name parameter must be set.                                            | Set a name parameter.                                                              |
| 400        | Create     | The type parameter must be set.                                            | Set a type parameter.                                                              |
| 400        | Create     | The dataset type is invalid.                                               | Set the type parameter to a supported type.                                        |
| 400        | Create     | You have exceeded capacity.                                                | Delete unused datasets to free up space.                                           |
| 400        | Create     | You have used an invalid csv delimiter.                                    | The csv delimiter must be one character long.                                      |
| 400        | Create     | The name must be less than 50 characters long.                             | Shorten your dataset name.                                                         |
| 400        | Create     | You used an invalid parameter for part: %v use file or an evaluation file. | The file parameters must be a named file or an evaluation file.                    |
| 499        | Create     | The upload connection was closed.                                          | Don't cancel the upload request.                                                   |
|            | Validation | The required field {} was not fund in the dataset (line: {})               | You are missing a required field, which must be supplied.                          |
|            | Validation | Custom validation rules per type                                           | There should be enough context in the validation error message to fix the dataset. |
|            | Validation | csv files must have a header with the required fields: \[{}, {}, ...].     | Fix your csv file to have a 'headers' row with the required field names.           |
| 404        | Get        | The dataset with id '{}' was not found.                                    | Make sure you're passing in the right id.                                          |

### Dataset Metadata Preservation

The Dataset API will preserve metadata if specified at time of upload. During the `create dataset` step, you can specify either `keep_fields` or `optional_fields` which are a list of strings which correspond to the field of the metadata you’d like to preserve. `keep_fields` is more restrictive, where if the field is missing from an entry, the dataset will fail validation whereas `optional_fields` will skip empty fields and validation will still pass.

#### Sample Dataset Input Format

```text JSONL
{"wiki_id": 69407798, "url": "https://en.wikipedia.org/wiki?curid=69407798", "views": 5674.4492597435465, "langs": 38, "title": "Deaths in 2022", "text": "The following notable deaths occurred in 2022. Names are reported under the date of death, in alphabetical order. A typical entry reports information in the following sequence:", "paragraph_id": 0, "id": 0}
{"wiki_id": 3524766, "url": "https://en.wikipedia.org/wiki?curid=3524766", "views": 5409.5609619796405, "title": "YouTube", "text": "YouTube is a global online video sharing and social media platform headquartered in San Bruno, California. It was launched on February 14, 2005, by Steve Chen, Chad Hurley, and Jawed Karim. It is owned by Google, and is the second most visited website, after Google Search. YouTube has more than 2.5 billion monthly users who collectively watch more than one billion hours of videos each day. , videos were being uploaded at a rate of more than 500 hours of content per minute.", "paragraph_id": 0, "id": 1}
```

As seen in the above example, the following would be a valid `create_dataset` call since `langs` is in the first entry but not in the second entry. The fields `wiki_id`, `url`, `views` and `title` are present in both JSONs.

```python PYTHON
# Upload a dataset for embed jobs
ds = co.datasets.create(
    name="sample_file",
    # insert your file path here - you can upload it on the right - we accept .csv and jsonl files
    data=open("embed_jobs_sample_data.jsonl", "rb"),
    keep_fields=["wiki_id", "url", "views", "title"],
    optional_fields=["langs"],
    type="embed-input",
)

# wait for the dataset to finish validation
print(co.wait(ds))
```

### Dataset Types

When a dataset is created, the `type` field *must* be specified in order to indicate the type of tasks this dataset is meant for.

The following table describes the types of datasets supported by the Dataset API:

#### Supported Dataset Types

| Dataset Type  | Description                           | Schema        | Rules                                      | Task Type | Status    | File Types Supported | Are Metadata Fields Supported? | Sample File                                                                                                                                                                                                                                                                                                 |
| ------------- | ------------------------------------- | ------------- | ------------------------------------------ | --------- | --------- | -------------------- | ------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `embed-input` | A file containing text to be embedded | `text:string` | None of the rows in the file can be empty. | Embed job | Supported | `csv` and `jsonl`    | Yes                            | [embed\_jobs\_sample\_data.jsonl](https://raw.githubusercontent.com/cohere-ai/cohere-developer-experience/main/notebooks/data/embed_jobs_sample_data.jsonl) / [embed\_jobs\_sample\_data.csv](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/data/embed_jobs_sample_data.csv) |

### Downloading a dataset

Datasets can be fetched using its unique `id`. Note that the dataset `name` and `id` are different from each other; names can be duplicated, while `id`s cannot.

Here is an example code snippet showing how to fetch a dataset by its unique `id`.

```python PYTHON
# fetch the dataset by ID
my_dataset_response = co.datasets.get(id="<DATASET_ID>")
my_dataset = my_dataset_response.dataset

# print each entry in the dataset
for record in my_dataset:
    print(record)

# save the dataset as jsonl
co.utils.save_dataset(
    dataset=my_dataset, filepath="./path/to/new/file.jsonl"
)
# or save the dataset as csv
co.utils.save_dataset(
    dataset=my_dataset, filepath="./path/to/new/file.csv"
)
```

### Deleting a dataset

Datasets are automatically deleted after 30 days, but they can also be deleted manually. Here's a code snippet showing how to do that:

```python PYTHON
co.datasets.delete(id="<DATASET_ID>")
```


# Help Us Improve The Cohere Docs

> Contribute to our docs content, stored in the cohere-developer-experience repo; we welcome your pull requests!

All our docs content is stored in [https://github.com/cohere-ai/cohere-developer-experience/](https://github.com/cohere-ai/cohere-developer-experience/).

We welcome contributions to this repo! Feel free to pull request any of the content you see and we will work with you to merge it. The OpenAPI specs and snippets are one-way synced from our internal repositories so we will need to take your changes and merge them behind the scenes.

Please see the repository readme for more guidance.


# Working with Cohere's API and SDK

> Cohere's NLP platform provides customizable large language models and tools for developers to build AI applications.

The Cohere platform allows you to leverage the power of [large language models](https://docs.cohere.com/v1/docs/the-cohere-platform#large-language-models-llms) (LLMs) with just a few lines of code and an [API key](https://dashboard.cohere.com/api-keys?_gl=1*14v2pj5*_gcl_au*NTczMTgyMTIzLjE3MzQ1NTY2OTA.*_ga*MTAxNTg1NTM1MS4xNjk1MjMwODQw*_ga_CRGS116RZS*MTczNjI3NzU2NS4xOS4xLjE3MzYyODExMTkuNDkuMC4w).

Our [Command](https://docs.cohere.com/v1/docs/command-r7b), [Embed](https://docs.cohere.com/v1/docs/cohere-embed), [Rerank](https://docs.cohere.com/v1/docs/rerank), and [Aya](https://docs.cohere.com/v1/docs/aya) models excel at a variety of applications, from the relatively simple ([semantic search](https://docs.cohere.com/v1/docs/semantic-search-embed), and [content generation](https://docs.cohere.com/v1/docs/introduction-to-text-generation-at-cohere)) to the more advanced ([retrieval augmented generation](https://docs.cohere.com/v1/docs/retrieval-augmented-generation-rag) and [agents](https://docs.cohere.com/v1/docs/multi-step-tool-use)). If you have a more specialized use case and custom data, you can also [train a custom model](https://docs.cohere.com/v1/docs/fine-tuning) to get better performance.

Check out [our documentation](https://docs.cohere.com/v1/docs/the-cohere-platform) if you're ready to start building, and you might want to check out our [API pricing](https://docs.cohere.com/v1/docs/rate-limits).

## SDKs

The Cohere SDK is the primary way of accessing Cohere's models. We support SDKs in four different languages. To get started, please see the installation methods and code snippets below.

### Python

[https://github.com/cohere-ai/cohere-python](https://github.com/cohere-ai/cohere-python)

```bash
python -m pip install cohere --upgrade
```

```python
import cohere

co = cohere.ClientV2("<<apiKey>>")
response = co.chat(
    model="command-a-03-2025", 
    messages=[{"role": "user", "content": "hello world!"}]
)

print(response)
```

### Typescript

[https://github.com/cohere-ai/cohere-typescript](https://github.com/cohere-ai/cohere-typescript)

```bash
npm i -s cohere-ai
```

```typescript
const { CohereClientV2 } = require('cohere-ai');

const cohere = new CohereClientV2({
  token: '<<apiKey>>',
});

(async () => {
  const response = await cohere.chat({
    model: 'command-a-03-2025',
    messages: [
      {
        role: 'user',
        content: 'hello world!',
      },
    ],
  });

  console.log(response);
})();
```

### Java

[https://github.com/cohere-ai/cohere-java](https://github.com/cohere-ai/cohere-java)

```gradle
implementation 'com.cohere:cohere-java:1.x.x'
```

```java
package chatv2post;

import com.cohere.api.Cohere;
import com.cohere.api.resources.v2.requests.V2ChatRequest;
import com.cohere.api.types.*;
import java.util.List;

public class Default {
    public static void main(String[] args) {
        Cohere cohere = Cohere.builder().token("<<apiKey>>").clientName("snippet").build();

        ChatResponse response =
                cohere.v2()
                        .chat(
                                V2ChatRequest.builder()
                                    .model("command-a-03-2025")
                                    .messages(
                                        List.of(
                                            ChatMessageV2.user(
                                                UserMessage.builder()
                                                    .content(
                                                        UserMessageContent
                                                                .of("Hello world!"))
                                                    .build())))
                                    .build());

        System.out.println(response);
    }
}
```

### Go

[https://github.com/cohere-ai/cohere-go](https://github.com/cohere-ai/cohere-go)

```bash
go get github.com/cohere-ai/cohere-go/v2
```

```go
package main

import (
	"context"
	"log"

	cohere "github.com/cohere-ai/cohere-go/v2"
	client "github.com/cohere-ai/cohere-go/v2/client"
)

func main() {
	co := client.NewClient(client.WithToken("Your API key"))

	resp, err := co.Chat(
		context.TODO(),
		&cohere.ChatRequest{
			Message: "Hello world!",
		},
	)

	if err != nil {
		log.Fatal(err)
	}

	log.Printf("%+v", resp)
}

```

## Request Specification

To make a request to any model, you must pass in the `Authorization` Header and the request must be made through a `POST` request.

The content of `Authorization` should be in the shape of `BEARER [API_KEY]`. All request bodies are sent through JSON.

Model names are found within the dashboard, and details about endpoints are available within the documentation.



---

**Navigation:** [← Previous](./06-build-an-onboarding-assistant-with-cohere.md) | [Index](./index.md) | [Next →](./08-teams-and-roles-on-the-cohere-platform.md)
