---
title: "Cohere Documentation"
description: "Formatted documentation for Cohere"
source: "llms-full.txt"
last_updated: "2025-11-08"
---

## Answer: 0.63

```
```mdx
Question:
What's the average evaluation score in run A
==================================================


Python REPL can execute arbitrary code. Use with caution.


TOOL PLAN:
I will write and execute Python code to calculate the average evaluation score in run A. 

TOOL CALLS:
Tool name: analyze_evaluation_results
    import pandas as pd
    
    df = pd.read_csv("evaluation_results.csv")
    
    # Calculate the average evaluation score in run A

    average_score_run_A = df[df["run"] == "A"]["score"].mean()
    
    print(f"Average evaluation score in run A: {average_score_run_A}")
None
==================================================
RESPONSE:
The average evaluation score in run A is 0.63.
==================================================
CITATIONS:

Start: 41| End:46| Text:'0.63.' 
Sources:
1. analyze_evaluation_results_phqpwwat2hgf:0
```
Next, we ask a slightly more complex question, this time about the latency of the highest-scoring run for one use case. This requires the agent to filter based on the use case, find the highest-scoring run, and return the latency value.
```python PYTHON
messages = run_agent(
    "What's the latency of the highest-scoring run for the summarize_article use case?"
)

---

**ðŸ“š [Back to Index](./index.md)** | **ðŸ“„ [Full Version](./documentation.md)** | **ðŸ”— [Original](../llms-full.txt)**
