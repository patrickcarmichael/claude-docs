---
title: "Cohere Documentation"
description: "Formatted documentation for Cohere"
source: "llms-full.txt"
last_updated: "2025-11-08"
---

## Introduction to Aya Vision

> In this notebook, we will explore the capabilities of Aya Vision, which can take text and image inputs to generates text responses.

<CookbookHeader href="https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/aya_vision_intro.ipynb" />

Introducing Aya Vision - a state-of-the-art open-weights multimodal multilingual model.

<img src="https://github.com/cohere-ai/cohere-developer-experience/raw/main/notebooks/images/aya-vision/Aya-Vision.jpg" />

In this notebook, we will explore the capabilities of Aya Vision, which can take text and image inputs to generates text responses.

The following links provide further details about the Aya Vision model:

* [The launch blog](https://cohere.com/blog/aya-vision)
* [Documentation](https://docs.cohere.com/docs/aya-multimodal)
* HuggingFace model page for the [32B](https://huggingface.co/CohereForAI/aya-vision-32b) and [8B](https://huggingface.co/CohereForAI/aya-vision-8b) models.

This tutorial will provide a walkthrough of the various use cases that you can build with Aya Vision. By the end of this notebook, you will have a solid understanding of how to use Aya Vision for a wide range of applications.

The list of possible use cases with multimodal models is endless, but this notebook will cover the following:

* Setup
* Question answering
* Multilingual multimodal understanding
* Captioning
* Recognizing text
* Classification
* Comparing multiple images
* Conclusion

---

**ðŸ“š [Back to Index](./index.md)** | **ðŸ“„ [Full Version](./documentation.md)** | **ðŸ”— [Original](../llms-full.txt)**
