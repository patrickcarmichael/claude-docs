---
title: "Cohere Documentation"
description: "Formatted documentation for Cohere"
source: "llms-full.txt"
last_updated: "2025-11-08"
---

## Answer: 0.63

```
```mdx

Question:
What's the average evaluation score in run A
==================================================
Tool plan:
I will query the connected SQL database to find the average evaluation score in run A. 

Tool calls:
Tool name: sql_table_query | Parameters: {"query":"SELECT AVG(score) AS average_score\r\nFROM evaluation_results\r\nWHERE run = 'A';"}
==================================================
Response:
The average evaluation score in run A is 0.63.
==================================================
CITATIONS:

Start: 41| End:46| Text:'0.63.' 
Sources:
1. sql_table_query_97h16txpbeqs:0
```
```python PYTHON
messages = run_agent(
    "What's the latency of the highest-scoring run for the summarize_article use case?"
)

---

**ðŸ“š [Back to Index](./index.md)** | **ðŸ“„ [Full Version](./documentation.md)** | **ðŸ”— [Original](../llms-full.txt)**
