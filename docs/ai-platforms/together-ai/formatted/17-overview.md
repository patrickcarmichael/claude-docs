---
title: "Together AI Documentation"
description: "Formatted documentation for Together AI"
source: "llms-full.txt"
last_updated: "2025-11-08"
---

## Overview

Large language models can serve as judges to evaluate other language models or assess different types of content. You can simply describe in detail how you want the LLM-as-a-Judge to assess your inputs, and it will perform this evaluation for you.

For example, they can identify and flag content containing harmful material, personal information, or other policy-violating elements.
Another common use case is comparing the quality of two LLMs, or configurations of the same model (for example prompts) to determine which performs better on your specific task. Our Evaluations service allows you to easily submit tasks for assessment by a judge language model.

With Evaluations, you can:

* **Compare models and configurations**: Understand which setup works best for your task
* **Measure performance**: Use a variety of metrics to score your model's responses
* **Filter datasets**: Apply LLM-as-a-Judge to filter and curate your datasets
* **Gain insights**: Understand where your model excels and where it needs improvement
* **Build with confidence**: Ensure your models meet quality standards before deploying them to production

---

**ðŸ“š [Back to Index](./index.md)** | **ðŸ“„ [Full Version](./documentation.md)** | **ðŸ”— [Original](../llms-full.txt)**
