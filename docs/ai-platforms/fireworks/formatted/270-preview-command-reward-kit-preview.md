---
title: "Fireworks Documentation"
description: "Formatted documentation for Fireworks"
source: "llms-full.txt"
last_updated: "2025-11-08"
---

## Preview Command (`reward-kit preview`)

The `preview` command allows you to test reward functions with sample data. A primary use case is to inspect or re-evaluate the `preview_input_output_pairs.jsonl` file generated by the `reward-kit run` command. This allows you to iterate on reward logic using a fixed set of model generations or to apply different metrics to the same outputs.

You can also use it with manually created sample files.

### Syntax

```bash
reward-kit preview [options]
```

### Options

* `--metrics-folders`: Specify local metric scripts to apply, in the format "name=path/to/metric\_script\_dir". The directory should contain a `main.py` with a `@reward_function`.
* `--samples`: Path to a JSONL file containing sample conversations or prompt/response pairs. This is typically the `preview_input_output_pairs.jsonl` file from a `reward-kit run` output directory.
* `--remote-url`: (Optional) URL of a deployed evaluator to use for scoring, instead of local `--metrics-folders`.
* `--max-samples`: Maximum number of samples to process (optional)
* `--output`: Path to save preview results (optional)
* `--verbose`: Enable verbose output (optional)

### Examples

```bash

---

**ðŸ“š [Back to Index](./index.md)** | **ðŸ“„ [Full Version](./documentation.md)** | **ðŸ”— [Original](../llms-full.txt)**
